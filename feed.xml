<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://antoineach.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="http://antoineach.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-12T11:39:30+00:00</updated><id>http://antoineach.github.io//feed.xml</id><title type="html">blank</title><entry><title type="html">Batching PointClouds</title><link href="http://antoineach.github.io//blog/2025/batchingPointclouds/" rel="alternate" type="text/html" title="Batching PointClouds"/><published>2025-10-09T00:00:00+00:00</published><updated>2025-10-09T00:00:00+00:00</updated><id>http://antoineach.github.io//blog/2025/batchingPointclouds</id><content type="html" xml:base="http://antoineach.github.io//blog/2025/batchingPointclouds/"><![CDATA[<h2 id="Ô∏è-characteristics-of-point-clouds">‚òÅÔ∏è Characteristics of Point Clouds</h2> <ol> <li><strong>Variable size</strong> ‚Äì each point cloud contains a different number of points \(N\).</li> <li><strong>Unordered</strong> ‚Äì permuting the points does not change the represented object.</li> <li><strong>Irregular</strong> ‚Äì there is no fixed neighborhood structure like in images.</li> <li><strong>Continuous</strong> ‚Äì each point lives in continuous 3D space:<br/> \((x, y, z) \in \mathbb{R}^3\)</li> </ol> <hr/> <h2 id="Ô∏è-the-variable-number-of-points-problem">‚ö†Ô∏è The Variable Number of Points Problem</h2> <p>The fact that each point cloud has a different number of points \(N\) prevents <strong>batch parallelization</strong> like in image-based neural networks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Classic computer vision: easy batching
</span><span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">224</span><span class="p">)</span>
<span class="c1"># ‚úÖ All images have the same shape ‚Üí can be stacked together
</span>
<span class="c1"># With point clouds ‚Äî impossible!
</span><span class="n">obj1</span> <span class="o">=</span> <span class="mi">1523</span> <span class="n">points</span>   <span class="c1"># chair
</span><span class="n">obj2</span> <span class="o">=</span> <span class="mi">3891</span> <span class="n">points</span>   <span class="c1"># table
</span><span class="n">obj3</span> <span class="o">=</span> <span class="mi">892</span> <span class="n">points</span>    <span class="c1"># lamp
</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">obj1</span><span class="p">,</span> <span class="n">obj2</span><span class="p">,</span> <span class="n">obj3</span><span class="p">])</span>  <span class="c1"># ‚ùå Different sizes ‚Üí error!
</span></code></pre></div></div> <hr/> <h2 id="-common-strategies-to-handle-variable-point-counts">üß© Common Strategies to Handle Variable Point Counts</h2> <h3 id="1Ô∏è‚É£-batch-size--1">1Ô∏è‚É£ Batch Size = 1</h3> <p>Process each object individually. ‚Üí <strong>Drawback:</strong> training is extremely slow, and statistical relations between samples in a batch are lost.</p> <hr/> <h3 id="2Ô∏è‚É£-downsampling">2Ô∏è‚É£ <strong>Downsampling</strong></h3> <p>Randomly sample each point cloud to reach a fixed size (e.g. 1024 points). ‚Üí <strong>Pros:</strong> Simple to implement ‚Üí <strong>Cons:</strong> Loss of geometric detail, especially when point counts differ greatly (e.g. from 1k to 10k ‚Üí 90% data loss).</p> <hr/> <h3 id="3Ô∏è‚É£-oversampling">3Ô∏è‚É£ <strong>Oversampling</strong></h3> <p>Duplicate some points to reach the target size ( \(N' = N + \Delta N\) ). This works for architectures like <strong>PointNet</strong>, since each point is independently projected via a shared MLP, then aggregated with <strong>max pooling</strong>.</p> <div class="row justify-content-center"> <div class="col-md-8 text-center"> <figure> <picture> <img src="/assets/img/maxpool.png" class="img-fluid rounded z-depth-1 shadow-sm" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2 text-muted">Example of PointNet using shared MLP + max pooling.</div> </div> </div> <p>However, if we replaced max pooling with <strong>mean pooling</strong>, duplicates would bias the average and distort the representation.</p> <hr/> <h3 id="4Ô∏è‚É£-sparse-tensor-representation-practical-solution">4Ô∏è‚É£ <strong>Sparse Tensor Representation (Practical Solution)</strong></h3> <p>In practice, frameworks like <strong>Torch Scatter</strong> allow concatenation of all points from a batch while preserving object boundaries.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Instead of stacking ‚Üí concatenate all points
</span><span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">obj1_points</span><span class="p">,</span> <span class="n">obj2_points</span><span class="p">,</span> <span class="n">obj3_points</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [6306, 3]  (x, y, z)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">obj1_features</span><span class="p">,</span> <span class="n">obj2_features</span><span class="p">,</span> <span class="n">obj3_features</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [6306, 64]  (features)
</span>
<span class="c1"># Track where each object ends using offsets
</span><span class="n">o</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1523</span><span class="p">,</span> <span class="mi">5414</span><span class="p">,</span> <span class="mi">6306</span><span class="p">])</span>  <span class="c1"># cumulative end indices
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|----------obj1----------|---------------obj2--------------|---obj3---|
0                       1523                             5414        6306
                         ‚Üë                                 ‚Üë          ‚Üë
                      offset[0]                       offset[1]  offset[2]
</code></pre></div></div> <p>These <strong>offsets</strong> let the model know where each object starts and ends in the concatenated tensor.</p> <div class="row justify-content-center"> <div class="col-md-8 text-center"> <figure> <picture> <img src="/assets/img/torch_scatter.svg" class="img-fluid rounded z-depth-1 shadow-sm" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2 text-muted">Example of Torch Scatter add operator.</div> </div> </div> <hr/> <h1 id="other-operator-that-supports-the-concatenation-of-pointclouds--nnlinear">Other operator that supports the concatenation of pointclouds : nn.Linear</h1> <h2 id="definition">Definition</h2> <p>The <code class="language-plaintext highlighter-rouge">torch.nn.Linear</code> layer applies an affine linear transformation:</p> \[y = xA^T + b\] <p>Where:</p> <ul> <li>\(x \in \mathbb{R}^{n \times d_{in}}\) is the input matrix (n samples, \(d_{in}\) input features)</li> <li>\(A \in \mathbb{R}^{d_{out} \times d_{in}}\) is the weight matrix</li> <li>\(b \in \mathbb{R}^{d_{out}}\) is the bias vector</li> <li>\(y \in \mathbb{R}^{n \times d_{out}}\) is the output matrix</li> </ul> <p>The bias \(b\) is broadcast and added to each row of \(xA^T\).</p> <h2 id="application-to-concatenated-point-clouds">Application to Concatenated Point Clouds</h2> <h3 id="setup">Setup</h3> <p>Consider two point clouds with different numbers of points:</p> <ul> <li>Point cloud 1: $N_1 = 2$ points</li> <li>Point cloud 2: $N_2 = 3$ points</li> <li>Each point has 3 coordinates (x, y, z)</li> </ul> <p><strong>Point Cloud 1:</strong> \(X_1 = \begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\ x_{21} &amp; x_{22} &amp; x_{23} \end{bmatrix} \in \mathbb{R}^{2 \times 3}\)</p> <p><strong>Point Cloud 2:</strong> \(X_2 = \begin{bmatrix} x_{31} &amp; x_{32} &amp; x_{33} \\ x_{41} &amp; x_{42} &amp; x_{43} \\ x_{51} &amp; x_{52} &amp; x_{53} \end{bmatrix} \in \mathbb{R}^{3 \times 3}\)</p> <h3 id="concatenation">Concatenation</h3> <p>Concatenate along the point dimension:</p> \[X = \begin{bmatrix} X_1 \\ X_2 \end{bmatrix} = \begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\ x_{21} &amp; x_{22} &amp; x_{23} \\ x_{31} &amp; x_{32} &amp; x_{33} \\ x_{41} &amp; x_{42} &amp; x_{43} \\ x_{51} &amp; x_{52} &amp; x_{53} \end{bmatrix} \in \mathbb{R}^{5 \times 3}\] <h3 id="linear-transformation">Linear Transformation</h3> <p>Apply <code class="language-plaintext highlighter-rouge">nn.Linear(in_features=3, out_features=2)</code>:</p> <p><strong>Weight matrix:</strong> \(A = \begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13} \\ w_{21} &amp; w_{22} &amp; w_{23} \end{bmatrix} \in \mathbb{R}^{2 \times 3}\)</p> <p><strong>Bias vector:</strong> \(b = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix} \in \mathbb{R}^{2}\)</p> <p><strong>Transpose of weight matrix:</strong> \(A^T = \begin{bmatrix} w_{11} &amp; w_{21} \\ w_{12} &amp; w_{22} \\ w_{13} &amp; w_{23} \end{bmatrix} \in \mathbb{R}^{3 \times 2}\)</p> <h3 id="matrix-multiplication-y--xat--b">Matrix Multiplication: $Y = XA^T + b$</h3> \[Y = \begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\ x_{21} &amp; x_{22} &amp; x_{23} \\ x_{31} &amp; x_{32} &amp; x_{33} \\ x_{41} &amp; x_{42} &amp; x_{43} \\ x_{51} &amp; x_{52} &amp; x_{53} \end{bmatrix} \begin{bmatrix} w_{11} &amp; w_{21} \\ w_{12} &amp; w_{22} \\ w_{13} &amp; w_{23} \end{bmatrix} + \begin{bmatrix} b_1 &amp; b_2 \end{bmatrix}\] <h3 id="row-by-row-computation">Row-by-Row Computation</h3> <p>Each output row is computed independently:</p> \[y_1 = \begin{bmatrix} x_{11}w_{11} + x_{12}w_{12} + x_{13}w_{13} + b_1 &amp; x_{11}w_{21} + x_{12}w_{22} + x_{13}w_{23} + b_2 \end{bmatrix}\] \[y_2 = \begin{bmatrix} x_{21}w_{11} + x_{22}w_{12} + x_{23}w_{13} + b_1 &amp; x_{21}w_{21} + x_{22}w_{22} + x_{23}w_{23} + b_2 \end{bmatrix}\] \[y_3 = \begin{bmatrix} x_{31}w_{11} + x_{32}w_{12} + x_{33}w_{13} + b_1 &amp; x_{31}w_{21} + x_{32}w_{22} + x_{33}w_{23} + b_2 \end{bmatrix}\] \[y_4 = \begin{bmatrix} x_{41}w_{11} + x_{42}w_{12} + x_{43}w_{13} + b_1 &amp; x_{41}w_{21} + x_{42}w_{22} + x_{43}w_{23} + b_2 \end{bmatrix}\] \[y_5 = \begin{bmatrix} x_{51}w_{11} + x_{52}w_{12} + x_{53}w_{13} + b_1 &amp; x_{51}w_{21} + x_{52}w_{22} + x_{53}w_{23} + b_2 \end{bmatrix}\] <h2 id="key-property-no-mixing-between-points">Key Property: No Mixing Between Points</h2> <p>Each output row $y_i$ depends <strong>only</strong> on its corresponding input row \(x_i\):</p> \[y_i = x_i A^T + b, \quad i = 1, 2, \ldots, 5\] <p>Therefore:</p> <ul> <li>Rows 1-2 (from PC1) are transformed independently</li> <li>Rows 3-5 (from PC2) are transformed independently</li> <li><strong>No information is mixed between different points or different point clouds</strong></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[A comprehensive explanation of Batching PointClouds.]]></summary></entry><entry><title type="html">PointTransformer v1 Explained</title><link href="http://antoineach.github.io//blog/2025/pointTransformerV1/" rel="alternate" type="text/html" title="PointTransformer v1 Explained"/><published>2025-10-09T00:00:00+00:00</published><updated>2025-10-09T00:00:00+00:00</updated><id>http://antoineach.github.io//blog/2025/pointTransformerV1</id><content type="html" xml:base="http://antoineach.github.io//blog/2025/pointTransformerV1/"><![CDATA[<div class="row justify-content-center"> <div class="col-md-14 text-center"> <figure> <picture> <img src="/assets/img/pointTransformerV1_architecture.svg" class="img-fluid rounded z-depth-1 shadow-sm" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2 text-muted"> Point transformer networks for semantic segmentation.</div> </div> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[A comprehensive explanation of PointTransformer v1 with key dimensional insights.]]></summary></entry><entry><title type="html">MultiHead Attention Visualized</title><link href="http://antoineach.github.io//blog/2025/multiheadattention/" rel="alternate" type="text/html" title="MultiHead Attention Visualized"/><published>2025-10-08T00:00:00+00:00</published><updated>2025-10-08T00:00:00+00:00</updated><id>http://antoineach.github.io//blog/2025/multiheadattention</id><content type="html" xml:base="http://antoineach.github.io//blog/2025/multiheadattention/"><![CDATA[<p>This post is adapted from <a href="https://www.geeksforgeeks.org/nlp/multi-head-attention-mechanism/">GeeksforGeeks</a>‚Äôs article on the Multi-Head Attention Mechanism by <em>sanjulika_sharma</em>.<br/> It provides an intuitive understanding of how multiple attention heads work in parallel to capture different representation subspaces.</p> <hr/> <h2 id="-what-is-multi-head-attention">üß† What is Multi-Head Attention?</h2> <p>The Multi-Head Attention mechanism allows a model to <strong>focus on different parts of a sequence simultaneously</strong>.<br/> Each head learns different contextual relationships ‚Äî for example, one might focus on word order while another captures long-range dependencies.</p> <hr/> <h2 id="-visualization">üìä Visualization</h2> <p>Below is a simple diagram illustrating how queries (Q), keys (K), and values (V) interact across multiple heads.</p> <div class="row justify-content-center mt-4"> <div class="col-md-8 text-center"> <figure> <picture> <img src="/assets/img/multiheadAttention.svg" class="img-fluid rounded z-depth-1 shadow-sm" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2 text-muted"> Multi-Head Attention ‚Äî each head performs scaled dot-product attention in parallel. </div> </div> </div> <hr/> <h2 id="Ô∏è-implementation-example">‚öôÔ∏è Implementation Example</h2> <p>Below is a PyTorch implementation of <strong>Multi-Head Attention</strong>.<br/> It combines several attention heads computed in parallel, each with its own query, key, and value subspace.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiheadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">H</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_dim</span> <span class="o">=</span> <span class="n">in_dim</span>      <span class="c1"># Input embedding size
</span>        <span class="n">self</span><span class="p">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">D</span>                <span class="c1"># Model embedding size
</span>        <span class="n">self</span><span class="p">.</span><span class="n">H</span> <span class="o">=</span> <span class="n">H</span>                <span class="c1"># Number of attention heads
</span>
        <span class="c1"># Compute Q, K, V for all heads at once
</span>        <span class="n">self</span><span class="p">.</span><span class="n">qkv_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">D</span><span class="p">)</span>
        <span class="c1"># Final projection layer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">in_dim</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>

        <span class="c1"># 1Ô∏è‚É£ Compute concatenated Q, K, V
</span>        <span class="n">qkv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">qkv_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, N, 3*D)
</span>
        <span class="c1"># 2Ô∏è‚É£ Split heads
</span>        <span class="n">qkv</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">H</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">D</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">H</span><span class="p">)</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Each: (B, H, N, D//H)
</span>
        <span class="c1"># 3Ô∏è‚É£ Scaled dot-product attention
</span>        <span class="n">d_k</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scaled</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">scaled</span> <span class="o">+=</span> <span class="n">mask</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scaled</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 4Ô∏è‚É£ Apply attention to values
</span>        <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="c1"># 5Ô∏è‚É£ Concatenate heads
</span>        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">D</span><span class="p">)</span>

        <span class="c1"># 6Ô∏è‚É£ Final linear projection
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_layer</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="-key-takeaway">üß© Key Takeaway</h2> <blockquote> <p>Multi-Head Attention enhances a model‚Äôs representational capacity by letting it attend to information from <strong>different representation subspaces</strong> simultaneously ‚Äî leading to richer contextual understanding.</p> </blockquote> <hr/>]]></content><author><name></name></author><summary type="html"><![CDATA[A comprehensive explanation of MultiHead Attention with dimensions.]]></summary></entry></feed>