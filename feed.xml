<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://antoineach.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="http://antoineach.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-24T12:15:17+00:00</updated><id>http://antoineach.github.io//feed.xml</id><title type="html">blank</title><entry><title type="html">Point Transformer v1: Architecture and Implementation Details</title><link href="http://antoineach.github.io//blog/2025/pointTransformerV1/" rel="alternate" type="text/html" title="Point Transformer v1: Architecture and Implementation Details"/><published>2025-10-13T00:00:00+00:00</published><updated>2025-10-13T00:00:00+00:00</updated><id>http://antoineach.github.io//blog/2025/pointTransformerV1</id><content type="html" xml:base="http://antoineach.github.io//blog/2025/pointTransformerV1/"><![CDATA[<h1 id="point-transformer-v1-architecture-and-implementation-details">Point Transformer v1: Architecture and Implementation Details</h1> <h2 id="introduction">Introduction</h2> <p><strong>Point Transformer v1</strong> is a model for segmentation and classification of 3D point clouds that adapts the Transformer mechanism to unstructured 3D data while respecting point-cloud specific constraints. Published in 2021, it adapts attention to local neighborhoods and the irregular nature of point clouds.</p> <p>The model follows a <strong>U-Net</strong>-like architecture composed of three main layer types:</p> <ul> <li><strong>PointTransformerLayer</strong>: local attention over the K nearest neighbors</li> <li><strong>TransitionDown</strong>: spatial downsampling using Furthest Point Sampling (FPS)</li> <li><strong>TransitionUp</strong>: upsampling with skip connections</li> </ul> <hr/> <h2 id="overall-architecture">Overall Architecture</h2> <p>The network follows an encoderâ€“decoder (U-Net) design:</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/pointTransformerV1_architecture.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Key features:</strong></p> <ul> <li><strong>Local attention</strong>: attention is computed only over K nearest neighbors (typically K = 16) rather than globally.</li> <li><strong>Permutation invariance</strong>: the architecture respects the lack of natural ordering in point clouds.</li> <li><strong>Skip connections</strong>: U-Net style skip connections preserve spatial details.</li> </ul> <hr/> <h3 id="-input-reminder--batched-point-clouds">ğŸ§± Input Reminder â€” Batched Point Clouds</h3> <p>Before diving into PointTransformer internals, recall that we handle <strong>batches of point clouds</strong> by concatenating them into a single tensor:</p> \[X \in \mathbb{R}^{N \times C}, \quad \text{where } N = N_1 + N_2 + \dots + N_B\] <p>and we keep <strong>offsets</strong> to delimit each cloudâ€™s boundaries: \(\text{offsets} = [N_1,, N_1{+}N_2,, \dots,, N_1{+}N_2{+}\dots{+}N_B]\)</p> <p>Each row of (X) corresponds to one 3D point and its features â€” so linear layers act point-wise, without mixing points from different objects.</p> <p>For a detailed explanation of this batching strategy, see ğŸ‘‰ <a href="/blog/2025/batchingPointclouds/">Batching Point Clouds</a>.</p> <hr/> <h2 id="pointtransformerblock-residual-block">PointTransformerBlock: Residual Block</h2> <p><code class="language-plaintext highlighter-rouge">PointTransformerBlock</code> wraps the <code class="language-plaintext highlighter-rouge">PointTransformerLayer</code> inside a residual block (ResNet-style).</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/pointTransformerBlock.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Residual connections improve gradient flow, help learn residual mappings, and preserve initial information.</p> <hr/> <h2 id="pointtransformerlayer-vectorial-local-attention">PointTransformerLayer: Vectorial Local Attention</h2> <h3 id="overview">Overview</h3> <p>The <code class="language-plaintext highlighter-rouge">PointTransformerLayer</code> implements a <strong>local vector attention</strong> mechanism inspired by Transformers, but adapted to point clouds.</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/pointTransformerLayer.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="why-use-q---k-instead-of-qkáµ€">Why use Q - K instead of QÂ·Káµ€?</h3> <p>The batching constraint is central here. In standard Transformers you compute:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">attention_scores</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="p">.</span><span class="n">T</span>  <span class="c1"># shape (N, N) -&gt; global attention
</span></code></pre></div></div> <p>But with concatenated point clouds:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>points = [ pc1_points | pc2_points | pc3_points ]
          â†    N_1   â†’ â†    N_2   â†’ â†    N_3   â†’
</code></pre></div></div> <p>A full \(N \times N\) attention matrix would include cross-cloud scores (e.g. between pc1 and pc2), which is <strong>invalid</strong>.</p> <p>Point Transformer avoids this by:</p> <ol> <li><strong>Local attention only</strong>: compute attention over the K nearest neighbors within the same cloud.</li> <li><strong>Neighbor search respecting offsets</strong>: <code class="language-plaintext highlighter-rouge">query_and_group</code> or neighbor routines use offsets to restrict neighbor search to the same cloud.</li> <li><strong>Using Q âˆ’ K (relative vector) rather than a global dot product</strong>:</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># For each query point, consider its K neighbors (guaranteed same cloud)
</span><span class="n">attention_input</span> <span class="o">=</span> <span class="n">key_neighbors</span> <span class="o">-</span> <span class="n">query_expanded</span>  <span class="c1"># shape (N, K, out_dim)
# A vector difference rather than a scalar product
</span></code></pre></div></div> <p>This vector difference captures relative relationships without producing a full NÃ—N matrix and without creating invalid cross-cloud attention.</p> <h3 id="position-encoding">Position encoding</h3> <p>Positions are explicitly encoded and added to the attention input:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">relative_positions</span> <span class="o">=</span> <span class="n">neighbor_positions</span> <span class="o">-</span> <span class="n">query_position</span>  <span class="c1"># (N, K, 3)
</span><span class="n">encoded_positions</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">relative_positions</span><span class="p">)</span>              <span class="c1"># (N, K, out_dim)
</span><span class="n">attention_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span> <span class="o">-</span> <span class="n">K</span><span class="p">)</span> <span class="o">+</span> <span class="n">encoded_positions</span>
</code></pre></div></div> <h3 id="vectorial-attention-with-groups">Vectorial attention with groups</h3> <p>Instead of a single scalar weight per neighbor, Point Transformer produces <strong><code class="language-plaintext highlighter-rouge">num_groups</code> weights per neighbor</strong>. Letâ€™s understand why and how this works.</p> <h4 id="visual-diagram">Visual Diagram</h4> <p>Hereâ€™s what happens for <strong>one point</strong> with <strong>K=3 neighbors</strong> and <strong>num_groups=4, out_dim=16</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Each neighbor's value vector (16 dims):
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ G0  â”‚ G1  â”‚ G2  â”‚ G3  â”‚  â† 4 groups of 4 dimensions
â”‚ [4] â”‚ [4] â”‚ [4] â”‚ [4] â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Attention weights for each neighbor (4 weights):
Neighbor 1: [wâ‚â½â°â¾, wâ‚â½Â¹â¾, wâ‚â½Â²â¾, wâ‚â½Â³â¾]
Neighbor 2: [wâ‚‚â½â°â¾, wâ‚‚â½Â¹â¾, wâ‚‚â½Â²â¾, wâ‚‚â½Â³â¾]
Neighbor 3: [wâ‚ƒâ½â°â¾, wâ‚ƒâ½Â¹â¾, wâ‚ƒâ½Â²â¾, wâ‚ƒâ½Â³â¾]

Weighted multiplication:
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Neighbor 1: â”‚wâ‚â½â°â¾Â·G0â”‚wâ‚â½Â¹â¾Â·G1â”‚wâ‚â½Â²â¾Â·G2â”‚wâ‚â½Â³â¾Â·G3â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Neighbor 2: â”‚wâ‚‚â½â°â¾Â·G0â”‚wâ‚‚â½Â¹â¾Â·G1â”‚wâ‚‚â½Â²â¾Â·G2â”‚wâ‚‚â½Â³â¾Â·G3â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Neighbor 3: â”‚wâ‚ƒâ½â°â¾Â·G0â”‚wâ‚ƒâ½Â¹â¾Â·G1â”‚wâ‚ƒâ½Â²â¾Â·G2â”‚wâ‚ƒâ½Â³â¾Â·G3â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“ sum over neighbors
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Output:     â”‚  G0   â”‚  G1   â”‚  G2   â”‚  G3   â”‚  (16 dims)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></div> <p>The shape <code class="language-plaintext highlighter-rouge">(N, K, num_groups, dim_per_group)</code> represents:</p> <ul> <li>For each of N points</li> <li>For each of K neighbors</li> <li>We have num_groups separate feature groups</li> <li>Each group has dim_per_group dimensions</li> </ul> <p>And each group gets its own attention weight, allowing fine-grained control over feature aggregation.</p> <p>For example a group may focus on</p> <ul> <li>Dimensions 0-15: color information</li> <li>Dimensions 16-31: geometric properties</li> <li>Dimensions 32-47: texture features</li> <li>Dimensions 48-63: semantic context</li> </ul> <hr/> <h2 id="transitiondown-spatial-downsampling">TransitionDown: Spatial Downsampling</h2> <p><code class="language-plaintext highlighter-rouge">TransitionDown</code> reduces the number of points (analogous to strided conv).</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/transitionDown_stride!=1.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Pipeline (high-level):</p> <ol> <li><strong>Compute new counts</strong>: for each cloud, new_count = old_count // stride.</li> <li><strong>Farthest Point Sampling (FPS)</strong>: choose M â‰ˆ N/stride representative points that maximize minimal distance; ensures spatial coverage.</li> <li><strong>K-NN grouping</strong>: for each sampled point, gather its K neighbors in the original cloud (with relative positions if <code class="language-plaintext highlighter-rouge">use_xyz=True</code>). Result: <code class="language-plaintext highlighter-rouge">(M, K, 3 + in_dim)</code>.</li> <li><strong>Projection + normalization</strong>: linear on neighbor features, BatchNorm + ReLU â†’ <code class="language-plaintext highlighter-rouge">(M, out_dim, K)</code>.</li> <li><strong>MaxPooling</strong>: aggregate K neighbors by channel-wise max â†’ <code class="language-plaintext highlighter-rouge">(M, out_dim)</code>.</li> </ol> <p>Result: reduce N points to M points (M â‰ˆ N/stride) with locally aggregated features.</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/fps_knn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="transitionup-upsampling-with-skip-connections">TransitionUp: Upsampling with Skip Connections</h2> <p><code class="language-plaintext highlighter-rouge">TransitionUp</code> increases resolution and fuses multi-scale information.</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/transitionUp_with_pxoo.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Explanation:</strong></p> <p>The interpolation transfers features from M source points to N target points (typically M &lt; N for upsampling).</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/interpolation.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Algorithm:</strong></p> <ol> <li><strong>K-NN</strong>: For each target point, find its K=3 nearest neighbors in the source cloud</li> <li><strong>Weights</strong>: Compute normalized inverse distance weights: points closer to the target have higher weights</li> <li><strong>Interpolation</strong>: Weighted average of the K neighbor features</li> </ol> <p><strong>Code:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">interpolation</span><span class="p">(</span><span class="n">p_source</span><span class="p">,</span> <span class="n">p_target</span><span class="p">,</span> <span class="n">x_source</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Args:
        p_source: (M, 3) - source positions
        p_target: (N, 3) - target positions  
        x_source: (M, C) - source features
    Returns:
        output: (N, C) - interpolated features
    </span><span class="sh">"""</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">p_target</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_source</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="c1"># Find K nearest neighbors
</span>        <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">p_source</span> <span class="o">-</span> <span class="n">p_target</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">k_nearest</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[:</span><span class="n">K</span><span class="p">]</span>
        
        <span class="c1"># Inverse distance weighting
</span>        <span class="n">dists</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">k_nearest</span><span class="p">]</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">dists</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">/=</span> <span class="n">weights</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>  <span class="c1"># normalize
</span>        
        <span class="c1"># Weighted average
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">output</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">+=</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_source</span><span class="p">[</span><span class="n">k_nearest</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p><strong>Formula:</strong> \(\text{output}[n] = \sum_{i=0}^{K-1} w_i \cdot \text{x}_\text{source}[\text{neighbor}_i], \quad w_i = \frac{1/d_i}{\sum_j 1/d_j}\)</p> <h2 id="references">References</h2> <ul> <li>Point Transformer paper (ICCV 2021): <a href="https://arxiv.org/abs/2012.09164">https://arxiv.org/abs/2012.09164</a></li> <li>Official code: <a href="https://github.com/POSTECH-CVLab/point-transformer">https://github.com/POSTECH-CVLab/point-transformer</a></li> <li>See also my post on <a href="/blog/2025/batchingPointclouds/">Batching of Point Clouds</a></li> </ul>]]></content><author><name></name></author><category term="computer-vision"/><category term="deep-learning"/><category term="point-cloud"/><category term="transformer"/><category term="architecture"/><summary type="html"><![CDATA[Detailed analysis of the Point Transformer v1 architecture for point-cloud segmentation and classification]]></summary></entry><entry><title type="html">Batching PointClouds</title><link href="http://antoineach.github.io//blog/2025/batchingPointclouds/" rel="alternate" type="text/html" title="Batching PointClouds"/><published>2025-10-09T00:00:00+00:00</published><updated>2025-10-09T00:00:00+00:00</updated><id>http://antoineach.github.io//blog/2025/batchingPointclouds</id><content type="html" xml:base="http://antoineach.github.io//blog/2025/batchingPointclouds/"><![CDATA[<h2 id="ï¸-characteristics-of-point-clouds">â˜ï¸ Characteristics of Point Clouds</h2> <ol> <li><strong>Variable size</strong> â€“ each point cloud contains a different number of points \(N\).</li> <li><strong>Unordered</strong> â€“ permuting the points does not change the represented object.</li> <li><strong>Irregular</strong> â€“ there is no fixed neighborhood structure like in images.</li> <li><strong>Continuous</strong> â€“ each point lives in continuous 3D space:<br/> \((x, y, z) \in \mathbb{R}^3\)</li> </ol> <hr/> <h2 id="ï¸-the-variable-number-of-points-problem">âš ï¸ The Variable Number of Points Problem</h2> <p>The fact that each point cloud has a different number of points \(N\) prevents <strong>batch parallelization</strong> like in image-based neural networks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Classic computer vision: easy batching
</span><span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">224</span><span class="p">)</span>
<span class="c1"># âœ… All images have the same shape â†’ can be stacked together
</span>
<span class="c1"># With point clouds â€” impossible!
</span><span class="n">obj1</span> <span class="o">=</span> <span class="mi">1523</span> <span class="n">points</span>   <span class="c1"># chair
</span><span class="n">obj2</span> <span class="o">=</span> <span class="mi">3891</span> <span class="n">points</span>   <span class="c1"># table
</span><span class="n">obj3</span> <span class="o">=</span> <span class="mi">892</span> <span class="n">points</span>    <span class="c1"># lamp
</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">obj1</span><span class="p">,</span> <span class="n">obj2</span><span class="p">,</span> <span class="n">obj3</span><span class="p">])</span>  <span class="c1"># âŒ Different sizes â†’ error!
</span></code></pre></div></div> <hr/> <h2 id="-common-strategies-to-handle-variable-point-counts">ğŸ§© Common Strategies to Handle Variable Point Counts</h2> <h3 id="1ï¸âƒ£-batch-size--1">1ï¸âƒ£ Batch Size = 1</h3> <p>Process each object individually. â†’ <strong>Drawback:</strong> training is extremely slow, and statistical relations between samples in a batch are lost.</p> <hr/> <h3 id="2ï¸âƒ£-downsampling">2ï¸âƒ£ <strong>Downsampling</strong></h3> <p>Randomly sample each point cloud to reach a fixed size (e.g. 1024 points). â†’ <strong>Pros:</strong> Simple to implement â†’ <strong>Cons:</strong> Loss of geometric detail, especially when point counts differ greatly (e.g. from 1k to 10k â†’ 90% data loss).</p> <hr/> <h3 id="3ï¸âƒ£-oversampling">3ï¸âƒ£ <strong>Oversampling</strong></h3> <p>Duplicate some points to reach the target size ( \(N' = N + \Delta N\) ). This works for architectures like <strong>PointNet</strong>, since each point is independently projected via a shared MLP, then aggregated with <strong>max pooling</strong>.</p> <div class="row justify-content-center"> <div class="col-md-8 text-center"> <figure> <picture> <img src="/assets/img/maxpool.png" class="img-fluid rounded z-depth-1 shadow-sm" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2 text-muted">Example of PointNet using shared MLP + max pooling.</div> </div> </div> <p>However, if we replaced max pooling with <strong>mean pooling</strong>, duplicates would bias the average and distort the representation.</p> <hr/> <h3 id="4ï¸âƒ£-sparse-tensor-representation-practical-solution">4ï¸âƒ£ <strong>Sparse Tensor Representation (Practical Solution)</strong></h3> <p>In practice, frameworks like <strong>Torch Scatter</strong> allow concatenation of all points from a batch while preserving object boundaries.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Instead of stacking â†’ concatenate all points
</span><span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">obj1_points</span><span class="p">,</span> <span class="n">obj2_points</span><span class="p">,</span> <span class="n">obj3_points</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [6306, 3]  (x, y, z)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">obj1_features</span><span class="p">,</span> <span class="n">obj2_features</span><span class="p">,</span> <span class="n">obj3_features</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [6306, 64]  (features)
</span>
<span class="c1"># Track where each object ends using offsets
</span><span class="n">o</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1523</span><span class="p">,</span> <span class="mi">5414</span><span class="p">,</span> <span class="mi">6306</span><span class="p">])</span>  <span class="c1"># cumulative end indices
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|----------obj1----------|---------------obj2--------------|---obj3---|
0                       1523                             5414        6306
                         â†‘                                 â†‘          â†‘
                      offset[0]                       offset[1]  offset[2]
</code></pre></div></div> <p>These <strong>offsets</strong> let the model know where each object starts and ends in the concatenated tensor.</p> <div class="row justify-content-center"> <div class="col-md-8 text-center"> <figure> <picture> <img src="/assets/img/torch_scatter.svg" class="img-fluid rounded z-depth-1 shadow-sm" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2 text-muted">Example of Torch Scatter add operator.</div> </div> </div> <hr/> <h1 id="other-operator-that-supports-the-concatenation-of-pointclouds--nnlinear">Other operator that supports the concatenation of pointclouds : nn.Linear</h1> <h2 id="definition">Definition</h2> <p>The <code class="language-plaintext highlighter-rouge">torch.nn.Linear</code> layer applies an affine linear transformation:</p> \[y = xA^T + b\] <p>Where:</p> <ul> <li>\(x \in \mathbb{R}^{n \times d_{in}}\) is the input matrix (n samples, \(d_{in}\) input features)</li> <li>\(A \in \mathbb{R}^{d_{out} \times d_{in}}\) is the weight matrix</li> <li>\(b \in \mathbb{R}^{d_{out}}\) is the bias vector</li> <li>\(y \in \mathbb{R}^{n \times d_{out}}\) is the output matrix</li> </ul> <p>The bias \(b\) is broadcast and added to each row of \(xA^T\).</p> <h2 id="application-to-concatenated-point-clouds">Application to Concatenated Point Clouds</h2> <h3 id="setup">Setup</h3> <p>Consider two point clouds with different numbers of points:</p> <ul> <li>Point cloud 1: $N_1 = 2$ points</li> <li>Point cloud 2: $N_2 = 3$ points</li> <li>Each point has 3 coordinates (x, y, z)</li> </ul> <p><strong>Point Cloud 1:</strong> \(X_1 = \begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\ x_{21} &amp; x_{22} &amp; x_{23} \end{bmatrix} \in \mathbb{R}^{2 \times 3}\)</p> <p><strong>Point Cloud 2:</strong> \(X_2 = \begin{bmatrix} x_{31} &amp; x_{32} &amp; x_{33} \\ x_{41} &amp; x_{42} &amp; x_{43} \\ x_{51} &amp; x_{52} &amp; x_{53} \end{bmatrix} \in \mathbb{R}^{3 \times 3}\)</p> <h3 id="concatenation">Concatenation</h3> <p>Concatenate along the point dimension:</p> \[X = \begin{bmatrix} X_1 \\ X_2 \end{bmatrix} = \begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\ x_{21} &amp; x_{22} &amp; x_{23} \\ x_{31} &amp; x_{32} &amp; x_{33} \\ x_{41} &amp; x_{42} &amp; x_{43} \\ x_{51} &amp; x_{52} &amp; x_{53} \end{bmatrix} \in \mathbb{R}^{5 \times 3}\] <h3 id="linear-transformation">Linear Transformation</h3> <p>Apply <code class="language-plaintext highlighter-rouge">nn.Linear(in_features=3, out_features=2)</code>:</p> <p><strong>Weight matrix:</strong> \(A = \begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13} \\ w_{21} &amp; w_{22} &amp; w_{23} \end{bmatrix} \in \mathbb{R}^{2 \times 3}\)</p> <p><strong>Bias vector:</strong> \(b = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix} \in \mathbb{R}^{2}\)</p> <p><strong>Transpose of weight matrix:</strong> \(A^T = \begin{bmatrix} w_{11} &amp; w_{21} \\ w_{12} &amp; w_{22} \\ w_{13} &amp; w_{23} \end{bmatrix} \in \mathbb{R}^{3 \times 2}\)</p> <h3 id="matrix-multiplication-y--xat--b">Matrix Multiplication: $Y = XA^T + b$</h3> \[Y = \begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\ x_{21} &amp; x_{22} &amp; x_{23} \\ x_{31} &amp; x_{32} &amp; x_{33} \\ x_{41} &amp; x_{42} &amp; x_{43} \\ x_{51} &amp; x_{52} &amp; x_{53} \end{bmatrix} \begin{bmatrix} w_{11} &amp; w_{21} \\ w_{12} &amp; w_{22} \\ w_{13} &amp; w_{23} \end{bmatrix} + \begin{bmatrix} b_1 &amp; b_2 \end{bmatrix}\] <h3 id="row-by-row-computation">Row-by-Row Computation</h3> <p>Each output row is computed independently:</p> \[y_1 = \begin{bmatrix} x_{11}w_{11} + x_{12}w_{12} + x_{13}w_{13} + b_1 &amp; x_{11}w_{21} + x_{12}w_{22} + x_{13}w_{23} + b_2 \end{bmatrix}\] \[y_2 = \begin{bmatrix} x_{21}w_{11} + x_{22}w_{12} + x_{23}w_{13} + b_1 &amp; x_{21}w_{21} + x_{22}w_{22} + x_{23}w_{23} + b_2 \end{bmatrix}\] \[y_3 = \begin{bmatrix} x_{31}w_{11} + x_{32}w_{12} + x_{33}w_{13} + b_1 &amp; x_{31}w_{21} + x_{32}w_{22} + x_{33}w_{23} + b_2 \end{bmatrix}\] \[y_4 = \begin{bmatrix} x_{41}w_{11} + x_{42}w_{12} + x_{43}w_{13} + b_1 &amp; x_{41}w_{21} + x_{42}w_{22} + x_{43}w_{23} + b_2 \end{bmatrix}\] \[y_5 = \begin{bmatrix} x_{51}w_{11} + x_{52}w_{12} + x_{53}w_{13} + b_1 &amp; x_{51}w_{21} + x_{52}w_{22} + x_{53}w_{23} + b_2 \end{bmatrix}\] <h2 id="key-property-no-mixing-between-points">Key Property: No Mixing Between Points</h2> <p>Each output row $y_i$ depends <strong>only</strong> on its corresponding input row \(x_i\):</p> \[y_i = x_i A^T + b, \quad i = 1, 2, \ldots, 5\] <p>Therefore:</p> <ul> <li>Rows 1-2 (from PC1) are transformed independently</li> <li>Rows 3-5 (from PC2) are transformed independently</li> <li><strong>No information is mixed between different points or different point clouds</strong></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[A comprehensive explanation of Batching PointClouds.]]></summary></entry><entry><title type="html">MultiHead Attention Visualized</title><link href="http://antoineach.github.io//blog/2025/multiheadattention/" rel="alternate" type="text/html" title="MultiHead Attention Visualized"/><published>2025-10-08T00:00:00+00:00</published><updated>2025-10-08T00:00:00+00:00</updated><id>http://antoineach.github.io//blog/2025/multiheadattention</id><content type="html" xml:base="http://antoineach.github.io//blog/2025/multiheadattention/"><![CDATA[<p>This post is adapted from <a href="https://www.geeksforgeeks.org/nlp/multi-head-attention-mechanism/">GeeksforGeeks</a>â€™s article on the Multi-Head Attention Mechanism by <em>sanjulika_sharma</em>.<br/> It provides an intuitive understanding of how multiple attention heads work in parallel to capture different representation subspaces.</p> <hr/> <h2 id="-what-is-multi-head-attention">ğŸ§  What is Multi-Head Attention?</h2> <p>The Multi-Head Attention mechanism allows a model to <strong>focus on different parts of a sequence simultaneously</strong>.<br/> Each head learns different contextual relationships â€” for example, one might focus on word order while another captures long-range dependencies.</p> <hr/> <h2 id="-visualization">ğŸ“Š Visualization</h2> <p>Below is a simple diagram illustrating how queries (Q), keys (K), and values (V) interact across multiple heads.</p> <div class="row justify-content-center mt-4"> <div class="col-md-8 text-center"> <figure> <picture> <img src="/assets/img/multiheadAttention.svg" class="img-fluid rounded z-depth-1 shadow-sm" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2 text-muted"> Multi-Head Attention â€” each head performs scaled dot-product attention in parallel. </div> </div> </div> <hr/> <h2 id="ï¸-implementation-example">âš™ï¸ Implementation Example</h2> <p>Below is a PyTorch implementation of <strong>Multi-Head Attention</strong>.<br/> It combines several attention heads computed in parallel, each with its own query, key, and value subspace.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiheadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">H</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_dim</span> <span class="o">=</span> <span class="n">in_dim</span>      <span class="c1"># Input embedding size
</span>        <span class="n">self</span><span class="p">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">D</span>                <span class="c1"># Model embedding size
</span>        <span class="n">self</span><span class="p">.</span><span class="n">H</span> <span class="o">=</span> <span class="n">H</span>                <span class="c1"># Number of attention heads
</span>
        <span class="c1"># Compute Q, K, V for all heads at once
</span>        <span class="n">self</span><span class="p">.</span><span class="n">qkv_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">D</span><span class="p">)</span>
        <span class="c1"># Final projection layer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">in_dim</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>

        <span class="c1"># 1ï¸âƒ£ Compute concatenated Q, K, V
</span>        <span class="n">qkv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">qkv_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, N, 3*D)
</span>
        <span class="c1"># 2ï¸âƒ£ Split heads
</span>        <span class="n">qkv</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">H</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">D</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">H</span><span class="p">)</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Each: (B, H, N, D//H)
</span>
        <span class="c1"># 3ï¸âƒ£ Scaled dot-product attention
</span>        <span class="n">d_k</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scaled</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">scaled</span> <span class="o">+=</span> <span class="n">mask</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scaled</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 4ï¸âƒ£ Apply attention to values
</span>        <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="c1"># 5ï¸âƒ£ Concatenate heads
</span>        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">D</span><span class="p">)</span>

        <span class="c1"># 6ï¸âƒ£ Final linear projection
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_layer</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="-key-takeaway">ğŸ§© Key Takeaway</h2> <blockquote> <p>Multi-Head Attention enhances a modelâ€™s representational capacity by letting it attend to information from <strong>different representation subspaces</strong> simultaneously â€” leading to richer contextual understanding.</p> </blockquote> <hr/>]]></content><author><name></name></author><summary type="html"><![CDATA[A comprehensive explanation of MultiHead Attention with dimensions.]]></summary></entry></feed>