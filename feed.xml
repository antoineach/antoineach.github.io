<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://antoineach.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="http://antoineach.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-27T15:10:35+00:00</updated><id>http://antoineach.github.io//feed.xml</id><title type="html">blank</title><entry><title type="html">Point Transformer v2: Architecture and Implementation Details</title><link href="http://antoineach.github.io//blog/2025/pointTransformerv2/" rel="alternate" type="text/html" title="Point Transformer v2: Architecture and Implementation Details"/><published>2025-10-26T00:00:00+00:00</published><updated>2025-10-26T00:00:00+00:00</updated><id>http://antoineach.github.io//blog/2025/pointTransformerv2</id><content type="html" xml:base="http://antoineach.github.io//blog/2025/pointTransformerv2/"><![CDATA[<h1 id="point-transformer-v2-architecture-et-améliorations">Point Transformer v2: Architecture et Améliorations</h1> <h2 id="introduction">Introduction</h2> <p><strong>Point Transformer v2</strong> améliore significativement son prédécesseur en termes d’efficacité computationnelle et de performances. Les innovations clés incluent :</p> <ul> <li><strong>Grid Pooling</strong> au lieu de Furthest Point Sampling (3-5× plus rapide)</li> <li><strong>Map Unpooling</strong> qui réutilise l’information du downsampling</li> <li><strong>GroupedLinear</strong> pour réduire drastiquement le nombre de paramètres</li> <li><strong>Attention vectorielle enrichie</strong> avec encodage de position sur les values</li> <li><strong>Masking des voisins invalides</strong> pour gérer les nuages de tailles variables</li> </ul> <p>Commençons par l’architecture globale avant de détailler chaque composant.</p> <hr/> <h2 id="architecture-globale">Architecture Globale</h2> <figure> <picture> <img src="/assets/img/poinTransformerV2/main_architecture.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>PTv2 suit une architecture U-Net avec :</p> <p><strong>Encodeur (Downsampling):</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input (N points, in_channels)
    ↓ GVAPatchEmbed
N points, 48 channels
    ↓ Encoder 1 (GridPool)
N1 points, 96 channels
    ↓ Encoder 2 (GridPool)
N2 points, 192 channels
    ↓ Encoder 3 (GridPool)
N3 points, 384 channels
    ↓ Encoder 4 (GridPool)
N4 points, 512 channels [BOTTLENECK]
</code></pre></div></div> <p><strong>Décodeur (Upsampling):</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>N4 points, 512 channels
    ↓ Decoder 4 (Unpool + skip)
N3 points, 384 channels
    ↓ Decoder 3 (Unpool + skip)
N2 points, 192 channels
    ↓ Decoder 2 (Unpool + skip)
N1 points, 96 channels
    ↓ Decoder 1 (Unpool + skip)
N points, 48 channels
    ↓ Segmentation Head
N points, num_classes
</code></pre></div></div> <p><strong>Points clés:</strong></p> <ul> <li>Chaque <strong>Encoder</strong> réduit le nombre de points via <strong>GridPool</strong> (voxelisation)</li> <li>Chaque <strong>Decoder</strong> remonte en résolution via <strong>Map Unpooling</strong> + skip connection</li> <li>Les <strong>clusters</strong> stockent le mapping de voxelisation pour l’unpooling</li> <li><strong>Pas de Furthest Point Sampling</strong> → beaucoup plus rapide !</li> </ul> <hr/> <h2 id="groupedlinear--réduction-paramétrique-intelligente">GroupedLinear : Réduction Paramétrique Intelligente</h2> <h3 id="le-problème-avec-linear-classique">Le problème avec Linear classique</h3> <p>Dans un réseau profond, générer des poids d’attention via des couches Linear standard accumule rapidement des paramètres :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Linear classique pour générer 8 poids d'attention depuis 64 features
</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="c1"># Paramètres: 64 × 8 = 512 poids + 8 biais = 520 paramètres
</span></code></pre></div></div> <h3 id="linnovation-groupedlinear">L’innovation GroupedLinear</h3> <figure> <picture> <img src="/assets/img/poinTransformerV2/groupedLinear.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>GroupedLinear</strong> remplace la matrice de poids par un <strong>vecteur de poids partagé</strong> :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GroupedLinear
</span><span class="n">weight</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># UN SEUL vecteur au lieu d'une matrice
# Paramètres: 64 (pas de biais)
</span></code></pre></div></div> <h3 id="fonctionnement-étape-par-étape">Fonctionnement étape par étape</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="c1"># input: (N, in_features) = (N, 64)
</span>    <span class="c1"># weight: (1, in_features) = (1, 64)
</span>    
    <span class="c1"># Étape 1: Multiplication élément par élément
</span>    <span class="n">temp</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">weight</span>  <span class="c1"># (N, 64)
</span>    
    <span class="c1"># Étape 2: Reshape en groupes
</span>    <span class="n">temp</span> <span class="o">=</span> <span class="n">temp</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">in_features</span><span class="o">/</span><span class="n">groups</span><span class="p">)</span>
    <span class="c1"># temp: (N, 8, 8)
</span>    
    <span class="c1"># Étape 3: Somme par groupe
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">temp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (N, 8)
</span>    
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <h3 id="exemple-numérique-concret">Exemple numérique concret</h3> <p>Prenons <strong>N=1, in_features=8, groups=4</strong> pour simplifier :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Input
</span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># (8,)
</span>
<span class="c1"># Weight (vecteur partagé)
</span><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>  <span class="c1"># (8,)
</span>
<span class="c1"># Étape 1: Multiplication élément par élément
</span><span class="n">temp</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="err">×</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="err">×</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">1</span><span class="err">×</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">4</span><span class="err">×</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">5</span><span class="err">×</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">2</span><span class="err">×</span><span class="mf">0.9</span><span class="p">,</span> <span class="mi">3</span><span class="err">×</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">1</span><span class="err">×</span><span class="mf">0.7</span><span class="p">]</span>
     <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>

<span class="c1"># Étape 2: Reshape en 4 groupes de 2 dimensions
</span><span class="n">temp_grouped</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span>     <span class="c1"># Groupe 0
</span>    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">],</span>     <span class="c1"># Groupe 1
</span>    <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">],</span>     <span class="c1"># Groupe 2
</span>    <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>      <span class="c1"># Groupe 3
</span><span class="p">]</span>

<span class="c1"># Étape 3: Somme par groupe
</span><span class="n">output</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mf">1.0</span> <span class="o">+</span> <span class="mf">3.0</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span>    <span class="c1"># Groupe 0
</span>    <span class="mf">0.2</span> <span class="o">+</span> <span class="mf">3.2</span> <span class="o">=</span> <span class="mf">3.4</span><span class="p">,</span>    <span class="c1"># Groupe 1
</span>    <span class="mf">1.5</span> <span class="o">+</span> <span class="mf">1.8</span> <span class="o">=</span> <span class="mf">3.3</span><span class="p">,</span>    <span class="c1"># Groupe 2
</span>    <span class="mf">1.2</span> <span class="o">+</span> <span class="mf">0.7</span> <span class="o">=</span> <span class="mf">1.9</span>     <span class="c1"># Groupe 3
</span><span class="p">]</span>
<span class="c1"># Résultat: [4.0, 3.4, 3.3, 1.9]
</span></code></pre></div></div> <h3 id="comparaison-des-paramètres">Comparaison des paramètres</h3> <table> <thead> <tr> <th>Configuration</th> <th>Linear classique</th> <th>GroupedLinear</th> <th>Réduction</th> </tr> </thead> <tbody> <tr> <td>64 → 8</td> <td>64×8 = <strong>512</strong></td> <td><strong>64</strong></td> <td>8×</td> </tr> <tr> <td>128 → 16</td> <td>128×16 = <strong>2048</strong></td> <td><strong>128</strong></td> <td>16×</td> </tr> <tr> <td>256 → 32</td> <td>256×32 = <strong>8192</strong></td> <td><strong>256</strong></td> <td>32×</td> </tr> </tbody> </table> <h3 id="pourquoi-ça-fonctionne-">Pourquoi ça fonctionne ?</h3> <p><strong>Inductive bias structuré :</strong> GroupedLinear force le modèle à utiliser les mêmes poids pour tous les groupes, mais appliqués sur des portions différentes de l’input. C’est comme dire :</p> <p><em>“Les 8 premiers channels utilisent les poids w₀-w₇ pour former le poids d’attention du groupe 0, les 8 suivants utilisent les poids w₈-w₁₅ pour le groupe 1, etc.”</em></p> <p>Cette contrainte :</p> <ul> <li>✅ Réduit le risque d’overfitting (moins de paramètres)</li> <li>✅ Force des représentations plus générales</li> <li>✅ Maintient les performances (validé empiriquement dans le papier)</li> </ul> <hr/> <h2 id="groupedvectorattention--attention-locale-enrichie">GroupedVectorAttention : Attention Locale Enrichie</h2> <h3 id="vue-densemble">Vue d’ensemble</h3> <p><code class="language-plaintext highlighter-rouge">GroupedVectorAttention</code> est le cœur de PTv2, avec plusieurs améliorations par rapport à PTv1.</p> <figure> <picture> <img src="/assets/img/poinTransformerV2/groupedVectorAttention.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Différences clés avec PTv1:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>PTv1</th> <th>PTv2</th> </tr> </thead> <tbody> <tr> <td><strong>Position Encoding sur values</strong></td> <td>❌ Non</td> <td>✅ Oui</td> </tr> <tr> <td><strong>Masking voisins invalides</strong></td> <td>❌ Non</td> <td>✅ Oui</td> </tr> <tr> <td><strong>Weight generation</strong></td> <td>MLP standard</td> <td><strong>GroupedLinear</strong> (8× moins de params)</td> </tr> <tr> <td><strong>Normalization</strong></td> <td>BatchNorm après Linear</td> <td><strong>BatchNorm + ReLU entre</strong> Q/K/V</td> </tr> </tbody> </table> <h3 id="innovation-1--position-encoding-sur-les-values">Innovation 1 : Position Encoding sur les Values</h3> <p><strong>Dans PTv1</strong>, l’encodage de position n’était ajouté qu’à la relation Q-K :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv1
</span><span class="n">relation_qk</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span> <span class="o">-</span> <span class="n">query</span><span class="p">)</span> <span class="o">+</span> <span class="nf">position_encoding</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span>
<span class="c1"># Les values ne sont PAS affectées par la géométrie
</span></code></pre></div></div> <p><strong>Dans PTv2</strong>, on ajoute aussi l’encodage aux <strong>values</strong> :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv2
</span><span class="n">pe_bias</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">relative_positions</span><span class="p">)</span>  <span class="c1"># (N, K, 3) → (N, K, C)
</span>
<span class="c1"># Sur la relation Q-K (comme PTv1)
</span><span class="n">relation_qk</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span> <span class="o">-</span> <span class="n">query</span><span class="p">)</span> <span class="o">+</span> <span class="n">pe_bias</span>

<span class="c1"># NOUVEAU: aussi sur les values !
</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span> <span class="o">+</span> <span class="n">pe_bias</span>
</code></pre></div></div> <p><strong>Pourquoi c’est important ?</strong></p> <p>L’encodage de position sur les values permet d’<strong>injecter directement l’information géométrique</strong> dans les features qui seront agrégées.</p> <p><strong>Exemple physique :</strong></p> <p>Imaginons un point représentant le coin d’une table, avec 3 voisins :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Voisin 1: dessus de la table (Δpos = [0, 0, 0.05m])
    → pe_bias₁ = [0.8, 0.1, 0.1, ...]  # proche, même surface
    
Voisin 2: pied de table (Δpos = [0, 0, -0.8m])
    → pe_bias₂ = [0.2, 0.1, 0.9, ...]  # loin, objet différent
    
Voisin 3: air vide (Δpos = [0.5, 0, 0])
    → pe_bias₃ = [0.1, 0.8, 0.1, ...]  # éloigné latéralement
</code></pre></div></div> <p>Ces encodages, ajoutés aux values, permettent au modèle de savoir <strong>où se trouvent géométriquement</strong> les features qu’il agrège, pas seulement leur importance relative (via les poids d’attention).</p> <h3 id="innovation-2--masking-des-voisins-invalides">Innovation 2 : Masking des Voisins Invalides</h3> <p><strong>Problème :</strong> Dans un batch, certains points (au bord du nuage, ou dans des régions peu denses) ont moins de K voisins. Le K-NN “pad” avec des indices <code class="language-plaintext highlighter-rouge">-1</code>.</p> <p><strong>Solution PTv2 :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># reference_index: (N, K)
# Contient -1 pour les voisins invalides (padding)
</span>
<span class="c1"># Création du masque
</span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sign</span><span class="p">(</span><span class="n">reference_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (N, K)
# Si reference_index[n, k] = -1  → sign(-1+1) = sign(0) = 0
# Si reference_index[n, k] ≥ 0   → sign(≥1) = 1
</span>
<span class="c1"># Application sur les poids d'attention
</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_weights</span> <span class="o">*</span> <span class="n">mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Shape: (N, K, groups) × (N, K, 1) → (N, K, groups)
</span></code></pre></div></div> <p><strong>Exemple concret :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Point isolé au bord du nuage avec seulement 3 vrais voisins
</span><span class="n">reference_index</span><span class="p">[</span><span class="n">point_42</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">...]</span>  <span class="c1"># K=16
</span><span class="n">mask</span><span class="p">[</span><span class="n">point_42</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">...]</span>

<span class="c1"># Poids d'attention avant masking (après softmax)
</span><span class="n">attention</span><span class="p">[</span><span class="n">point_42</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="p">...],</span>  <span class="c1"># Voisin 15 (valide)
</span>    <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.18</span><span class="p">,</span> <span class="p">...],</span> <span class="c1"># Voisin 23 (valide)
</span>    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="p">...],</span>  <span class="c1"># Voisin 8 (valide)
</span>    <span class="p">[</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">,</span> <span class="p">...],</span> <span class="c1"># Invalide mais a des poids !
</span>    <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="p">...],</span> <span class="c1"># Invalide
</span>    <span class="bp">...</span>
<span class="p">]</span>

<span class="c1"># Après masking
</span><span class="n">attention</span><span class="p">[</span><span class="n">point_42</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="p">...],</span>   <span class="c1"># OK
</span>    <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.18</span><span class="p">,</span> <span class="p">...],</span> <span class="c1"># OK
</span>    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="p">...],</span>  <span class="c1"># OK
</span>    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">...],</span>       <span class="c1"># Annulé ✓
</span>    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">...],</span>       <span class="c1"># Annulé ✓
</span>    <span class="bp">...</span>
<span class="p">]</span>
</code></pre></div></div> <p>Sans ce masking, les voisins “padding” contribueraient avec des <strong>features aléatoires/garbage</strong>, polluant l’agrégation finale !</p> <h3 id="innovation-3--groupedlinear-pour-les-poids-dattention">Innovation 3 : GroupedLinear pour les Poids d’Attention</h3> <p>Au lieu d’un MLP standard <code class="language-plaintext highlighter-rouge">Linear(C, groups)</code> avec C×groups paramètres, PTv2 utilise <code class="language-plaintext highlighter-rouge">GroupedLinear(C, groups)</code> avec seulement C paramètres.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv1: MLP standard
</span><span class="n">self</span><span class="p">.</span><span class="n">linear_w</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">mid_planes</span><span class="p">,</span> <span class="n">mid_planes</span> <span class="o">//</span> <span class="n">share_planes</span><span class="p">),</span>  <span class="c1"># C × C/G paramètres
</span>    <span class="bp">...</span>
<span class="p">)</span>

<span class="c1"># PTv2: avec GroupedLinear
</span><span class="n">self</span><span class="p">.</span><span class="n">weight_encoding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="nc">GroupedLinear</span><span class="p">(</span><span class="n">embed_channels</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">groups</span><span class="p">),</span>  <span class="c1"># Seulement C paramètres !
</span>    <span class="bp">...</span>
<span class="p">)</span>
</code></pre></div></div> <p><strong>Gain :</strong> 8× moins de paramètres pour générer les poids d’attention, sans perte de performance.</p> <h3 id="flux-complet-avec-exemple-numérique">Flux Complet avec Exemple Numérique</h3> <p>Prenons un exemple complet avec <strong>N=1000 points, K=16 voisins, C=64 channels, groups=8</strong>.</p> <h4 id="étape-1--projections-q-k-v">Étape 1 : Projections Q, K, V</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Input
</span><span class="n">feat</span><span class="p">:</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>

<span class="c1"># Projections avec normalisation
</span><span class="n">query</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span> <span class="err">→</span> <span class="n">BatchNorm1d</span> <span class="err">→</span> <span class="n">ReLU</span>  <span class="c1"># (1000, 64)
</span><span class="n">key</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span> <span class="err">→</span> <span class="n">BatchNorm1d</span> <span class="err">→</span> <span class="n">ReLU</span>    <span class="c1"># (1000, 64)
</span><span class="n">value</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>                        <span class="c1"># (1000, 64)
</span></code></pre></div></div> <h4 id="étape-2--grouping-des-voisins">Étape 2 : Grouping des Voisins</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Récupération des K voisins via reference_index
</span><span class="n">key_neighbors</span> <span class="o">=</span> <span class="nf">grouping</span><span class="p">(</span><span class="n">reference_index</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">coord</span><span class="p">,</span> <span class="n">with_xyz</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># Shape: (1000, 16, 3+64) = (1000, 16, 67)
# Les 3 premières dims sont les positions relatives
</span>
<span class="n">value_neighbors</span> <span class="o">=</span> <span class="nf">grouping</span><span class="p">(</span><span class="n">reference_index</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">coord</span><span class="p">,</span> <span class="n">with_xyz</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c1"># Shape: (1000, 16, 64)
</span></code></pre></div></div> <p><strong>Note :</strong> <code class="language-plaintext highlighter-rouge">reference_index</code> (N, K) contient les indices des K voisins pour chaque point, pré-calculés dans <code class="language-plaintext highlighter-rouge">BlockSequence</code>.</p> <h4 id="étape-3--séparation-positions--features">Étape 3 : Séparation Positions / Features</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">relative_positions</span> <span class="o">=</span> <span class="n">key_neighbors</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>  <span class="c1"># (1000, 16, 3)
</span><span class="n">key_neighbors</span> <span class="o">=</span> <span class="n">key_neighbors</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">3</span><span class="p">:]</span>        <span class="c1"># (1000, 16, 64)
</span></code></pre></div></div> <h4 id="étape-4--encodage-des-positions">Étape 4 : Encodage des Positions</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># MLP sur positions relatives
</span><span class="n">pe_bias</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">relative_positions</span><span class="p">)</span>  <span class="c1"># (1000, 16, 3) → (1000, 16, 64)
# Le MLP transforme les positions 3D en features de dimension C
</span></code></pre></div></div> <p><strong>Exemple pour un point :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Positions relatives de ses 16 voisins
</span><span class="n">relative_positions</span><span class="p">[</span><span class="n">point_0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">],</span>   <span class="c1"># Voisin très proche
</span>    <span class="p">[</span><span class="mf">0.15</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">],</span>  <span class="c1"># Voisin moyen
</span>    <span class="p">[</span><span class="mf">0.50</span><span class="p">,</span> <span class="mf">0.30</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.20</span><span class="p">],</span>  <span class="c1"># Voisin lointain
</span>    <span class="bp">...</span>
<span class="p">]</span>

<span class="c1"># Encodage via MLP
</span><span class="n">pe_bias</span><span class="p">[</span><span class="n">point_0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="p">...],</span>   <span class="c1"># Features pour voisin proche
</span>    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="p">...],</span>   <span class="c1"># Features pour voisin moyen
</span>    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="p">...],</span>   <span class="c1"># Features pour voisin lointain
</span>    <span class="bp">...</span>
<span class="p">]</span>
</code></pre></div></div> <h4 id="étape-5--application-position-encoding">Étape 5 : Application Position Encoding</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sur la relation Q-K
</span><span class="n">relation_qk</span> <span class="o">=</span> <span class="p">(</span><span class="n">key_neighbors</span> <span class="o">-</span> <span class="n">query</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">pe_bias</span>
<span class="c1"># Shape: (1000, 16, 64)
</span>
<span class="c1"># Sur les values (NOUVEAU dans PTv2 !)
</span><span class="n">value_with_pos</span> <span class="o">=</span> <span class="n">value_neighbors</span> <span class="o">+</span> <span class="n">pe_bias</span>
<span class="c1"># Shape: (1000, 16, 64)
</span></code></pre></div></div> <h4 id="étape-6--génération-des-poids-dattention">Étape 6 : Génération des Poids d’Attention</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># MLP contenant GroupedLinear
</span><span class="n">attention_scores</span> <span class="o">=</span> <span class="nf">weight_encoding</span><span class="p">(</span><span class="n">relation_qk</span><span class="p">)</span>
<span class="c1"># Shape: (1000, 16, 64) → (1000, 16, 8)
</span>
<span class="c1"># Normalization: softmax sur les voisins
</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Shape: (1000, 16, 8)
# Pour chaque point, les poids des 16 voisins somment à 1 (par groupe)
</span></code></pre></div></div> <h4 id="étape-7--masking">Étape 7 : Masking</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mask</span> <span class="o">=</span> <span class="nf">sign</span><span class="p">(</span><span class="n">reference_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (1000, 16)
</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_weights</span> <span class="o">*</span> <span class="n">mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Shape: (1000, 16, 8)
# Les poids des voisins invalides (-1) sont mis à 0
</span></code></pre></div></div> <h4 id="étape-8--agrégation-par-groupes">Étape 8 : Agrégation par Groupes</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reshape values en groupes
</span><span class="n">value_grouped</span> <span class="o">=</span> <span class="n">value_with_pos</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="c1"># Shape: (N, K, groups, C/groups)
</span>
<span class="c1"># Préparation des poids pour broadcasting
</span><span class="n">attention_exp</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Shape: (1000, 16, 8, 1)
</span>
<span class="c1"># Multiplication groupe par groupe
</span><span class="n">weighted</span> <span class="o">=</span> <span class="n">value_grouped</span> <span class="o">*</span> <span class="n">attention_exp</span>
<span class="c1"># Shape: (1000, 16, 8, 8)
</span>
<span class="c1"># Agrégation sur les K=16 voisins
</span><span class="n">aggregated</span> <span class="o">=</span> <span class="n">weighted</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Shape: (1000, 8, 8)
</span>
<span class="c1"># Flatten
</span><span class="n">output</span> <span class="o">=</span> <span class="n">aggregated</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="c1"># Shape: (1000, 64)
</span></code></pre></div></div> <p><strong>Visualisation pour un point avec 3 voisins :</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Point central ◉ avec 3 voisins:

Voisin 1 ●₁: value = [v₁⁰, v₁¹, ..., v₁⁶³]
    → Découpe en 8 groupes de 8 dims
    → Poids: [w₁⁰=0.5, w₁¹=0.3, ..., w₁⁷=0.2]
    
Voisin 2 ●₂: value = [v₂⁰, v₂¹, ..., v₂⁶³]
    → Découpe en 8 groupes de 8 dims
    → Poids: [w₂⁰=0.3, w₂¹=0.4, ..., w₂⁷=0.5]
    
Voisin 3 ●₃: value = [v₃⁰, v₃¹, ..., v₃⁶³]
    → Découpe en 8 groupes de 8 dims
    → Poids: [w₃⁰=0.2, w₃¹=0.3, ..., w₃⁷=0.3]

Agrégation pour le groupe g=0 (dims 0-7):
output[groupe_0] = w₁⁰ × value₁[0:8] + w₂⁰ × value₂[0:8] + w₃⁰ × value₃[0:8]
                 = 0.5 × [v₁⁰,...,v₁⁷] + 0.3 × [v₂⁰,...,v₂⁷] + 0.2 × [v₃⁰,...,v₃⁷]

... répété pour les 8 groupes
</code></pre></div></div> <hr/> <p>Voici la section comparative enrichie pour GroupedVectorAttention :</p> <hr/> <h2 id="groupedvectorattention--attention-locale-enrichie-1">GroupedVectorAttention : Attention Locale Enrichie</h2> <h3 id="vue-densemble-1">Vue d’ensemble</h3> <p><code class="language-plaintext highlighter-rouge">GroupedVectorAttention</code> est le cœur de PTv2, avec plusieurs améliorations par rapport à PTv1.</p> <figure> <picture> <img src="/assets/img/poinTransformerV2/groupedVectorAttention.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="comparaison-détaillée-avec-ptv1">Comparaison détaillée avec PTv1</h3> <table> <thead> <tr> <th>Aspect</th> <th>PTv1 (PointTransformerLayer)</th> <th>PTv2 (GroupedVectorAttention)</th> </tr> </thead> <tbody> <tr> <td><strong>Projections Q, K, V</strong></td> <td>Simple Linear</td> <td>Linear + <strong>BatchNorm1d + ReLU</strong></td> </tr> <tr> <td><strong>Position Encoding</strong></td> <td>Additif uniquement</td> <td>Additif (+ option multiplicatif)</td> </tr> <tr> <td><strong>PE sur values</strong></td> <td>❌ Non</td> <td>✅ <strong>Oui</strong></td> </tr> <tr> <td><strong>Masking voisins invalides</strong></td> <td>❌ Non (assume tous valides)</td> <td>✅ <strong>Oui</strong></td> </tr> <tr> <td><strong>Weight generation</strong></td> <td>MLP standard (C×C/G params)</td> <td><strong>GroupedLinear</strong> (C params seulement)</td> </tr> <tr> <td><strong>Normalisation</strong></td> <td>Après weight encoding</td> <td><strong>Avant et après</strong> attention</td> </tr> </tbody> </table> <h3 id="innovation-1--normalisation-des-projections-q-k-v">Innovation 1 : Normalisation des Projections Q, K, V</h3> <p><strong>PTv1 :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Projections simples sans normalisation
</span><span class="n">self</span><span class="p">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">mid_planes</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">linear_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">mid_planes</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">linear_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">)</span>

<span class="c1"># Usage
</span><span class="n">query</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_q</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>  <span class="c1"># (N, C)
</span><span class="n">key</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_k</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>    <span class="c1"># (N, C)
</span><span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_v</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>  <span class="c1"># (N, C)
</span></code></pre></div></div> <p><strong>PTv2 :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Projections avec normalisation et activation
</span><span class="n">self</span><span class="p">.</span><span class="n">linear_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_channels</span><span class="p">,</span> <span class="n">embed_channels</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">),</span>
    <span class="nc">PointBatchNorm</span><span class="p">(</span><span class="n">embed_channels</span><span class="p">),</span>  <span class="c1"># Normalisation !
</span>    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>            <span class="c1"># Activation !
</span><span class="p">)</span>
<span class="c1"># Idem pour linear_k
</span>
<span class="c1"># Usage
</span><span class="n">query</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_q</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>  <span class="c1"># (N, C) - normalisé et activé
</span></code></pre></div></div> <p><strong>Pourquoi c’est important ?</strong></p> <p>La normalisation des Q, K stabilise l’entraînement en évitant des valeurs extrêmes dans la relation Q-K :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sans normalisation (PTv1)
</span><span class="n">query</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>  <span class="c1"># Peut avoir de grandes variations
</span><span class="n">key</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>    
<span class="n">relation_qk</span> <span class="o">=</span> <span class="n">key</span> <span class="o">-</span> <span class="n">query</span>  <span class="c1"># Peut exploser en magnitude !
</span>
<span class="c1"># Avec normalisation (PTv2)
</span><span class="n">query</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span> <span class="err">→</span> <span class="n">BatchNorm</span> <span class="err">→</span> <span class="n">ReLU</span>  <span class="c1"># Contrôlé et stable
</span><span class="n">key</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span> <span class="err">→</span> <span class="n">BatchNorm</span> <span class="err">→</span> <span class="n">ReLU</span>
<span class="n">relation_qk</span> <span class="o">=</span> <span class="n">key</span> <span class="o">-</span> <span class="n">query</span>  <span class="c1"># Magnitude stable
</span></code></pre></div></div> <p><strong>Impact :</strong> Convergence plus rapide et training plus stable.</p> <hr/> <h3 id="innovation-2--position-encoding-sur-les-values">Innovation 2 : Position Encoding sur les Values</h3> <p><strong>PTv1 :</strong> L’encodage de position n’est ajouté qu’à la relation Q-K</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Code PTv1 (simplifié)
</span><span class="n">relative_positions</span> <span class="o">=</span> <span class="n">neighbor_positions</span> <span class="o">-</span> <span class="n">query_position</span>  <span class="c1"># (N, K, 3)
</span><span class="n">encoded_positions</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">relative_positions</span><span class="p">)</span>               <span class="c1"># (N, K, out_dim)
</span>
<span class="c1"># Application UNIQUEMENT sur relation Q-K
</span><span class="n">relation_qk</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span> <span class="o">-</span> <span class="n">query</span><span class="p">)</span> <span class="o">+</span> <span class="n">encoded_positions</span>
<span class="c1"># Les values ne sont PAS modifiées par la géométrie
</span>
<span class="c1"># Agrégation
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">attention_weights</span> <span class="o">*</span> <span class="n">value</span><span class="p">)</span>
</code></pre></div></div> <p><strong>PTv2 :</strong> L’encodage est ajouté à la relation Q-K <strong>ET aux values</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Code PTv2
</span><span class="n">pe_bias</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">relative_positions</span><span class="p">)</span>  <span class="c1"># (N, K, C)
</span>
<span class="c1"># Sur la relation Q-K (comme PTv1)
</span><span class="n">relation_qk</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span> <span class="o">-</span> <span class="n">query</span><span class="p">)</span> <span class="o">+</span> <span class="n">pe_bias</span>

<span class="c1"># NOUVEAU: aussi sur les values !
</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span> <span class="o">+</span> <span class="n">pe_bias</span>

<span class="c1"># Agrégation (values contiennent maintenant l'info géométrique)
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">attention_weights</span> <span class="o">*</span> <span class="n">value</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Exemple physique comparatif :</strong></p> <p>Imaginons un point représentant le coin d’une table avec 3 voisins :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Voisin 1: dessus de la table    (Δpos = [0, 0, 0.05m])
Voisin 2: pied de table         (Δpos = [0, 0, -0.8m])
Voisin 3: air environnant       (Δpos = [0.5, 0, 0])
</code></pre></div></div> <p><strong>Avec PTv1 :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Encodage position
</span><span class="n">pe</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">])</span> <span class="err">→</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="p">...]</span>  <span class="c1"># proche
</span>
<span class="c1"># Application sur attention seulement
</span><span class="n">relation_qk</span><span class="p">[</span><span class="n">voisin_1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span> <span class="o">-</span> <span class="n">query</span><span class="p">)</span> <span class="o">+</span> <span class="n">pe</span>
<span class="c1"># → Le poids d'attention capture la géométrie
</span>
<span class="c1"># Mais la value reste inchangée !
</span><span class="n">value</span><span class="p">[</span><span class="n">voisin_1</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">semantic_features</span><span class="p">...]</span>  <span class="c1"># Pas d'info géométrique
</span>
<span class="c1"># Agrégation
</span><span class="n">output</span> <span class="o">=</span> <span class="mf">0.6</span> <span class="err">×</span> <span class="n">value</span><span class="p">[</span><span class="n">voisin_1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">...</span>
<span class="c1">#        ↑ poids tient compte de la géométrie
#            ↑ mais la value non !
</span></code></pre></div></div> <p><strong>Avec PTv2 :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Encodage position
</span><span class="n">pe</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">])</span> <span class="err">→</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="p">...]</span>

<span class="c1"># Application sur attention (comme PTv1)
</span><span class="n">relation_qk</span><span class="p">[</span><span class="n">voisin_1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span> <span class="o">-</span> <span class="n">query</span><span class="p">)</span> <span class="o">+</span> <span class="n">pe</span>

<span class="c1"># NOUVEAU: aussi sur la value !
</span><span class="n">value</span><span class="p">[</span><span class="n">voisin_1</span><span class="p">]</span> <span class="o">=</span> <span class="n">value_original</span> <span class="o">+</span> <span class="n">pe</span>
<span class="c1"># → La value contient maintenant l'info : "je suis proche et au-dessus"
</span>
<span class="c1"># Agrégation
</span><span class="n">output</span> <span class="o">=</span> <span class="mf">0.6</span> <span class="err">×</span> <span class="n">value</span><span class="p">[</span><span class="n">voisin_1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">...</span>
<span class="c1">#        ↑ poids géométrique
#            ↑ value aussi géométrique !
</span></code></pre></div></div> <p><strong>Intuition :</strong> PTv2 permet au modèle d’apprendre des patterns du type :</p> <p><em>“Quand j’agrège des features de voisins proches au-dessus de moi (dessus de table), leurs features doivent être modifiées différemment que des voisins lointains en-dessous (pied de table)”</em></p> <p>PTv1 ne pouvait capturer cela que via les poids d’attention - les features agrégées étaient “aveugles” à la géométrie.</p> <hr/> <h3 id="innovation-3--masking-des-voisins-invalides">Innovation 3 : Masking des Voisins Invalides</h3> <p><strong>Problème commun :</strong> Dans un batch, certains points ont moins de K voisins.</p> <p><strong>PTv1 : Pas de masking explicite</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv1 assume que tous les voisins sont valides
# Si un point a seulement 10 voisins au lieu de 16 :
# - Les 10 vrais voisins sont dans reference_index
# - Les 6 restants sont des duplicates du dernier voisin valide
#   (comportement de queryandgroup avec padding)
</span>
<span class="n">attention_weights</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>  <span class="c1"># (N, K, C/G)
# Les poids des voisins "padding" ne sont PAS mis à zéro
# → Ils contribuent avec des features dupliquées
</span></code></pre></div></div> <p><strong>PTv2 : Masking explicite avec -1</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv2 utilise -1 pour marquer les voisins invalides
</span><span class="n">reference_index</span><span class="p">[</span><span class="n">point_42</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">...]</span>
<span class="c1">#                            ↑ valides  ↑ invalides (padding)
</span>
<span class="c1"># Création du masque
</span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sign</span><span class="p">(</span><span class="n">reference_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (N, K)
# sign(-1 + 1) = sign(0) = 0  → voisin invalide
# sign(i + 1)  = sign(&gt;0) = 1 → voisin valide
</span>
<span class="c1"># Application sur les poids
</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_weights</span> <span class="o">*</span> <span class="n">mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Les poids des voisins invalides deviennent exactement 0
</span></code></pre></div></div> <p><strong>Comparaison sur un exemple :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Point isolé avec 3 vrais voisins sur K=8
</span>
<span class="c1"># PTv1 behavior
</span><span class="n">reference_index</span> <span class="o">=</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>  <span class="c1"># duplicates du dernier
</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.20</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>
<span class="c1"># Les voisins "padding" (indices 3-7) ont des poids non-nuls
# → Agrégation polluée par 5× la même feature (voisin 8)
</span>
<span class="c1"># PTv2 behavior
</span><span class="n">reference_index</span> <span class="o">=</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># explicit invalid
</span><span class="n">mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.20</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="c1"># Les voisins invalides sont complètement ignorés ✓
</span></code></pre></div></div> <p><strong>Impact :</strong> Plus robuste pour les nuages avec densité variable ou points isolés.</p> <hr/> <h3 id="innovation-4--groupedlinear-vs-mlp-standard">Innovation 4 : GroupedLinear vs MLP Standard</h3> <p><strong>PTv1 : MLP standard pour générer les poids d’attention</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv1
</span><span class="n">self</span><span class="p">.</span><span class="n">linear_w</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="n">mid_planes</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">mid_planes</span><span class="p">,</span> <span class="n">mid_planes</span> <span class="o">//</span> <span class="n">share_planes</span><span class="p">),</span>  <span class="c1"># C → C/G
</span>    <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="n">mid_planes</span> <span class="o">//</span> <span class="n">share_planes</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">out_planes</span> <span class="o">//</span> <span class="n">share_planes</span><span class="p">,</span> <span class="n">out_planes</span> <span class="o">//</span> <span class="n">share_planes</span><span class="p">)</span>  <span class="c1"># C/G → C/G
</span><span class="p">)</span>

<span class="c1"># Paramètres totaux pour C=64, G=8:
# Première Linear: 64 × 8 = 512 paramètres
# Deuxième Linear: 8 × 8 = 64 paramètres
# Total: ~576 paramètres
</span></code></pre></div></div> <p><strong>PTv2 : GroupedLinear</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv2
</span><span class="n">self</span><span class="p">.</span><span class="n">weight_encoding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="nc">GroupedLinear</span><span class="p">(</span><span class="n">embed_channels</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">groups</span><span class="p">),</span>  <span class="c1"># C → G
</span>    <span class="nc">PointBatchNorm</span><span class="p">(</span><span class="n">groups</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>  <span class="c1"># G → G
</span><span class="p">)</span>

<span class="c1"># Paramètres totaux pour C=64, G=8:
# GroupedLinear: 64 paramètres (vecteur partagé)
# Linear: 8 × 8 = 64 paramètres
# Total: ~128 paramètres
</span></code></pre></div></div> <p><strong>Réduction : 576 → 128 paramètres (4.5× moins !)</strong></p> <hr/> <h3 id="innovation-5--architecture-de-normalisation">Innovation 5 : Architecture de Normalisation</h3> <p><strong>PTv1 :</strong> Normalisation minimale</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv1 - Pas de normalisation sur les projections Q, K, V
</span><span class="n">query</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Pas normalisé
</span><span class="n">key</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">value</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Normalisation seulement dans le MLP des poids
</span><span class="n">attention_scores</span> <span class="o">=</span> <span class="nc">MLP_with_BatchNorm</span><span class="p">(</span><span class="n">relation_qk</span><span class="p">)</span>
</code></pre></div></div> <p><strong>PTv2 :</strong> Normalisation extensive</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv2 - Normalisation partout
</span><span class="n">query</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="err">→</span> <span class="n">BatchNorm</span> <span class="err">→</span> <span class="n">ReLU</span>  <span class="c1"># Normalisé
</span><span class="n">key</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="err">→</span> <span class="n">BatchNorm</span> <span class="err">→</span> <span class="n">ReLU</span>
<span class="n">value</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Pas d'activation (reste linéaire)
</span>
<span class="c1"># Position encoding aussi normalisé
</span><span class="n">pe_bias</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span> <span class="err">→</span> <span class="n">BatchNorm</span> <span class="err">→</span> <span class="n">ReLU</span> <span class="err">→</span> <span class="n">Linear</span>

<span class="c1"># Weight encoding aussi normalisé
</span><span class="n">attention_scores</span> <span class="o">=</span> <span class="n">GroupedLinear</span> <span class="err">→</span> <span class="n">BatchNorm</span> <span class="err">→</span> <span class="n">ReLU</span> <span class="err">→</span> <span class="n">Linear</span>
</code></pre></div></div> <p><strong>Impact :</strong> Training plus stable, convergence plus rapide, moins sensible aux hyperparamètres.</p> <hr/> <h3 id="flux-complet-comparatif">Flux Complet Comparatif</h3> <p><strong>PTv1 :</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Q, K, V = Linear(feat)
2. Grouper K voisins (K-NN à chaque couche)
3. PE = MLP(relative_pos)
4. relation = (K - Q) + PE
5. weights = MLP_standard(relation) → softmax
6. output = sum(weights × V)
</code></pre></div></div> <p><strong>PTv2 :</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Q, K = Linear(feat) → BatchNorm → ReLU
   V = Linear(feat)
2. Grouper K voisins (référence pré-calculée)
3. PE = MLP_normalized(relative_pos)
4. relation = (K - Q) + PE
5. V = V + PE  ← NOUVEAU
6. weights = GroupedLinear(relation) → softmax
7. mask = sign(ref_index + 1)  ← NOUVEAU
8. weights = weights × mask
9. output = sum(weights × V)
</code></pre></div></div> <hr/> <h3 id="tableau-récapitulatif">Tableau Récapitulatif</h3> <table> <thead> <tr> <th>Innovation</th> <th>PTv1</th> <th>PTv2</th> <th>Gain</th> </tr> </thead> <tbody> <tr> <td><strong>Normalisation Q/K/V</strong></td> <td>❌ Non</td> <td>✅ Oui</td> <td>Stabilité training</td> </tr> <tr> <td><strong>PE sur values</strong></td> <td>❌ Non</td> <td>✅ Oui</td> <td>Features géométriques</td> </tr> <tr> <td><strong>Masking invalides</strong></td> <td>❌ Non</td> <td>✅ Oui</td> <td>Robustesse densité variable</td> </tr> <tr> <td><strong>Paramètres weight gen</strong></td> <td>~576</td> <td>~128</td> <td>4.5× réduction</td> </tr> <tr> <td><strong>K-NN par couche</strong></td> <td>Oui</td> <td>Non (pré-calc)</td> <td>~6× speedup</td> </tr> </tbody> </table> <p>PTv2 améliore donc <strong>significativement</strong> l’attention locale tout en réduisant les paramètres et le coût computationnel ! 🎯</p> <h1 id="block-et-blocksequence--architecture-résiduelle">Block et BlockSequence : Architecture Résiduelle</h1> <h2 id="block--residual-block-avec-droppath">Block : Residual Block avec DropPath</h2> <p>Le <code class="language-plaintext highlighter-rouge">Block</code> de PTv2 encapsule <code class="language-plaintext highlighter-rouge">GroupedVectorAttention</code> dans une structure résiduelle similaire à ResNet, avec une innovation clé : <strong>DropPath</strong>.</p> <figure> <picture> <img src="/assets/img/poinTransformerV2/block.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="comparaison-avec-ptv1">Comparaison avec PTv1</h3> <table> <thead> <tr> <th>Aspect</th> <th>PTv1 (PointTransformerBlock)</th> <th>PTv2 (Block)</th> </tr> </thead> <tbody> <tr> <td><strong>Structure</strong></td> <td>Linear → Attention → Linear + Skip</td> <td>Linear → Attention → Linear + Skip</td> </tr> <tr> <td><strong>Régularisation</strong></td> <td>Dropout uniquement</td> <td><strong>DropPath</strong> + Dropout</td> </tr> <tr> <td><strong>Normalisation</strong></td> <td>3× BatchNorm</td> <td>3× BatchNorm (identique)</td> </tr> <tr> <td><strong>Skip connection</strong></td> <td>Simple addition</td> <td>Addition avec <strong>DropPath</strong></td> </tr> </tbody> </table> <h3 id="architecture-détaillée">Architecture Détaillée</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input features (N, C)
    ↓
[Linear + BatchNorm1d + ReLU]  ← Pre-activation (expansion)
    ↓
[GroupedVectorAttention]  ← Attention locale sur K voisins
    ↓
[BatchNorm1d + ReLU]  ← Post-attention normalization
    ↓
[Linear + BatchNorm1d]  ← Projection
    ↓
[DropPath]  ← Régularisation stochastique (NOUVEAU)
    ↓
[+ Skip Connection]  ← Connexion résiduelle
    ↓
[ReLU]  ← Activation finale
    ↓
Output features (N, C)
</code></pre></div></div> <h3 id="droppath--stochastic-depth">DropPath : Stochastic Depth</h3> <p><strong>DropPath</strong> (Stochastic Depth) est une technique de régularisation qui <strong>dropout des chemins entiers</strong> dans un réseau résiduel, plutôt que des neurones individuels.</p> <p><strong>Dropout classique vs DropPath :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Dropout classique (agit sur les features)
</span><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="nf">random</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">p</span>  <span class="c1"># Masque aléatoire par élément
</span>    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">mask</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="nf">dropout</span><span class="p">(</span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="c1"># Certaines features de f(x) sont mises à 0
</span>

<span class="c1"># DropPath (agit sur le chemin entier)
</span><span class="k">def</span> <span class="nf">drop_path</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">training</span> <span class="ow">and</span> <span class="nf">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">p</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>  <span class="c1"># Tout le chemin est ignoré !
</span>    <span class="k">return</span> <span class="n">x</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="nf">drop_path</span><span class="p">(</span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="c1"># Soit tout f(x) est gardé, soit tout est ignoré
</span></code></pre></div></div> <p><strong>Fonctionnement en pratique :</strong></p> <p>Durant l’entraînement, avec probabilité <code class="language-plaintext highlighter-rouge">drop_path_rate</code> (typiquement 0.1), on saute complètement le bloc transformé :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sans DropPath (PTv1)
</span><span class="n">feat_transformed</span> <span class="o">=</span> <span class="n">Linear</span> <span class="err">→</span> <span class="n">Attention</span> <span class="err">→</span> <span class="n">Linear</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">identity</span> <span class="o">+</span> <span class="n">feat_transformed</span>  <span class="c1"># Toujours calculé
</span>
<span class="c1"># Avec DropPath (PTv2)
</span><span class="n">feat_transformed</span> <span class="o">=</span> <span class="n">Linear</span> <span class="err">→</span> <span class="n">Attention</span> <span class="err">→</span> <span class="n">Linear</span>

<span class="k">if</span> <span class="n">training</span> <span class="ow">and</span> <span class="nf">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">drop_path_rate</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">identity</span>  <span class="c1"># On saute feat_transformed complètement !
</span><span class="k">else</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">identity</span> <span class="o">+</span> <span class="n">feat_transformed</span>

<span class="c1"># À l'inférence
</span><span class="n">output</span> <span class="o">=</span> <span class="n">identity</span> <span class="o">+</span> <span class="n">feat_transformed</span>  <span class="c1"># Toujours actif
</span></code></pre></div></div> <p><strong>Visualisation sur un réseau de 12 blocs :</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Avec drop_path_rate = 0.1 (10% de chance de drop par bloc)

Training iteration 1:
Input → [Block1] → [Block2] → [SKIP] → [Block4] → ... → [SKIP] → [Block12]
        ✓          ✓          ✗          ✓              ✗          ✓
        (réseau de ~10 blocs actifs)

Training iteration 2:
Input → [Block1] → [SKIP] → [Block3] → [Block4] → ... → [Block11] → [Block12]
        ✓          ✗        ✓          ✓                  ✓          ✓
        (réseau de ~11 blocs actifs)

Inference:
Input → [Block1] → [Block2] → [Block3] → [Block4] → ... → [Block11] → [Block12]
        ✓          ✓          ✓          ✓                  ✓          ✓
        (tous les 12 blocs actifs)
</code></pre></div></div> <p><strong>Pourquoi ça marche ?</strong></p> <ol> <li><strong>Régularisation :</strong> Force chaque bloc à être utile indépendamment</li> <li><strong>Gradient flow :</strong> Crée des “chemins courts” pendant l’entraînement</li> <li><strong>Ensemble implicite :</strong> Entraîne effectivement plusieurs sous-réseaux de profondeurs différentes</li> <li><strong>Réduit l’overfitting :</strong> Les blocs ne peuvent pas trop dépendre les uns des autres</li> </ol> <p><strong>Drop Path Rate Scheduler :</strong></p> <p>Dans PTv2, le <code class="language-plaintext highlighter-rouge">drop_path_rate</code> augmente progressivement à travers les couches :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Configuration PTv2 avec drop_path_rate = 0.3
</span><span class="n">enc_depths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>  <span class="c1"># 12 couches au total
</span>
<span class="n">drop_path_rates</span> <span class="o">=</span> <span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="nf">sum</span><span class="p">(</span><span class="n">enc_depths</span><span class="p">))</span>
<span class="c1"># [0.00, 0.03, 0.05, 0.08, 0.11, 0.14, 0.16, 0.19, 0.22, 0.24, 0.27, 0.30]
</span>
<span class="c1"># Les premières couches ont drop_path_rate faible (plus stables)
# Les dernières couches ont drop_path_rate élevé (plus régularisées)
</span></code></pre></div></div> <p><strong>Intuition :</strong> Les couches profondes bénéficient plus de la régularisation car elles ont tendance à overfitter.</p> <hr/> <h2 id="blocksequence--réutilisation-du-k-nn">BlockSequence : Réutilisation du K-NN</h2> <p><code class="language-plaintext highlighter-rouge">BlockSequence</code> empile plusieurs <code class="language-plaintext highlighter-rouge">Block</code> et introduit une optimisation majeure : <strong>partage du reference_index</strong>.</p> <figure> <picture> <img src="/assets/img/poinTransformerV2/blockSequence.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="innovation-clé--k-nn-calculé-une-seule-fois">Innovation Clé : K-NN Calculé Une Seule Fois</h3> <p><strong>Problème PTv1 :</strong></p> <p>Dans PTv1, chaque <code class="language-plaintext highlighter-rouge">PointTransformerLayer</code> recalcule les K plus proches voisins via K-NN :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv1 - Dans PointTransformerLayer.forward()
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">pxo</span><span class="p">):</span>
    <span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">o</span> <span class="o">=</span> <span class="n">pxo</span>
    
    <span class="c1"># K-NN calculé À CHAQUE COUCHE
</span>    <span class="n">x_k</span> <span class="o">=</span> <span class="n">pointops</span><span class="p">.</span><span class="nf">queryandgroup</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nsample</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">x_k</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">use_xyz</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">x_v</span> <span class="o">=</span> <span class="n">pointops</span><span class="p">.</span><span class="nf">queryandgroup</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nsample</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">x_v</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">use_xyz</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="c1"># ...
</span></code></pre></div></div> <p>Pour un bloc avec 6 couches <code class="language-plaintext highlighter-rouge">PointTransformerLayer</code>, on fait <strong>6 fois</strong> la même recherche K-NN !</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Bloc avec 6 couches PTv1:
Layer 1: K-NN (N points, find K=16 neighbors) → O(N log N)
Layer 2: K-NN (N points, find K=16 neighbors) → O(N log N)
Layer 3: K-NN (N points, find K=16 neighbors) → O(N log N)
Layer 4: K-NN (N points, find K=16 neighbors) → O(N log N)
Layer 5: K-NN (N points, find K=16 neighbors) → O(N log N)
Layer 6: K-NN (N points, find K=16 neighbors) → O(N log N)

Coût total: 6 × O(N log N)
</code></pre></div></div> <p><strong>Solution PTv2 :</strong></p> <p>Dans PTv2, <code class="language-plaintext highlighter-rouge">BlockSequence</code> calcule le K-NN <strong>une seule fois</strong> au début et tous les <code class="language-plaintext highlighter-rouge">Block</code> partagent le même <code class="language-plaintext highlighter-rouge">reference_index</code> :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv2 - Dans BlockSequence.forward()
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">points</span><span class="p">):</span>
    <span class="n">coord</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">points</span>
    
    <span class="c1"># K-NN calculé UNE SEULE FOIS au début
</span>    <span class="n">reference_index</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pointops</span><span class="p">.</span><span class="nf">knn_query</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">neighbours</span><span class="p">,</span> <span class="n">coord</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
    <span class="c1"># reference_index: (N, K) - indices des K voisins pour chaque point
</span>    
    <span class="c1"># Tous les blocks partagent le même reference_index
</span>    <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">blocks</span><span class="p">:</span>
        <span class="n">points</span> <span class="o">=</span> <span class="nf">block</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">reference_index</span><span class="p">)</span>  <span class="c1"># Pas de recalcul !
</span>    
    <span class="k">return</span> <span class="n">points</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Bloc avec 6 couches PTv2:
K-NN (une fois): O(N log N)
Layer 1: Utilise reference_index → O(1) lookup
Layer 2: Utilise reference_index → O(1) lookup
Layer 3: Utilise reference_index → O(1) lookup
Layer 4: Utilise reference_index → O(1) lookup
Layer 5: Utilise reference_index → O(1) lookup
Layer 6: Utilise reference_index → O(1) lookup

Coût total: O(N log N)  ← 6× plus rapide !
</code></pre></div></div> <h3 id="pourquoi-cest-valide-">Pourquoi c’est Valide ?</h3> <p><strong>Question :</strong> Peut-on vraiment réutiliser les mêmes voisins à travers toutes les couches ?</p> <p><strong>Réponse :</strong> <strong>OUI</strong>, car dans <code class="language-plaintext highlighter-rouge">BlockSequence</code>, les <strong>positions ne changent pas</strong> !</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Dans Block.forward()
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">reference_index</span><span class="p">):</span>
    <span class="n">coord</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">points</span>
    
    <span class="c1"># coord (positions) reste INCHANGÉ à travers le bloc
</span>    <span class="n">feat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>  <span class="c1"># Seulement les features changent
</span>    <span class="n">feat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn</span><span class="p">(</span><span class="n">feat</span><span class="p">,</span> <span class="n">coord</span><span class="p">,</span> <span class="n">reference_index</span><span class="p">)</span>  <span class="c1"># coord fixe
</span>    <span class="n">feat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc3</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>
    <span class="c1"># ...
</span>    
    <span class="k">return</span> <span class="p">[</span><span class="n">coord</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">offset</span><span class="p">]</span>  <span class="c1"># coord identique en sortie
</span></code></pre></div></div> <p>Les positions 3D (<code class="language-plaintext highlighter-rouge">coord</code>) sont <strong>constantes</strong> dans un <code class="language-plaintext highlighter-rouge">BlockSequence</code> - seules les <strong>features</strong> évoluent. Les K plus proches voisins restent donc identiques géométriquement !</p> <p><strong>Cas où on DOIT recalculer le K-NN :</strong></p> <p>Les positions changent uniquement lors des transitions entre niveaux de l’architecture (downsampling/upsampling) :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Encoder
</span><span class="n">points</span> <span class="o">=</span> <span class="nc">BlockSequence</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>  <span class="c1"># Positions fixes, K-NN partagé ✓
</span><span class="n">points</span> <span class="o">=</span> <span class="nc">GridPool</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>        <span class="c1"># Positions changent (downsampling) ✗
</span><span class="n">points</span> <span class="o">=</span> <span class="nc">BlockSequence</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>  <span class="c1"># Nouvelles positions → nouveau K-NN ✓
</span>
<span class="c1"># Decoder
</span><span class="n">points</span> <span class="o">=</span> <span class="nc">UnpoolWithSkip</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">skip</span><span class="p">)</span>  <span class="c1"># Positions changent (upsampling) ✗
</span><span class="n">points</span> <span class="o">=</span> <span class="nc">BlockSequence</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>         <span class="c1"># Nouvelles positions → nouveau K-NN ✓
</span></code></pre></div></div> <h3 id="comparaison-des-coûts">Comparaison des Coûts</h3> <p><strong>Pour un Encoder avec 4 niveaux × 6 couches chacun (24 couches totales) :</strong></p> <table> <thead> <tr> <th>Opération</th> <th>PTv1</th> <th>PTv2</th> <th>Speedup</th> </tr> </thead> <tbody> <tr> <td><strong>K-NN queries</strong></td> <td>24 fois</td> <td>4 fois</td> <td>6×</td> </tr> <tr> <td><strong>K-NN cost</strong></td> <td>24 × O(N log N)</td> <td>4 × O(N log N)</td> <td>6×</td> </tr> <tr> <td><strong>Memory</strong></td> <td>Recalculé chaque fois</td> <td>Stocké et réutilisé</td> <td>-</td> </tr> </tbody> </table> <p><strong>Note :</strong> Le speedup réel dépend du ratio (coût K-NN / coût attention), mais empiriquement PTv2 est ~2-3× plus rapide en pratique sur cette optimisation seule.</p> <h3 id="gestion-du-reference_index">Gestion du reference_index</h3> <p><strong>Structure du reference_index :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># reference_index: (N, K)
# Pour chaque point n ∈ [0, N), contient les indices de ses K voisins
</span>
<span class="c1"># Exemple avec N=5 points, K=3 voisins
</span><span class="n">reference_index</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>    <span class="c1"># Point 0: ses 3 voisins sont les points 1, 2, 4
</span>    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>    <span class="c1"># Point 1: ses 3 voisins sont les points 0, 2, 3
</span>    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>    <span class="c1"># Point 2: ...
</span>    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>    <span class="c1"># Point 3: ...
</span>    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>     <span class="c1"># Point 4: ...
</span><span class="p">]</span>
</code></pre></div></div> <p><strong>Gestion des voisins invalides (padding) :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Point isolé avec seulement 2 vrais voisins (K=3)
</span><span class="n">reference_index</span><span class="p">[</span><span class="n">point_42</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="c1">#                                    ↑ Pas assez de voisins → -1 (invalide)
</span>
<span class="c1"># Le masking dans GroupedVectorAttention gère automatiquement
</span><span class="n">mask</span> <span class="o">=</span> <span class="nf">sign</span><span class="p">(</span><span class="n">reference_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># [1, 1, 0]
</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_weights</span> <span class="o">*</span> <span class="n">mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Le voisin invalide (-1) est ignoré ✓
</span></code></pre></div></div> <h3 id="exemple-complet">Exemple Complet</h3> <p><strong>Configuration :</strong></p> <ul> <li>BlockSequence avec <code class="language-plaintext highlighter-rouge">depth=6</code> (6 blocks)</li> <li><code class="language-plaintext highlighter-rouge">neighbours=16</code> (K=16 voisins)</li> <li>N=1000 points, C=64 channels</li> </ul> <p><strong>Flux :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Input
</span><span class="n">coord</span><span class="p">:</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">feat</span><span class="p">:</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">offset</span><span class="p">:</span> <span class="p">(</span><span class="n">B</span><span class="p">,)</span>

<span class="c1"># Étape 1: K-NN une fois
</span><span class="n">reference_index</span> <span class="o">=</span> <span class="nf">knn_query</span><span class="p">(</span><span class="n">K</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">coord</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
<span class="c1"># reference_index: (1000, 16)
# Pour chaque point: indices de ses 16 voisins
</span>
<span class="c1"># Étape 2: Block 1
</span><span class="n">coord</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="nc">Block_1</span><span class="p">(</span><span class="n">coord</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">reference_index</span><span class="p">)</span>
<span class="c1"># coord inchangé: (1000, 3)
# feat transformé: (1000, 64)
</span>
<span class="c1"># Étape 3: Block 2 (réutilise reference_index)
</span><span class="n">coord</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="nc">Block_2</span><span class="p">(</span><span class="n">coord</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">reference_index</span><span class="p">)</span>
<span class="c1"># coord toujours inchangé: (1000, 3)
# feat transformé: (1000, 64)
</span>
<span class="c1"># Étapes 4-6: Blocks 3-6 (tous réutilisent reference_index)
</span><span class="bp">...</span>

<span class="c1"># Output
</span><span class="n">coord</span><span class="p">:</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="err">←</span> <span class="n">Identique</span> <span class="n">à</span> <span class="n">l</span><span class="sh">'</span><span class="s">input
feat: (1000, 64)  ← Transformé par 6 couches d</span><span class="sh">'</span><span class="n">attention</span>
<span class="n">offset</span><span class="p">:</span> <span class="p">(</span><span class="n">B</span><span class="p">,)</span>      <span class="err">←</span> <span class="n">Identique</span> <span class="n">à</span> <span class="n">l</span><span class="sh">'</span><span class="s">input
</span></code></pre></div></div> <p><strong>Visualisation :</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Point central ◉ à la position (x, y, z)

Au début de BlockSequence:
  K-NN → Trouve ses 16 voisins: ●₁, ●₂, ..., ●₁₆
  Stocke dans reference_index[◉] = [idx₁, idx₂, ..., idx₁₆]

Block 1:
  Attention sur ●₁, ●₂, ..., ●₁₆ (lookup via reference_index)
  → Features de ◉ mises à jour
  → Position de ◉ INCHANGÉE

Block 2:
  Attention sur les MÊMES ●₁, ●₂, ..., ●₁₆ (lookup via reference_index)
  → Features de ◉ encore mises à jour
  → Position de ◉ toujours INCHANGÉE

...

Block 6:
  Attention sur les MÊMES voisins
  → Features finales de ◉
</code></pre></div></div> <p>Les voisins géométriques restent identiques, mais leurs <strong>features évoluent</strong> à chaque couche !</p> <hr/> <h2 id="gvapatchembed--embedding-initial">GVAPatchEmbed : Embedding Initial</h2> <p>Avant de downsampler, PTv2 applique un <code class="language-plaintext highlighter-rouge">GVAPatchEmbed</code> qui enrichit les features à pleine résolution.</p> <figure> <picture> <img src="/assets/img/poinTransformerV2/GVAPatchEmbed.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="rôle">Rôle</h3> <p><strong>GVAPatchEmbed</strong> = Projection linéaire + BlockSequence (sans downsampling)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Input</span><span class="p">:</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">)</span>
    <span class="err">↓</span>
<span class="n">Linear</span> <span class="o">+</span> <span class="n">BatchNorm1d</span> <span class="o">+</span> <span class="n">ReLU</span>
    <span class="err">↓</span>
<span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">embed_channels</span><span class="p">)</span>
    <span class="err">↓</span>
<span class="nc">BlockSequence </span><span class="p">(</span><span class="n">depth</span> <span class="n">blocks</span><span class="p">)</span>
    <span class="err">↓</span>
<span class="n">Output</span><span class="p">:</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">embed_channels</span><span class="p">)</span>
</code></pre></div></div> <h3 id="comparaison-avec-ptv1-1">Comparaison avec PTv1</h3> <table> <thead> <tr> <th>Aspect</th> <th>PTv1</th> <th>PTv2</th> </tr> </thead> <tbody> <tr> <td><strong>Initial embedding</strong></td> <td>❌ Aucun</td> <td>✅ GVAPatchEmbed</td> </tr> <tr> <td><strong>Première opération</strong></td> <td>TransitionDown (downsampling immédiat)</td> <td>GVAPatchEmbed (attention à pleine résolution)</td> </tr> <tr> <td><strong>Philosophy</strong></td> <td>Downsample vite</td> <td>Apprendre des features riches d’abord</td> </tr> </tbody> </table> <p><strong>PTv1 :</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input (N, in_channels)
    ↓
TransitionDown (stride=1)  ← Simple Linear + BN + ReLU
    ↓
PointTransformerBlock
    ↓
TransitionDown (stride=4)  ← Downsampling immédiat
</code></pre></div></div> <p><strong>PTv2 :</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input (N, in_channels)
    ↓
GVAPatchEmbed:
  - Linear + BN + ReLU
  - depth × Block (GroupedVectorAttention)
    ↓
(N, embed_channels)  ← Features riches avant downsampling
    ↓
Encoder 1 (GridPool)  ← Premier downsampling
</code></pre></div></div> <h3 id="pourquoi-cest-important-">Pourquoi c’est Important ?</h3> <p><strong>Analogie avec les CNNs :</strong></p> <p>Dans les CNNs modernes (ResNet, EfficientNet), on a un <strong>“stem”</strong> initial qui traite l’image à haute résolution avant le pooling :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ResNet stem
</span><span class="nc">Input </span><span class="p">(</span><span class="mi">224</span><span class="err">×</span><span class="mi">224</span><span class="p">)</span>
    <span class="err">↓</span>
<span class="n">Conv</span> <span class="mi">7</span><span class="err">×</span><span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span>  <span class="err">→</span> <span class="p">(</span><span class="mi">112</span><span class="err">×</span><span class="mi">112</span><span class="p">)</span>
    <span class="err">↓</span>
<span class="n">MaxPool</span> <span class="mi">3</span><span class="err">×</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span> <span class="err">→</span> <span class="p">(</span><span class="mi">56</span><span class="err">×</span><span class="mi">56</span><span class="p">)</span>
    <span class="err">↓</span>
<span class="n">ResNet</span> <span class="n">blocks</span><span class="bp">...</span>
</code></pre></div></div> <p><strong>PTv2 adopte la même philosophie :</strong></p> <ul> <li>Apprendre des features riches à <strong>pleine résolution</strong> avant de downsampler</li> <li>Permet de capturer des détails fins dès le début</li> <li>Les features initiales de meilleure qualité aident tout le réseau</li> </ul> <h3 id="configuration-typique">Configuration Typique</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv2 default config
</span><span class="nc">GVAPatchEmbed</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>          <span class="c1"># xyz + rgb
</span>    <span class="n">embed_channels</span><span class="o">=</span><span class="mi">48</span><span class="p">,</span>      <span class="c1"># Dimension d'embedding
</span>    <span class="n">groups</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>               <span class="c1"># 6 groupes pour l'attention
</span>    <span class="n">depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>                <span class="c1"># 1 block (peut être plus)
</span>    <span class="n">neighbours</span><span class="o">=</span><span class="mi">8</span>            <span class="c1"># K=8 voisins
</span><span class="p">)</span>
</code></pre></div></div> <p>Avec <code class="language-plaintext highlighter-rouge">depth=1</code>, c’est modeste, mais déjà bénéfique. Certaines variantes utilisent <code class="language-plaintext highlighter-rouge">depth=2</code> pour des features encore plus riches.</p> <h1 id="gridpool--downsampling-par-voxelisation">GridPool : Downsampling par Voxelisation</h1> <h2 id="vue-densemble-2">Vue d’ensemble</h2> <p><code class="language-plaintext highlighter-rouge">GridPool</code> est l’une des innovations majeures de PTv2, remplaçant le <strong>Furthest Point Sampling (FPS)</strong> de PTv1 par une approche basée sur la <strong>voxelisation</strong>.</p> <figure> <picture> <img src="/assets/img/poinTransformerV2/gridPool.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="comparaison--fps-vs-grid-pooling">Comparaison : FPS vs Grid Pooling</h3> <table> <thead> <tr> <th>Aspect</th> <th>PTv1 (FPS)</th> <th>PTv2 (Grid Pooling)</th> </tr> </thead> <tbody> <tr> <td><strong>Complexité</strong></td> <td>O(N²)</td> <td><strong>O(N)</strong></td> </tr> <tr> <td><strong>Méthode</strong></td> <td>Sélection itérative des points les plus éloignés</td> <td>Voxelisation + agrégation</td> </tr> <tr> <td><strong>Déterminisme</strong></td> <td>Non (ordre dépend du point de départ)</td> <td>Oui (basé sur la grille)</td> </tr> <tr> <td><strong>Speedup</strong></td> <td>-</td> <td><strong>3-5× plus rapide</strong></td> </tr> <tr> <td><strong>Couverture spatiale</strong></td> <td>Maximise les distances</td> <td>Uniforme par design</td> </tr> <tr> <td><strong>Unpooling</strong></td> <td>K-NN interpolation (coûteux)</td> <td><strong>Map unpooling</strong> (gratuit)</td> </tr> </tbody> </table> <hr/> <h2 id="problème-avec-fps-ptv1">Problème avec FPS (PTv1)</h2> <h3 id="algorithme-fps">Algorithme FPS</h3> <p><strong>Furthest Point Sampling</strong> sélectionne itérativement les points les plus éloignés :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">furthest_point_sampling</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
    <span class="c1"># 1. Choisir un point de départ aléatoire
</span>    <span class="n">selected</span> <span class="o">=</span> <span class="p">[</span><span class="n">random_point</span><span class="p">]</span>
    
    <span class="c1"># 2. Répéter jusqu'à avoir num_samples points
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_samples</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Pour chaque point non sélectionné:
</span>        <span class="c1">#   - Calculer sa distance au point sélectionné le plus proche
</span>        <span class="n">distances</span> <span class="o">=</span> <span class="p">[</span><span class="nf">min_distance_to_selected</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">points</span><span class="p">]</span>
        
        <span class="c1"># Sélectionner le point le plus éloigné
</span>        <span class="n">farthest</span> <span class="o">=</span> <span class="nf">argmax</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
        <span class="n">selected</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">farthest</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">selected</span>
</code></pre></div></div> <p><strong>Exemple visuel :</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Nuage de 16 points, on veut en garder 4:

Étape 1: Point aléatoire
    ●────●────●────●
    │    │    │    │
    ◉────●────●────●   ← Point de départ
    │    │    │    │
    ●────●────●────●
    │    │    │    │
    ●────●────●────●

Étape 2: Point le plus éloigné
    ◉────●────●────●
    │    │    │    │
    ◉────●────●────●
    │    │    │    │
    ●────●────●────●
    │    │    │    │
    ●────●────●────◉   ← Coin opposé (le plus loin)

Étape 3: Encore le plus éloigné
    ◉────●────●────◉
    │    │    │    │
    ◉────●────●────●
    │    │    │    │
    ●────●────●────●
    │    │    │    │
    ●────●────●────◉

Étape 4: Dernier point
    ◉────●────●────◉
    │    │    │    │
    ◉────●────●────●
    │    │    │    │
    ●────●────●────●
    │    │    │    │
    ◉────●────●────◉   ← 4 points bien espacés
</code></pre></div></div> <p><strong>Problèmes :</strong></p> <ol> <li><strong>Complexité O(N²)</strong> : Pour sélectionner M points parmi N, on doit calculer O(M × N) distances</li> <li><strong>Coût élevé</strong> : Pour N=100k points → M=25k points, c’est 2.5 milliards de calculs de distance !</li> <li><strong>Non-déterministe</strong> : Dépend du point de départ aléatoire</li> <li><strong>Pas de mapping</strong> : On perd l’information de correspondance pour l’unpooling</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Coût FPS pour downsampler 4 niveaux dans PTv1
</span><span class="n">Level</span> <span class="mi">1</span><span class="p">:</span> <span class="nc">FPS</span><span class="p">(</span><span class="n">N</span> <span class="err">→</span> <span class="n">N</span><span class="o">/</span><span class="mi">4</span><span class="p">)</span>     <span class="err">→</span> <span class="nc">O</span><span class="p">(</span><span class="n">N</span><span class="err">²</span><span class="p">)</span>
<span class="n">Level</span> <span class="mi">2</span><span class="p">:</span> <span class="nc">FPS</span><span class="p">(</span><span class="n">N</span><span class="o">/</span><span class="mi">4</span> <span class="err">→</span> <span class="n">N</span><span class="o">/</span><span class="mi">16</span><span class="p">)</span>  <span class="err">→</span> <span class="nc">O</span><span class="p">((</span><span class="n">N</span><span class="o">/</span><span class="mi">4</span><span class="p">)</span><span class="err">²</span><span class="p">)</span>
<span class="n">Level</span> <span class="mi">3</span><span class="p">:</span> <span class="nc">FPS</span><span class="p">(</span><span class="n">N</span><span class="o">/</span><span class="mi">16</span> <span class="err">→</span> <span class="n">N</span><span class="o">/</span><span class="mi">64</span><span class="p">)</span> <span class="err">→</span> <span class="nc">O</span><span class="p">((</span><span class="n">N</span><span class="o">/</span><span class="mi">16</span><span class="p">)</span><span class="err">²</span><span class="p">)</span>
<span class="n">Level</span> <span class="mi">4</span><span class="p">:</span> <span class="nc">FPS</span><span class="p">(</span><span class="n">N</span><span class="o">/</span><span class="mi">64</span> <span class="err">→</span> <span class="n">N</span><span class="o">/</span><span class="mi">256</span><span class="p">)</span><span class="err">→</span> <span class="nc">O</span><span class="p">((</span><span class="n">N</span><span class="o">/</span><span class="mi">64</span><span class="p">)</span><span class="err">²</span><span class="p">)</span>

<span class="n">Total</span><span class="p">:</span> <span class="nc">O</span><span class="p">(</span><span class="n">N</span><span class="err">²</span><span class="p">)</span> <span class="n">dominant</span> <span class="n">pour</span> <span class="n">les</span> <span class="n">premiers</span> <span class="n">niveaux</span>
</code></pre></div></div> <hr/> <h2 id="solution--grid-pooling-ptv2">Solution : Grid Pooling (PTv2)</h2> <h3 id="principe--voxelisation">Principe : Voxelisation</h3> <p>Au lieu de sélectionner des points, on <strong>partitionne l’espace en voxels</strong> (cubes 3D) et on agrège tous les points d’un même voxel.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Nuage de points + grille 3D:

        grid_size = 0.5m
        
    ┌─────┬─────┬─────┬─────┐
    │ ●   │     │  ●  │     │
    │   ● │     │     │  ●  │
    ├─────┼─────┼─────┼─────┤
    │     │ ●●  │     │     │
    │ ●   │  ●  │  ●  │     │
    ├─────┼─────┼─────┼─────┤
    │  ●  │     │ ●   │  ●  │
    │     │  ●  │   ● │     │
    ├─────┼─────┼─────┼─────┤
    │     │     │     │ ●●  │
    │  ●  │  ●  │  ●  │  ●  │
    └─────┴─────┴─────┴─────┘

Après voxelisation (1 point par voxel):

    ┌─────┬─────┬─────┬─────┐
    │  ◉  │     │  ◉  │  ◉  │
    │     │     │     │     │
    ├─────┼─────┼─────┼─────┤
    │  ◉  │  ◉  │  ◉  │     │
    │     │     │     │     │
    ├─────┼─────┼─────┼─────┤
    │  ◉  │  ◉  │  ◉  │  ◉  │
    │     │     │     │     │
    ├─────┼─────┼─────┼─────┤
    │  ◉  │  ◉  │  ◉  │  ◉  │
    │     │     │     │     │
    └─────┴─────┴─────┴─────┘
</code></pre></div></div> <h3 id="algorithme-étape-par-étape">Algorithme Étape par Étape</h3> <h4 id="étape-1--projection-des-features">Étape 1 : Projection des Features</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Input
</span><span class="n">coord</span><span class="p">:</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">feat</span><span class="p">:</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">)</span>

<span class="c1"># Projection linéaire + normalisation
</span><span class="n">feat</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>  <span class="c1"># (N, in_channels) → (N, out_channels)
</span><span class="n">feat</span> <span class="o">=</span> <span class="nc">BatchNorm1d</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span> <span class="o">+</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>
<span class="c1"># feat: (N, out_channels)
</span></code></pre></div></div> <h4 id="étape-2--calcul-du-point-de-départ-par-nuage">Étape 2 : Calcul du Point de Départ par Nuage</h4> <p>Chaque nuage dans le batch a besoin d’un point de référence (coin minimal) :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Conversion offset → batch
</span><span class="n">batch</span> <span class="o">=</span> <span class="nf">offset2batch</span><span class="p">(</span><span class="n">offset</span><span class="p">)</span>  <span class="c1"># (N,)
# batch[i] indique à quel nuage appartient le point i
</span>
<span class="c1"># Calcul du coin minimal de chaque nuage
</span><span class="n">start</span> <span class="o">=</span> <span class="nf">segment_csr</span><span class="p">(</span><span class="n">coord</span><span class="p">,</span> <span class="n">batch_ptr</span><span class="p">,</span> <span class="nb">reduce</span><span class="o">=</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># start: (B, 3) - coin minimal (x_min, y_min, z_min) de chaque nuage
</span></code></pre></div></div> <p><strong>Exemple :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 2 nuages dans le batch
</span><span class="n">Nuage</span> <span class="mi">1</span><span class="p">:</span> <span class="n">points</span> <span class="n">aux</span> <span class="n">positions</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">]]</span>
    <span class="err">→</span> <span class="n">start</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># Minimum de chaque dimension
</span>
<span class="n">Nuage</span> <span class="mi">2</span><span class="p">:</span> <span class="n">points</span> <span class="n">aux</span> <span class="n">positions</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">]]</span>
    <span class="err">→</span> <span class="n">start</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
</code></pre></div></div> <h4 id="étape-3--voxelisation">Étape 3 : Voxelisation</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Normalisation des coordonnées par rapport au début de chaque nuage
</span><span class="n">coord_normalized</span> <span class="o">=</span> <span class="n">coord</span> <span class="o">-</span> <span class="n">start</span><span class="p">[</span><span class="n">batch</span><span class="p">]</span>  <span class="c1"># (N, 3)
</span>
<span class="c1"># Assignation à une grille avec voxels de taille grid_size
</span><span class="n">cluster</span> <span class="o">=</span> <span class="nf">voxel_grid</span><span class="p">(</span>
    <span class="n">pos</span><span class="o">=</span><span class="n">coord_normalized</span><span class="p">,</span> 
    <span class="n">size</span><span class="o">=</span><span class="n">grid_size</span><span class="p">,</span>  <span class="c1"># ex: 0.06m
</span>    <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
    <span class="n">start</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="c1"># cluster: (N,) - ID du voxel pour chaque point
</span></code></pre></div></div> <p><strong>Exemple avec grid_size=1.0 :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Points d'un nuage (après normalisation)
</span><span class="n">points</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>  <span class="c1"># Voxel (0, 0, 0)
</span>    <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>  <span class="c1"># Voxel (0, 0, 0)
</span>    <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>  <span class="c1"># Voxel (1, 0, 0)
</span>    <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>  <span class="c1"># Voxel (1, 1, 0)
</span>    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>  <span class="c1"># Voxel (0, 1, 0)
</span><span class="p">]</span>

<span class="c1"># Calcul du voxel ID
</span><span class="n">voxel_id</span> <span class="o">=</span> <span class="nf">floor</span><span class="p">(</span><span class="n">coord</span> <span class="o">/</span> <span class="n">grid_size</span><span class="p">)</span>

<span class="n">cluster</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mi">0</span><span class="p">,</span>  <span class="c1"># (0,0,0) → ID unique du voxel
</span>    <span class="mi">0</span><span class="p">,</span>  <span class="c1"># (0,0,0) → même voxel
</span>    <span class="mi">1</span><span class="p">,</span>  <span class="c1"># (1,0,0)
</span>    <span class="mi">2</span><span class="p">,</span>  <span class="c1"># (1,1,0)
</span>    <span class="mi">3</span><span class="p">,</span>  <span class="c1"># (0,1,0)
</span><span class="p">]</span>
</code></pre></div></div> <h4 id="étape-4--identification-des-voxels-uniques">Étape 4 : Identification des Voxels Uniques</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">unique</span><span class="p">,</span> <span class="n">cluster_inverse</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span>
    <span class="n">cluster</span><span class="p">,</span> 
    <span class="nb">sorted</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    <span class="n">return_inverse</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    <span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
</code></pre></div></div> <p><strong>Que retourne torch.unique ?</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Input cluster (exemple)
</span><span class="n">cluster</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="c1">#          ↑─────↑  ↑──↑  ↑────────↑  ↑──────↑
#          3 pts   2 pts  4 points   3 points
</span>
<span class="n">unique</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># Les voxels uniques
# Nvoxel = 4
</span>
<span class="n">cluster_inverse</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="c1"># Mapping: point i appartient au voxel unique[cluster_inverse[i]]
</span>
<span class="n">counts</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># Nombre de points par voxel
</span></code></pre></div></div> <h4 id="étape-5--tri-et-index-pointers">Étape 5 : Tri et Index Pointers</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Tri les points par voxel
</span><span class="n">_</span><span class="p">,</span> <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">cluster_inverse</span><span class="p">)</span>
<span class="c1"># sorted_indices: ordre pour regrouper les points du même voxel ensemble
</span>
<span class="c1"># Création des pointeurs pour chaque voxel
</span><span class="n">idx_ptr</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> 
    <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="p">])</span>
<span class="c1"># idx_ptr: (Nvoxel + 1,)
</span></code></pre></div></div> <p><strong>Exemple :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Après tri
</span><span class="n">sorted_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">]</span>
<span class="c1"># Points triés par voxel
</span>
<span class="c1"># Index pointers
</span><span class="n">counts</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">idx_ptr</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>
<span class="c1">#          ↑  ↑  ↑  ↑  ↑
#          │  │  │  │  └─ Fin (12 points)
#          │  │  │  └──── Voxel 3 commence à l'indice 9
#          │  │  └─────── Voxel 2 commence à l'indice 5
#          │  └────────── Voxel 1 commence à l'indice 3
#          └───────────── Voxel 0 commence à l'indice 0
</span></code></pre></div></div> <h4 id="étape-6--agrégation-des-coordonnées-moyenne">Étape 6 : Agrégation des Coordonnées (Moyenne)</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coord_pooled</span> <span class="o">=</span> <span class="nf">segment_csr</span><span class="p">(</span>
    <span class="n">coord</span><span class="p">[</span><span class="n">sorted_indices</span><span class="p">],</span>  <span class="c1"># Coordonnées triées par voxel
</span>    <span class="n">idx_ptr</span><span class="p">,</span> 
    <span class="nb">reduce</span><span class="o">=</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span>
<span class="p">)</span>
<span class="c1"># coord_pooled: (Nvoxel, 3)
# Position moyenne de tous les points dans chaque voxel
</span></code></pre></div></div> <p><strong>Exemple :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Voxel 0 contient 3 points aux positions:
</span><span class="n">points_voxel_0</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">]]</span>
<span class="n">coord_pooled</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nf">mean</span><span class="p">(</span><span class="n">points_voxel_0</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.37</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">]</span>

<span class="c1"># Voxel 1 contient 2 points:
</span><span class="n">points_voxel_1</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]]</span>
<span class="n">coord_pooled</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nf">mean</span><span class="p">(</span><span class="n">points_voxel_1</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">]</span>
</code></pre></div></div> <h4 id="étape-7--agrégation-des-features-max">Étape 7 : Agrégation des Features (Max)</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feat_pooled</span> <span class="o">=</span> <span class="nf">segment_csr</span><span class="p">(</span>
    <span class="n">feat</span><span class="p">[</span><span class="n">sorted_indices</span><span class="p">],</span>  <span class="c1"># Features triées par voxel
</span>    <span class="n">idx_ptr</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="sh">"</span><span class="s">max</span><span class="sh">"</span>
<span class="p">)</span>
<span class="c1"># feat_pooled: (Nvoxel, out_channels)
# Maximum des features dans chaque voxel
</span></code></pre></div></div> <p><strong>Pourquoi Max au lieu de Mean ?</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Exemple avec 3 points dans un voxel
</span>
<span class="c1"># Mean pooling
</span><span class="n">feat_mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">feat1</span> <span class="o">+</span> <span class="n">feat2</span> <span class="o">+</span> <span class="n">feat3</span><span class="p">)</span> <span class="o">/</span> <span class="mi">3</span>
<span class="c1"># Peut "diluer" les features importantes
</span>
<span class="c1"># Max pooling (utilisé par PTv2)
</span><span class="n">feat_max</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">feat1</span><span class="p">,</span> <span class="n">feat2</span><span class="p">,</span> <span class="n">feat3</span><span class="p">)</span>
<span class="c1"># Préserve les features dominantes de chaque canal
# Plus robuste au bruit et aux outliers
</span></code></pre></div></div> <p><strong>Exemple concret :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Voxel contient 3 points avec features:
</span><span class="n">feat1</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
<span class="n">feat2</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]</span>
<span class="n">feat3</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>

<span class="c1"># Max pooling (canal par canal)
</span><span class="n">feat_pooled</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nf">max</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span>  <span class="c1"># Canal 0
</span>    <span class="nf">max</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">)</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>  <span class="c1"># Canal 1
</span>    <span class="nf">max</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">)</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">,</span>  <span class="c1"># Canal 2
</span>    <span class="nf">max</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span> <span class="o">=</span> <span class="mf">0.6</span>   <span class="c1"># Canal 3
</span><span class="p">]</span>
<span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]</span>
</code></pre></div></div> <h4 id="étape-8--reconstruction-des-offsets">Étape 8 : Reconstruction des Offsets</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Récupération du batch ID pour chaque voxel
# (prend le batch du premier point de chaque voxel)
</span><span class="n">batch_pooled</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="n">idx_ptr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="c1"># batch_pooled: (Nvoxel,)
</span>
<span class="c1"># Conversion batch → offset
</span><span class="n">offset_pooled</span> <span class="o">=</span> <span class="nf">batch2offset</span><span class="p">(</span><span class="n">batch_pooled</span><span class="p">)</span>
<span class="c1"># offset_pooled: (B,)
</span></code></pre></div></div> <h4 id="étape-9--retour-du-cluster-mapping">Étape 9 : Retour du Cluster Mapping</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">return</span> <span class="p">[</span><span class="n">coord_pooled</span><span class="p">,</span> <span class="n">feat_pooled</span><span class="p">,</span> <span class="n">offset_pooled</span><span class="p">],</span> <span class="n">cluster_inverse</span>
</code></pre></div></div> <p>Le <code class="language-plaintext highlighter-rouge">cluster_inverse</code> est <strong>crucial</strong> car il permet le <strong>Map Unpooling</strong> plus tard :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># cluster_inverse: (N,) - pour chaque point, son voxel d'appartenance
</span><span class="n">cluster_inverse</span><span class="p">[</span><span class="n">point_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">voxel_id</span>

<span class="c1"># Exemple
</span><span class="n">cluster_inverse</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="c1">#                  ↑─────↑  ↑──↑  ↑────────↑  ↑──────↑
#                  Points du voxel 0, 1, 2, 3
</span></code></pre></div></div> <p>Ce mapping sera réutilisé dans <code class="language-plaintext highlighter-rouge">UnpoolWithSkip</code> pour “dépooler” efficacement !</p> <hr/> <h2 id="exemple-complet-numérique">Exemple Complet Numérique</h2> <p><strong>Configuration :</strong></p> <ul> <li>N = 12 points</li> <li>grid_size = 1.0</li> <li>in_channels = 3, out_channels = 4</li> </ul> <p><strong>Input :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coord</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>  <span class="c1"># Point 0
</span>    <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>  <span class="c1"># Point 1
</span>    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">],</span> <span class="c1"># Point 2
</span>    <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>  <span class="c1"># Point 3
</span>    <span class="p">[</span><span class="mf">1.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>  <span class="c1"># Point 4
</span>    <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>  <span class="c1"># Point 5
</span>    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>  <span class="c1"># Point 6
</span>    <span class="p">[</span><span class="mf">2.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>  <span class="c1"># Point 7
</span>    <span class="p">[</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>  <span class="c1"># Point 8
</span>    <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>  <span class="c1"># Point 9
</span>    <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>  <span class="c1"># Point 10
</span>    <span class="p">[</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>  <span class="c1"># Point 11
</span><span class="p">]</span>  <span class="c1"># (12, 3)
</span>
<span class="n">feat</span> <span class="o">=</span> <span class="p">[...]</span> <span class="c1"># (12, 3)
</span><span class="n">offset</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">]</span>  <span class="c1"># 1 nuage avec 12 points
</span></code></pre></div></div> <p><strong>Étape 1 : Projection</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feat</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span> <span class="err">→</span> <span class="n">BatchNorm</span> <span class="err">→</span> <span class="n">ReLU</span>
<span class="c1"># feat: (12, 4)
</span></code></pre></div></div> <p><strong>Étape 2 : Start point</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">start</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>  <span class="c1"># Minimum de chaque dimension
</span></code></pre></div></div> <p><strong>Étape 3 : Voxelisation</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Coordonnées normalisées
</span><span class="n">coord_norm</span> <span class="o">=</span> <span class="n">coord</span> <span class="o">-</span> <span class="n">start</span>

<span class="c1"># Assignation aux voxels (floor(coord_norm / 1.0))
</span><span class="n">cluster</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mi">0</span><span class="p">,</span>  <span class="c1"># (0,0,0)  Points 0, 1, 2
</span>    <span class="mi">0</span><span class="p">,</span>  <span class="c1"># (0,0,0)
</span>    <span class="mi">0</span><span class="p">,</span>  <span class="c1"># (0,0,0)
</span>    <span class="mi">1</span><span class="p">,</span>  <span class="c1"># (1,0,0)  Points 3, 4
</span>    <span class="mi">1</span><span class="p">,</span>  <span class="c1"># (1,0,0)
</span>    <span class="mi">2</span><span class="p">,</span>  <span class="c1"># (1,1,0)  Points 5, 9, 10
</span>    <span class="mi">3</span><span class="p">,</span>  <span class="c1"># (0,1,0)  Point 6
</span>    <span class="mi">4</span><span class="p">,</span>  <span class="c1"># (2,0,0)  Points 7, 8
</span>    <span class="mi">4</span><span class="p">,</span>  <span class="c1"># (2,0,0)
</span>    <span class="mi">2</span><span class="p">,</span>  <span class="c1"># (1,1,0)
</span>    <span class="mi">2</span><span class="p">,</span>  <span class="c1"># (1,1,0)
</span>    <span class="mi">5</span><span class="p">,</span>  <span class="c1"># (2,1,0)  Point 11
</span><span class="p">]</span>
</code></pre></div></div> <p><strong>Étape 4 : Unique</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">unique</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>  <span class="c1"># 6 voxels
</span><span class="n">counts</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># Points par voxel
</span><span class="n">cluster_inverse</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
</code></pre></div></div> <p><strong>Étape 5 : Tri</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sorted_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">11</span><span class="p">]</span>
<span class="n">idx_ptr</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>
</code></pre></div></div> <p><strong>Étape 6 : Agrégation Coordonnées</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coord_pooled</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nf">mean</span><span class="p">(</span><span class="n">coord</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.37</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">],</span>   <span class="c1"># Voxel 0
</span>    <span class="nf">mean</span><span class="p">(</span><span class="n">coord</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>   <span class="o">=</span> <span class="p">[</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">],</span>     <span class="c1"># Voxel 1
</span>    <span class="nf">mean</span><span class="p">(</span><span class="n">coord</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span><span class="o">=</span> <span class="p">[</span><span class="mf">1.17</span><span class="p">,</span> <span class="mf">1.83</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>    <span class="c1"># Voxel 2
</span>    <span class="n">coord</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span>           <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>      <span class="c1"># Voxel 3
</span>    <span class="nf">mean</span><span class="p">(</span><span class="n">coord</span><span class="p">[</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">])</span>   <span class="o">=</span> <span class="p">[</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>      <span class="c1"># Voxel 4
</span>    <span class="n">coord</span><span class="p">[</span><span class="mi">11</span><span class="p">]</span>          <span class="o">=</span> <span class="p">[</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>      <span class="c1"># Voxel 5
</span><span class="p">]</span>  <span class="c1"># (6, 3)
</span></code></pre></div></div> <p><strong>Étape 7 : Agrégation Features</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feat_pooled</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nf">max</span><span class="p">(</span><span class="n">feat</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>   <span class="c1"># (4,)
</span>    <span class="nf">max</span><span class="p">(</span><span class="n">feat</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>     <span class="c1"># (4,)
</span>    <span class="nf">max</span><span class="p">(</span><span class="n">feat</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>  <span class="c1"># (4,)
</span>    <span class="n">feat</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span>                   <span class="c1"># (4,)
</span>    <span class="nf">max</span><span class="p">(</span><span class="n">feat</span><span class="p">[</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>     <span class="c1"># (4,)
</span>    <span class="n">feat</span><span class="p">[</span><span class="mi">11</span><span class="p">],</span>                  <span class="c1"># (4,)
</span><span class="p">]</span>  <span class="c1"># (6, 4)
</span></code></pre></div></div> <p><strong>Output :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coord_pooled</span><span class="p">:</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>     <span class="c1"># 6 voxels au lieu de 12 points
</span><span class="n">feat_pooled</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>      <span class="c1"># 6 voxels avec 4 channels
</span><span class="n">offset_pooled</span><span class="p">:</span> <span class="p">[</span><span class="mi">6</span><span class="p">]</span>       <span class="c1"># 1 nuage avec 6 points
</span><span class="n">cluster_inverse</span><span class="p">:</span> <span class="p">(</span><span class="mi">12</span><span class="p">,)</span>   <span class="c1"># Mapping points → voxels
</span></code></pre></div></div> <p><strong>Réduction :</strong> 12 points → 6 voxels (2× downsampling)</p> <hr/> <h2 id="complexité-et-performance">Complexité et Performance</h2> <h3 id="complexité-algorithmique">Complexité Algorithmique</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GridPool
</span><span class="mf">1.</span> <span class="n">Linear</span><span class="p">:</span> <span class="nc">O</span><span class="p">(</span><span class="n">N</span> <span class="err">×</span> <span class="n">C</span><span class="p">)</span>
<span class="mf">2.</span> <span class="n">Start</span> <span class="n">computation</span><span class="p">:</span> <span class="nc">O</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="mf">3.</span> <span class="n">Voxel</span> <span class="n">assignment</span><span class="p">:</span> <span class="nc">O</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="mf">4.</span> <span class="n">Unique</span><span class="p">:</span> <span class="nc">O</span><span class="p">(</span><span class="n">N</span> <span class="n">log</span> <span class="n">N</span><span class="p">)</span>
<span class="mf">5.</span> <span class="n">Sort</span><span class="p">:</span> <span class="nc">O</span><span class="p">(</span><span class="n">N</span> <span class="n">log</span> <span class="n">N</span><span class="p">)</span>
<span class="mf">6.</span> <span class="n">Aggregation</span><span class="p">:</span> <span class="nc">O</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

<span class="n">Total</span><span class="p">:</span> <span class="nc">O</span><span class="p">(</span><span class="n">N</span> <span class="n">log</span> <span class="n">N</span><span class="p">)</span>  <span class="err">←</span> <span class="n">Dominé</span> <span class="n">par</span> <span class="n">le</span> <span class="n">tri</span>
</code></pre></div></div> <h3 id="comparaison-avec-fps">Comparaison avec FPS</h3> <table> <thead> <tr> <th>Opération</th> <th>FPS (PTv1)</th> <th>Grid Pooling (PTv2)</th> </tr> </thead> <tbody> <tr> <td><strong>Complexité</strong></td> <td>O(N²)</td> <td>O(N log N)</td> </tr> <tr> <td><strong>N=10k</strong></td> <td>100M ops</td> <td>~133k ops</td> </tr> <tr> <td><strong>N=100k</strong></td> <td>10B ops</td> <td>~1.7M ops</td> </tr> <tr> <td><strong>Speedup empirique</strong></td> <td>-</td> <td><strong>3-5×</strong></td> </tr> </tbody> </table> <h3 id="visualisation-du-speedup">Visualisation du Speedup</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Temps de downsampling (N→N/4):

FPS (PTv1):
N=10k:  ████████████ 120ms
N=50k:  ████████████████████████████████ 3000ms
N=100k: ████████████████████████████████████████████████████ 12000ms

Grid Pooling (PTv2):
N=10k:  ███ 30ms
N=50k:  ████████ 200ms
N=100k: ██████████████ 350ms

Speedup: ~4×  ~15×  ~34×
</code></pre></div></div> <hr/> <h2 id="avantages-de-grid-pooling">Avantages de Grid Pooling</h2> <h3 id="1-vitesse">1. Vitesse</h3> <p><strong>3-5× plus rapide</strong> que FPS, surtout pour les grands nuages.</p> <h3 id="2-déterminisme">2. Déterminisme</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># FPS: résultat dépend du point de départ aléatoire
</span><span class="n">run1</span> <span class="o">=</span> <span class="nc">FPS</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">run2</span> <span class="o">=</span> <span class="nc">FPS</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">43</span><span class="p">)</span>
<span class="c1"># run1 ≠ run2  (points sélectionnés différents)
</span>
<span class="c1"># Grid Pooling: toujours le même résultat
</span><span class="n">run1</span> <span class="o">=</span> <span class="nc">GridPool</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">=</span><span class="mf">0.06</span><span class="p">)</span>
<span class="n">run2</span> <span class="o">=</span> <span class="nc">GridPool</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">=</span><span class="mf">0.06</span><span class="p">)</span>
<span class="c1"># run1 == run2  (voxels identiques)
</span></code></pre></div></div> <h3 id="3-couverture-spatiale-uniforme">3. Couverture Spatiale Uniforme</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># FPS: peut "rater" des régions
</span>    <span class="err">●─────●─────────●</span>
    <span class="err">│</span>              <span class="err">│</span>
    <span class="err">│</span>   <span class="err">●●●●</span>       <span class="err">│</span>  <span class="err">←</span> <span class="n">Zone</span> <span class="n">dense</span> <span class="n">peu</span> <span class="n">échantillonnée</span>
    <span class="err">│</span>              <span class="err">│</span>
    <span class="err">●─────────────●</span>

<span class="c1"># Grid Pooling: couverture garantie
</span>    <span class="err">●─────●─────●─────●</span>
    <span class="err">│</span>     <span class="err">│</span>     <span class="err">│</span>     <span class="err">│</span>
    <span class="err">├─────┼─────┼─────┤</span>
    <span class="err">│</span>  <span class="err">●</span>  <span class="err">│</span> <span class="err">●●</span>  <span class="err">│</span>     <span class="err">│</span>  <span class="err">←</span> <span class="n">Chaque</span> <span class="n">voxel</span> <span class="n">représenté</span>
    <span class="err">├─────┼─────┼─────┤</span>
    <span class="err">●─────●─────●─────●</span>
</code></pre></div></div> <h3 id="4-map-unpooling-gratuit">4. Map Unpooling Gratuit</h3> <p>Le <code class="language-plaintext highlighter-rouge">cluster_inverse</code> permet un unpooling <strong>sans calcul</strong> :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv1: doit recalculer K-NN pour l'interpolation
</span><span class="n">upsampled</span> <span class="o">=</span> <span class="nf">knn_interpolation</span><span class="p">(</span><span class="n">low_res</span><span class="p">,</span> <span class="n">high_res</span><span class="p">)</span>  <span class="c1"># Coûteux !
</span>
<span class="c1"># PTv2: réutilise le cluster mapping
</span><span class="n">upsampled</span> <span class="o">=</span> <span class="n">feat_low_res</span><span class="p">[</span><span class="n">cluster_inverse</span><span class="p">]</span>  <span class="c1"># Lookup instantané !
</span></code></pre></div></div> <h1 id="unpoolwithskip--map-unpooling-avec-skip-connections">UnpoolWithSkip : Map Unpooling avec Skip Connections</h1> <h2 id="vue-densemble-3">Vue d’ensemble</h2> <p><code class="language-plaintext highlighter-rouge">UnpoolWithSkip</code> est le pendant de <code class="language-plaintext highlighter-rouge">GridPool</code> dans le décodeur, permettant de remonter en résolution tout en fusionnant l’information multi-échelle via les skip connections.</p> <figure> <picture> <img src="/assets/img/poinTransformerV2/unpoolWithSkip.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="comparaison--interpolation-vs-map-unpooling">Comparaison : Interpolation vs Map Unpooling</h3> <table> <thead> <tr> <th>Aspect</th> <th>PTv1 (K-NN Interpolation)</th> <th>PTv2 (Map Unpooling)</th> </tr> </thead> <tbody> <tr> <td><strong>Méthode</strong></td> <td>K-NN + weighted average</td> <td>Lookup direct via cluster</td> </tr> <tr> <td><strong>Complexité</strong></td> <td>O(M log N) K-NN search</td> <td><strong>O(1) lookup</strong></td> </tr> <tr> <td><strong>Information utilisée</strong></td> <td>Distances géométriques</td> <td>Mapping du downsampling</td> </tr> <tr> <td><strong>Coût</strong></td> <td>Coûteux (recherche K-NN)</td> <td><strong>Gratuit</strong> (indexing)</td> </tr> <tr> <td><strong>Précision</strong></td> <td>Interpolation lisse</td> <td>Mapping exact</td> </tr> </tbody> </table> <hr/> <h2 id="problème-avec-k-nn-interpolation-ptv1">Problème avec K-NN Interpolation (PTv1)</h2> <h3 id="algorithme-dinterpolation-ptv1">Algorithme d’Interpolation PTv1</h3> <p>Dans PTv1, pour passer de M points (basse résolution) à N points (haute résolution), on utilise une <strong>interpolation par K-NN</strong> :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">interpolation</span><span class="p">(</span><span class="n">coord_low</span><span class="p">,</span> <span class="n">coord_high</span><span class="p">,</span> <span class="n">feat_low</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Args:
        coord_low: (M, 3) - positions basse résolution
        coord_high: (N, 3) - positions haute résolution (cibles)
        feat_low: (M, C) - features basse résolution
    Returns:
        feat_high: (N, C) - features interpolées
    </span><span class="sh">"""</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">coord_high</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="c1"># 1. Trouver les K voisins les plus proches dans coord_low
</span>        <span class="n">distances</span> <span class="o">=</span> <span class="o">||</span><span class="n">coord_low</span> <span class="o">-</span> <span class="n">coord_high</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="o">||</span>  <span class="c1"># (M,)
</span>        <span class="n">k_nearest</span> <span class="o">=</span> <span class="nf">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[:</span><span class="n">K</span><span class="p">]</span>  <span class="c1"># K indices
</span>        
        <span class="c1"># 2. Poids inversement proportionnels aux distances
</span>        <span class="n">dists</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">k_nearest</span><span class="p">]</span>  <span class="c1"># (K,)
</span>        <span class="n">weights</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">dists</span> <span class="o">+</span> <span class="n">ε</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="nf">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>  <span class="c1"># Normalisation
</span>        
        <span class="c1"># 3. Moyenne pondérée
</span>        <span class="n">feat_high</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">Σ</span> <span class="n">weights</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="err">×</span> <span class="n">feat_low</span><span class="p">[</span><span class="n">k_nearest</span><span class="p">[</span><span class="n">k</span><span class="p">]]</span>
    
    <span class="k">return</span> <span class="n">feat_high</span>
</code></pre></div></div> <p><strong>Visualisation :</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Basse résolution (M=4 points):        Haute résolution (N=9 cibles):
    
    ◉₁         ◉₂                         ●₁    ●₂    ●₃
                                          
                                          ●₄    ●₅    ●₆
                                          
    ◉₃         ◉₄                         ●₇    ●₈    ●₉

Pour interpoler ●₅ (centre):
1. K-NN: trouver les 3 plus proches parmi {◉₁, ◉₂, ◉₃, ◉₄}
   → ◉₁ (dist=1.4), ◉₂ (dist=1.4), ◉₃ (dist=1.4), ◉₄ (dist=1.4)
   → Tous équidistants !

2. Poids: w₁=w₂=w₃=w₄ = 0.25

3. Interpolation:
   feat[●₅] = 0.25×feat[◉₁] + 0.25×feat[◉₂] + 0.25×feat[◉₃] + 0.25×feat[◉₄]
</code></pre></div></div> <h3 id="problèmes-de-linterpolation">Problèmes de l’Interpolation</h3> <p><strong>1. Coût computationnel :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pour chaque point haute résolution N:
#   - Calculer M distances
#   - Trier pour trouver les K plus proches
#   - Calculer la moyenne pondérée
</span>
<span class="n">Complexité</span><span class="p">:</span> <span class="nc">O</span><span class="p">(</span><span class="n">N</span> <span class="err">×</span> <span class="n">M</span> <span class="n">log</span> <span class="n">M</span><span class="p">)</span>

<span class="c1"># Exemple: M=25k, N=100k
</span><span class="n">Opérations</span><span class="p">:</span> <span class="mi">100</span><span class="n">k</span> <span class="err">×</span> <span class="mi">25</span><span class="n">k</span> <span class="err">×</span> <span class="nf">log</span><span class="p">(</span><span class="mi">25</span><span class="n">k</span><span class="p">)</span> <span class="err">≈</span> <span class="mi">35</span> <span class="n">milliards</span> <span class="err">!</span>
</code></pre></div></div> <p><strong>2. Perte d’information :</strong></p> <p>L’interpolation crée de <strong>nouvelles</strong> features qui n’existaient pas dans la résolution d’origine :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GridPool: Point original → Voxel A
</span><span class="n">point_42</span><span class="p">:</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">],</span> <span class="n">feat_original</span>

<span class="c1"># Après downsampling
</span><span class="n">voxel_A</span><span class="p">:</span> <span class="p">[</span><span class="n">x_mean</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">],</span> <span class="n">feat_pooled</span>

<span class="c1"># Après interpolation (PTv1)
</span><span class="n">point_42</span><span class="p">:</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">],</span> <span class="n">feat_interpolated</span>  <span class="c1"># ≠ feat_original !
# L'interpolation "invente" des features
</span></code></pre></div></div> <p><strong>3. Non-déterminisme pour les cas ambigus :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Point équidistant de plusieurs voisins
</span><span class="n">distances</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>  <span class="c1"># 4 voisins équidistants
</span>
<span class="c1"># Avec K=3, lesquels choisir ?
# → Dépend de l'ordre dans le tableau (non déterministe)
</span></code></pre></div></div> <hr/> <h2 id="solution--map-unpooling-ptv2">Solution : Map Unpooling (PTv2)</h2> <h3 id="principe--réutilisation-du-cluster-mapping">Principe : Réutilisation du Cluster Mapping</h3> <p>L’idée géniale de PTv2 : <strong>stocker le mapping lors du downsampling</strong> et le <strong>réutiliser lors de l’upsampling</strong> !</p> <p><strong>Rappel du GridPool :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GridPool retourne le cluster_inverse
</span><span class="n">coord_pooled</span><span class="p">,</span> <span class="n">feat_pooled</span><span class="p">,</span> <span class="n">offset_pooled</span><span class="p">,</span> <span class="n">cluster</span> <span class="o">=</span> <span class="nc">GridPool</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>

<span class="c1"># cluster: (N,) - pour chaque point original, son voxel d'appartenance
</span><span class="n">cluster</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="c1">#          └──┬──┘  └─┬─┘  └────┬────┘  └──┬──┘
#          Voxel 0  Voxel 1  Voxel 2   Voxel 3
</span></code></pre></div></div> <p><strong>Map Unpooling :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pour remonter en résolution, simple indexing !
</span><span class="n">feat_upsampled</span> <span class="o">=</span> <span class="n">feat_pooled</span><span class="p">[</span><span class="n">cluster</span><span class="p">]</span>  <span class="c1"># (N, C)
</span>
<span class="c1"># Chaque point récupère les features de son voxel d'origine
</span></code></pre></div></div> <p><strong>C’est tout !</strong> Un simple lookup, complexité <strong>O(1)</strong> par point, donc <strong>O(N)</strong> total.</p> <hr/> <h2 id="algorithme-détaillé">Algorithme Détaillé</h2> <h3 id="inputs">Inputs</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Points actuels (basse résolution)
</span><span class="n">coord_low</span><span class="p">:</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>        <span class="c1"># Positions des voxels
</span><span class="n">feat_low</span><span class="p">:</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">in_ch</span><span class="p">)</span>     <span class="c1"># Features des voxels
</span><span class="n">offset_low</span><span class="p">:</span> <span class="p">(</span><span class="n">B</span><span class="p">,)</span>

<span class="c1"># Points skip (haute résolution - de l'encodeur)
</span><span class="n">coord_skip</span><span class="p">:</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>       <span class="c1"># Positions originales
</span><span class="n">feat_skip</span><span class="p">:</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">skip_ch</span><span class="p">)</span>  <span class="c1"># Features originales
</span><span class="n">offset_skip</span><span class="p">:</span> <span class="p">(</span><span class="n">B</span><span class="p">,)</span>

<span class="c1"># Cluster mapping (du GridPool correspondant)
</span><span class="n">cluster</span><span class="p">:</span> <span class="p">(</span><span class="n">N</span><span class="p">,)</span>            <span class="c1"># Pour chaque point, son voxel
</span></code></pre></div></div> <h3 id="étape-1--projection-des-features-basse-résolution">Étape 1 : Projection des Features Basse Résolution</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feat_low_proj</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">feat_low</span><span class="p">)</span> <span class="err">→</span> <span class="n">BatchNorm1d</span> <span class="err">→</span> <span class="n">ReLU</span>
<span class="c1"># feat_low_proj: (M, out_ch)
</span></code></pre></div></div> <h3 id="étape-2--map-unpooling">Étape 2 : Map Unpooling</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Lookup direct via cluster
</span><span class="n">feat_mapped</span> <span class="o">=</span> <span class="n">feat_low_proj</span><span class="p">[</span><span class="n">cluster</span><span class="p">]</span>
<span class="c1"># feat_mapped: (N, out_ch)
</span></code></pre></div></div> <p><strong>Explication :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Exemple avec M=4 voxels, N=12 points
</span><span class="n">feat_low_proj</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="n">f</span><span class="err">₀⁰</span><span class="p">,</span> <span class="n">f</span><span class="err">₀¹</span><span class="p">,</span> <span class="n">f</span><span class="err">₀²</span><span class="p">,</span> <span class="n">f</span><span class="err">₀³</span><span class="p">],</span>  <span class="c1"># Features du voxel 0
</span>    <span class="p">[</span><span class="n">f</span><span class="err">₁⁰</span><span class="p">,</span> <span class="n">f</span><span class="err">₁¹</span><span class="p">,</span> <span class="n">f</span><span class="err">₁²</span><span class="p">,</span> <span class="n">f</span><span class="err">₁³</span><span class="p">],</span>  <span class="c1"># Features du voxel 1
</span>    <span class="p">[</span><span class="n">f</span><span class="err">₂⁰</span><span class="p">,</span> <span class="n">f</span><span class="err">₂¹</span><span class="p">,</span> <span class="n">f</span><span class="err">₂²</span><span class="p">,</span> <span class="n">f</span><span class="err">₂³</span><span class="p">],</span>  <span class="c1"># Features du voxel 2
</span>    <span class="p">[</span><span class="n">f</span><span class="err">₃⁰</span><span class="p">,</span> <span class="n">f</span><span class="err">₃¹</span><span class="p">,</span> <span class="n">f</span><span class="err">₃²</span><span class="p">,</span> <span class="n">f</span><span class="err">₃³</span><span class="p">],</span>  <span class="c1"># Features du voxel 3
</span><span class="p">]</span>  <span class="c1"># (4, 4)
</span>
<span class="n">cluster</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>

<span class="c1"># Map unpooling
</span><span class="n">feat_mapped</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">feat_low_proj</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>  <span class="c1"># Point 0 → voxel 0
</span>    <span class="n">feat_low_proj</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>  <span class="c1"># Point 1 → voxel 0
</span>    <span class="n">feat_low_proj</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>  <span class="c1"># Point 2 → voxel 0
</span>    <span class="n">feat_low_proj</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Point 3 → voxel 1
</span>    <span class="n">feat_low_proj</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Point 4 → voxel 1
</span>    <span class="n">feat_low_proj</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>  <span class="c1"># Point 5 → voxel 2
</span>    <span class="n">feat_low_proj</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>  <span class="c1"># Point 6 → voxel 2
</span>    <span class="n">feat_low_proj</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>  <span class="c1"># Point 7 → voxel 2
</span>    <span class="n">feat_low_proj</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>  <span class="c1"># Point 8 → voxel 2
</span>    <span class="n">feat_low_proj</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>  <span class="c1"># Point 9 → voxel 3
</span>    <span class="n">feat_low_proj</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>  <span class="c1"># Point 10 → voxel 3
</span>    <span class="n">feat_low_proj</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>  <span class="c1"># Point 11 → voxel 3
</span><span class="p">]</span>  <span class="c1"># (12, 4)
</span></code></pre></div></div> <p>Chaque point récupère <strong>exactement</strong> les features de son voxel d’origine !</p> <h3 id="étape-3--projection-des-features-skip">Étape 3 : Projection des Features Skip</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feat_skip_proj</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">feat_skip</span><span class="p">)</span> <span class="err">→</span> <span class="n">BatchNorm1d</span> <span class="err">→</span> <span class="n">ReLU</span>
<span class="c1"># feat_skip_proj: (N, out_ch)
</span></code></pre></div></div> <h3 id="étape-4--fusion-skip-connection">Étape 4 : Fusion Skip Connection</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feat_fused</span> <span class="o">=</span> <span class="n">feat_mapped</span> <span class="o">+</span> <span class="n">feat_skip_proj</span>
<span class="c1"># feat_fused: (N, out_ch)
</span></code></pre></div></div> <p><strong>Visualisation :</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Basse résolution (upsampled):        Skip (haute résolution):
    feat_mapped                           feat_skip_proj
         ↓                                      ↓
    [0.2, 0.5, 0.1, 0.8]              [0.3, 0.1, 0.6, 0.2]
         ↓                                      ↓
         └──────────────── + ────────────────┘
                             ↓
                    [0.5, 0.6, 0.7, 1.0]
                         feat_fused
</code></pre></div></div> <h3 id="étape-5--output">Étape 5 : Output</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">return</span> <span class="p">[</span><span class="n">coord_skip</span><span class="p">,</span> <span class="n">feat_fused</span><span class="p">,</span> <span class="n">offset_skip</span><span class="p">]</span>
<span class="c1"># On retourne les coordonnées skip (haute résolution)
# Avec les features fusionnées
</span></code></pre></div></div> <hr/> <h2 id="exemple-complet-numérique-1">Exemple Complet Numérique</h2> <p><strong>Configuration :</strong></p> <ul> <li>M = 4 voxels (basse résolution)</li> <li>N = 12 points (haute résolution)</li> <li>in_ch = 3, skip_ch = 3, out_ch = 4</li> </ul> <p><strong>Inputs :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Basse résolution (voxels du GridPool)
</span><span class="n">coord_low</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">0.37</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">],</span>  <span class="c1"># Voxel 0 (moyenne de 3 points)
</span>    <span class="p">[</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">],</span>    <span class="c1"># Voxel 1 (moyenne de 2 points)
</span>    <span class="p">[</span><span class="mf">1.17</span><span class="p">,</span> <span class="mf">1.83</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>   <span class="c1"># Voxel 2 (moyenne de 4 points)
</span>    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>     <span class="c1"># Voxel 3 (moyenne de 3 points)
</span><span class="p">]</span>  <span class="c1"># (4, 3)
</span>
<span class="n">feat_low</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>  <span class="c1"># Features voxel 0
</span>    <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>  <span class="c1"># Features voxel 1
</span>    <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>  <span class="c1"># Features voxel 2
</span>    <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>  <span class="c1"># Features voxel 3
</span><span class="p">]</span>  <span class="c1"># (4, 3)
</span>
<span class="c1"># Haute résolution (points originaux de l'encodeur)
</span><span class="n">coord_skip</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>   <span class="c1"># Point 0 (était dans voxel 0)
</span>    <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>   <span class="c1"># Point 1 (était dans voxel 0)
</span>    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">],</span>  <span class="c1"># Point 2 (était dans voxel 0)
</span>    <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>   <span class="c1"># Point 3 (était dans voxel 1)
</span>    <span class="p">[</span><span class="mf">1.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>   <span class="c1"># Point 4 (était dans voxel 1)
</span>    <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>   <span class="c1"># Point 5 (était dans voxel 2)
</span>    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>   <span class="c1"># Point 6 (était dans voxel 3)
</span>    <span class="p">[</span><span class="mf">2.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>   <span class="c1"># Point 7 (était dans voxel 2)
</span>    <span class="p">[</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>   <span class="c1"># Point 8 (était dans voxel 2)
</span>    <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>   <span class="c1"># Point 9 (était dans voxel 2)
</span>    <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>   <span class="c1"># Point 10 (était dans voxel 3)
</span>    <span class="p">[</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>   <span class="c1"># Point 11 (était dans voxel 3)
</span><span class="p">]</span>  <span class="c1"># (12, 3)
</span>
<span class="n">feat_skip</span> <span class="o">=</span> <span class="p">[...]</span> <span class="c1"># (12, 3)
</span>
<span class="c1"># Cluster mapping (du GridPool correspondant)
</span><span class="n">cluster</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
</code></pre></div></div> <p><strong>Étape 1 : Projection feat_low</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feat_low_proj</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">feat_low</span><span class="p">)</span> <span class="err">→</span> <span class="n">BatchNorm</span> <span class="err">→</span> <span class="n">ReLU</span>
<span class="c1"># feat_low_proj: (4, 4) par exemple
</span><span class="n">feat_low_proj</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>  <span class="c1"># Voxel 0
</span>    <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>  <span class="c1"># Voxel 1
</span>    <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>  <span class="c1"># Voxel 2
</span>    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>  <span class="c1"># Voxel 3
</span><span class="p">]</span>
</code></pre></div></div> <p><strong>Étape 2 : Map Unpooling</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feat_mapped</span> <span class="o">=</span> <span class="n">feat_low_proj</span><span class="p">[</span><span class="n">cluster</span><span class="p">]</span>  <span class="c1"># Simple indexing !
</span>
<span class="c1"># Point 0 → cluster[0]=0 → feat_low_proj[0]
</span><span class="n">feat_mapped</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>

<span class="c1"># Point 1 → cluster[1]=0 → feat_low_proj[0]
</span><span class="n">feat_mapped</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>

<span class="c1"># Point 2 → cluster[2]=0 → feat_low_proj[0]
</span><span class="n">feat_mapped</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>

<span class="c1"># Point 3 → cluster[3]=1 → feat_low_proj[1]
</span><span class="n">feat_mapped</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>

<span class="c1"># Point 4 → cluster[4]=1 → feat_low_proj[1]
</span><span class="n">feat_mapped</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>

<span class="c1"># Point 5 → cluster[5]=2 → feat_low_proj[2]
</span><span class="n">feat_mapped</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>

<span class="c1"># Point 6 → cluster[6]=3 → feat_low_proj[3]
</span><span class="n">feat_mapped</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]</span>

<span class="c1"># Point 7 → cluster[7]=2 → feat_low_proj[2]
</span><span class="n">feat_mapped</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>

<span class="c1"># Point 8 → cluster[8]=2 → feat_low_proj[2]
</span><span class="n">feat_mapped</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>

<span class="c1"># Point 9 → cluster[9]=2 → feat_low_proj[2]
</span><span class="n">feat_mapped</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>

<span class="c1"># Point 10 → cluster[10]=3 → feat_low_proj[3]
</span><span class="n">feat_mapped</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]</span>

<span class="c1"># Point 11 → cluster[11]=3 → feat_low_proj[3]
</span><span class="n">feat_mapped</span><span class="p">[</span><span class="mi">11</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]</span>

<span class="c1"># Résultat: (12, 4)
</span></code></pre></div></div> <p><strong>Étape 3 : Projection feat_skip</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feat_skip_proj</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">feat_skip</span><span class="p">)</span> <span class="err">→</span> <span class="n">BatchNorm</span> <span class="err">→</span> <span class="n">ReLU</span>
<span class="c1"># feat_skip_proj: (12, 4)
</span></code></pre></div></div> <p><strong>Étape 4 : Fusion</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feat_fused</span> <span class="o">=</span> <span class="n">feat_mapped</span> <span class="o">+</span> <span class="n">feat_skip_proj</span>
<span class="c1"># feat_fused: (12, 4)
</span>
<span class="c1"># Exemple pour point 0
</span><span class="n">feat_fused</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>
              <span class="o">=</span> <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
</code></pre></div></div> <p><strong>Output :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coord_skip</span><span class="p">:</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>   <span class="c1"># Positions haute résolution
</span><span class="n">feat_fused</span><span class="p">:</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>   <span class="c1"># Features fusionnées
</span><span class="n">offset_skip</span><span class="p">:</span> <span class="p">(</span><span class="n">B</span><span class="p">,)</span>
</code></pre></div></div> <hr/> <h2 id="comparaison-visuelle--interpolation-vs-map-unpooling">Comparaison Visuelle : Interpolation vs Map Unpooling</h2> <h3 id="ptv1--k-nn-interpolation">PTv1 : K-NN Interpolation</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Downsampling (FPS):              Upsampling (K-NN Interpolation):
                                 
16 points → 4 points              4 points → 16 points
                                  
●●●●                              ●───●───●───●
●●●●    → FPS →    ◉   ◉         │   │   │   │
●●●●                              ●───●───●───●   ← K-NN pour chacun
●●●●               ◉   ◉         │   │   │   │
                                  ●───●───●───●
                                  │   │   │   │
                                  ●───●───●───●

Chaque ● cible:
  1. Cherche K=3 voisins parmi les 4 ◉
  2. Calcule les distances
  3. Moyenne pondérée

Coût: 16 × O(4 log 4) = O(N × M log M)
</code></pre></div></div> <h3 id="ptv2--map-unpooling">PTv2 : Map Unpooling</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Downsampling (GridPool):         Upsampling (Map Unpooling):

16 points → 4 voxels              4 voxels → 16 points
+ cluster mapping

●●●●                              ●●●●
●●●●  → Grid →  [0,0,0,0,         [0,0,0,0,  → ●●●●
●●●●             1,1,1,1,         1,1,1,1,     ●●●●
●●●●             2,2,2,2,         2,2,2,2,     ●●●●
                 3,3,3,3]         3,3,3,3]

Voxel 0 → 4 points                feat[●₀] = feat_voxel[0]
Voxel 1 → 4 points                feat[●₁] = feat_voxel[0]
Voxel 2 → 4 points                feat[●₂] = feat_voxel[0]
Voxel 3 → 4 points                feat[●₃] = feat_voxel[0]
                                  ...

Coût: 16 × O(1) = O(N)  ← Lookup direct !
</code></pre></div></div> <hr/> <h2 id="avantages-du-map-unpooling">Avantages du Map Unpooling</h2> <h3 id="1-vitesse-1">1. Vitesse</h3> <p><strong>Complexité :</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv1 : K-NN Interpolation
</span><span class="n">Complexité</span><span class="p">:</span> <span class="nc">O</span><span class="p">(</span><span class="n">N</span> <span class="err">×</span> <span class="n">M</span> <span class="n">log</span> <span class="n">M</span><span class="p">)</span>

<span class="c1"># PTv2 : Map Unpooling
</span><span class="n">Complexité</span><span class="p">:</span> <span class="nc">O</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>  <span class="err">←</span> <span class="n">Juste</span> <span class="n">un</span> <span class="n">indexing</span> <span class="err">!</span>

<span class="c1"># Exemple: N=100k, M=25k
</span><span class="n">PTv1</span><span class="p">:</span> <span class="mi">100</span><span class="n">k</span> <span class="err">×</span> <span class="mi">25</span><span class="n">k</span> <span class="err">×</span> <span class="nf">log</span><span class="p">(</span><span class="mi">25</span><span class="n">k</span><span class="p">)</span> <span class="err">≈</span> <span class="mi">35</span> <span class="n">milliards</span> <span class="n">d</span><span class="sh">'</span><span class="s">ops
PTv2: 100k ≈ 100k ops

Speedup: ~350,000× sur cette opération !
</span></code></pre></div></div> <p><strong>En pratique, le speedup global est ~10-20× car l’interpolation n’est qu’une partie du décodeur.</strong></p> <h3 id="2-exactitude">2. Exactitude</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv1 : Interpolation
</span><span class="n">point_original</span> <span class="err">→</span> <span class="n">voxel_A</span> <span class="err">→</span> <span class="n">interpolation</span>
<span class="n">feat_final</span> <span class="err">≈</span> <span class="n">feat_original</span>  <span class="c1"># Approximation
</span>
<span class="c1"># PTv2 : Map exact
</span><span class="n">point_original</span> <span class="err">→</span> <span class="n">voxel_A</span> <span class="err">→</span> <span class="nb">map</span> <span class="n">unpooling</span>
<span class="n">feat_final</span> <span class="o">=</span> <span class="n">feat_voxel</span><span class="p">[</span><span class="n">A</span><span class="p">]</span>  <span class="c1"># Exact (pas d'interpolation)
</span></code></pre></div></div> <p>Les points récupèrent <strong>exactement</strong> les features de leur voxel d’origine, sans interpolation artificielle.</p> <h3 id="3-mémoire">3. Mémoire</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv1 : Doit stocker les K-NN indices temporaires
</span><span class="n">knn_indices</span><span class="p">:</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>

<span class="c1"># PTv2 : Cluster mapping déjà stocké du downsampling
</span><span class="n">cluster</span><span class="p">:</span> <span class="p">(</span><span class="n">N</span><span class="p">,)</span>  <span class="c1"># Déjà en mémoire, réutilisé
</span></code></pre></div></div> <h3 id="4-déterminisme">4. Déterminisme</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv1 : K-NN peut être ambigu
</span><span class="n">distances</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>  <span class="c1"># 4 équidistants, K=3
# → Quel trio choisir ? Non déterministe
</span>
<span class="c1"># PTv2 : Mapping exact du downsampling
</span><span class="n">cluster</span><span class="p">[</span><span class="n">point_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">voxel_id</span>  <span class="c1"># Toujours le même
</span></code></pre></div></div> <hr/> <h2 id="encoder-et-decoder--vue-complète">Encoder et Decoder : Vue Complète</h2> <h3 id="encoder">Encoder</h3> <figure> <picture> <img src="/assets/img/poinTransformerV2/encoder.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Encoder</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">points</span><span class="p">):</span>
        <span class="c1"># Downsampling + enrichissement features
</span>        <span class="n">points_pooled</span><span class="p">,</span> <span class="n">cluster</span> <span class="o">=</span> <span class="nc">GridPool</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
        
        <span class="c1"># Attention locale sur les voxels
</span>        <span class="n">points_out</span> <span class="o">=</span> <span class="nc">BlockSequence</span><span class="p">(</span><span class="n">points_pooled</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">points_out</span><span class="p">,</span> <span class="n">cluster</span>
</code></pre></div></div> <p><strong>Flow :</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: N points, in_ch
    ↓
GridPool (voxelisation)
    ↓
Nvoxel points, embed_ch
+ cluster mapping (N,)
    ↓
BlockSequence (depth blocks)
    ↓
Output: Nvoxel points, embed_ch
+ cluster (N,)  ← Stocké pour le décodeur !
</code></pre></div></div> <h3 id="decoder">Decoder</h3> <figure> <picture> <img src="/assets/img/poinTransformerV2/decoder.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Decoder</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">points_low</span><span class="p">,</span> <span class="n">points_skip</span><span class="p">,</span> <span class="n">cluster</span><span class="p">):</span>
        <span class="c1"># Upsampling + fusion skip
</span>        <span class="n">points_up</span> <span class="o">=</span> <span class="nc">UnpoolWithSkip</span><span class="p">(</span><span class="n">points_low</span><span class="p">,</span> <span class="n">points_skip</span><span class="p">,</span> <span class="n">cluster</span><span class="p">)</span>
        
        <span class="c1"># Attention locale sur les points upsampled
</span>        <span class="n">points_out</span> <span class="o">=</span> <span class="nc">BlockSequence</span><span class="p">(</span><span class="n">points_up</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">points_out</span>
</code></pre></div></div> <p><strong>Flow :</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input basse résolution: M points, in_ch
Input skip (encodeur): N points, skip_ch
cluster mapping: (N,)
    ↓
UnpoolWithSkip (map unpooling + fusion)
    ↓
N points, embed_ch
    ↓
BlockSequence (depth blocks)
    ↓
Output: N points, embed_ch
</code></pre></div></div> <hr/> <h2 id="tableau-récapitulatif-des-innovations">Tableau Récapitulatif des Innovations</h2> <table> <thead> <tr> <th>Composant</th> <th>PTv1</th> <th>PTv2</th> <th>Gain</th> </tr> </thead> <tbody> <tr> <td><strong>Downsampling</strong></td> <td>FPS O(N²)</td> <td><strong>GridPool O(N log N)</strong></td> <td>3-5× speedup</td> </tr> <tr> <td><strong>Upsampling</strong></td> <td>K-NN Interpolation O(NM log M)</td> <td><strong>Map Unpooling O(N)</strong></td> <td>10-20× speedup</td> </tr> <tr> <td><strong>Mapping stocké</strong></td> <td>❌ Non</td> <td>✅ <strong>cluster</strong> réutilisé</td> <td>Mémoire efficient</td> </tr> <tr> <td><strong>Déterminisme</strong></td> <td>❌ Non (FPS aléatoire)</td> <td>✅ Oui (grille fixe)</td> <td>Reproductibilité</td> </tr> <tr> <td><strong>Exactitude</strong></td> <td>Interpolation approximative</td> <td><strong>Mapping exact</strong></td> <td>Plus précis</td> </tr> </tbody> </table> <hr/> <h2 id="performance-globale--ptv1-vs-ptv2">Performance Globale : PTv1 vs PTv2</h2> <h3 id="speedup-par-composant">Speedup par Composant</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Component                PTv1        PTv2        Speedup
──────────────────────────────────────────────────────
K-NN queries            24×         4×          6×
Downsampling (FPS)      O(N²)       O(N log N)  3-5×
Upsampling (Interp)     O(NM log M) O(N)        10-20×
Attention weights       576 params  128 params  4.5×
──────────────────────────────────────────────────────
Overall                 Baseline    2-3× faster
</code></pre></div></div> <h3 id="mémoire">Mémoire</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PTv1
</span><span class="o">-</span> <span class="n">Pas</span> <span class="n">de</span> <span class="n">cluster</span> <span class="n">mapping</span>
<span class="o">-</span> <span class="n">K</span><span class="o">-</span><span class="n">NN</span> <span class="n">temporaire</span> <span class="n">à</span> <span class="n">chaque</span> <span class="n">couche</span>
<span class="n">Total</span><span class="p">:</span> <span class="o">~</span><span class="mf">1.2</span><span class="err">×</span> <span class="n">baseline</span>

<span class="c1"># PTv2
</span><span class="o">-</span> <span class="n">cluster</span> <span class="n">mapping</span> <span class="n">stocké</span> <span class="p">(</span><span class="n">N</span><span class="p">,)</span> <span class="n">par</span> <span class="n">niveau</span>
<span class="o">-</span> <span class="n">K</span><span class="o">-</span><span class="n">NN</span> <span class="n">une</span> <span class="n">fois</span> <span class="n">par</span> <span class="n">BlockSequence</span>
<span class="n">Total</span><span class="p">:</span> <span class="o">~</span><span class="mf">1.0</span><span class="err">×</span> <span class="nf">baseline  </span><span class="p">(</span><span class="n">plus</span> <span class="n">efficient</span> <span class="err">!</span><span class="p">)</span>
</code></pre></div></div> <h3 id="précision">Précision</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dataset: S3DIS (segmentation sémantique)

PTv1: 70.4% mIoU
PTv2: 72.5% mIoU  (+2.1 points)

Speedup + meilleure précision ! 🎯
</code></pre></div></div> <hr/> <p>Voilà ! Nous avons couvert toute l’architecture de PTv2 :</p> <p>✅ <strong>GroupedLinear</strong> : Réduction paramétrique<br/> ✅ <strong>GroupedVectorAttention</strong> : Attention enrichie<br/> ✅ <strong>Block &amp; BlockSequence</strong> : Architecture résiduelle + K-NN partagé<br/> ✅ <strong>GVAPatchEmbed</strong> : Embedding initial<br/> ✅ <strong>GridPool</strong> : Downsampling par voxelisation<br/> ✅ <strong>UnpoolWithSkip</strong> : Map unpooling + skip connections<br/> ✅ <strong>Encoder &amp; Decoder</strong> : Architecture U-Net complète</p>]]></content><author><name></name></author><category term="computer-vision"/><category term="deep-learning"/><category term="point-cloud"/><category term="transformer"/><category term="architecture"/><summary type="html"><![CDATA[Detailed analysis of the Point Transformer v2 architecture for point-cloud segmentation and classification]]></summary></entry><entry><title type="html">Point Transformer v1: Architecture and Implementation Details</title><link href="http://antoineach.github.io//blog/2025/pointTransformerV1/" rel="alternate" type="text/html" title="Point Transformer v1: Architecture and Implementation Details"/><published>2025-10-13T00:00:00+00:00</published><updated>2025-10-13T00:00:00+00:00</updated><id>http://antoineach.github.io//blog/2025/pointTransformerV1</id><content type="html" xml:base="http://antoineach.github.io//blog/2025/pointTransformerV1/"><![CDATA[<h1 id="point-transformer-v1-architecture-and-implementation-details">Point Transformer v1: Architecture and Implementation Details</h1> <h2 id="introduction">Introduction</h2> <p><strong>Point Transformer v1</strong> is a model for segmentation and classification of 3D point clouds that adapts the Transformer mechanism to unstructured 3D data while respecting point-cloud specific constraints. Published in 2021, it adapts attention to local neighborhoods and the irregular nature of point clouds.</p> <p>The model follows a <strong>U-Net</strong>-like architecture composed of three main layer types:</p> <ul> <li><strong>PointTransformerLayer</strong>: local attention over the K nearest neighbors</li> <li><strong>TransitionDown</strong>: spatial downsampling using Furthest Point Sampling (FPS)</li> <li><strong>TransitionUp</strong>: upsampling with skip connections</li> </ul> <hr/> <h2 id="overall-architecture">Overall Architecture</h2> <p>The network follows an encoder–decoder (U-Net) design:</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/pointTransformerV1_architecture.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Key features:</strong></p> <ul> <li><strong>Local attention</strong>: attention is computed only over K nearest neighbors (typically K = 16) rather than globally.</li> <li><strong>Permutation invariance</strong>: the architecture respects the lack of natural ordering in point clouds.</li> <li><strong>Skip connections</strong>: U-Net style skip connections preserve spatial details.</li> </ul> <hr/> <h3 id="-input-reminder--batched-point-clouds">🧱 Input Reminder — Batched Point Clouds</h3> <p>Before diving into PointTransformer internals, recall that we handle <strong>batches of point clouds</strong> by concatenating them into a single tensor:</p> \[X \in \mathbb{R}^{N \times C}, \quad \text{where } N = N_1 + N_2 + \dots + N_B\] <p>and we keep <strong>offsets</strong> to delimit each cloud’s boundaries: \(\text{offsets} = [N_1,, N_1{+}N_2,, \dots,, N_1{+}N_2{+}\dots{+}N_B]\)</p> <p>Each row of (X) corresponds to one 3D point and its features — so linear layers act point-wise, without mixing points from different objects.</p> <p>For a detailed explanation of this batching strategy, see 👉 <a href="/blog/2025/batchingPointclouds/">Batching Point Clouds</a>.</p> <hr/> <h2 id="pointtransformerblock-residual-block">PointTransformerBlock: Residual Block</h2> <p><code class="language-plaintext highlighter-rouge">PointTransformerBlock</code> wraps the <code class="language-plaintext highlighter-rouge">PointTransformerLayer</code> inside a residual block (ResNet-style).</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/pointTransformerBlock.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Residual connections improve gradient flow, help learn residual mappings, and preserve initial information.</p> <hr/> <h2 id="pointtransformerlayer-vectorial-local-attention">PointTransformerLayer: Vectorial Local Attention</h2> <h3 id="overview">Overview</h3> <p>The <code class="language-plaintext highlighter-rouge">PointTransformerLayer</code> implements a <strong>local vector attention</strong> mechanism inspired by Transformers, but adapted to point clouds.</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/pointTransformerLayer.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="why-use-q---k-instead-of-qkᵀ">Why use Q - K instead of Q·Kᵀ?</h3> <p>The batching constraint is central here. In standard Transformers you compute:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">attention_scores</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="p">.</span><span class="n">T</span>  <span class="c1"># shape (N, N) -&gt; global attention
</span></code></pre></div></div> <p>But with concatenated point clouds:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>points = [ pc1_points | pc2_points | pc3_points ]
          ←    N_1   → ←    N_2   → ←    N_3   →
</code></pre></div></div> <p>A full \(N \times N\) attention matrix would include cross-cloud scores (e.g. between pc1 and pc2), which is <strong>invalid</strong>.</p> <p>Point Transformer avoids this by:</p> <ol> <li><strong>Local attention only</strong>: compute attention over the K nearest neighbors within the same cloud.</li> <li><strong>Neighbor search respecting offsets</strong>: <code class="language-plaintext highlighter-rouge">query_and_group</code> or neighbor routines use offsets to restrict neighbor search to the same cloud.</li> <li><strong>Using Q − K (relative vector) rather than a global dot product</strong>:</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># For each query point, consider its K neighbors (guaranteed same cloud)
</span><span class="n">attention_input</span> <span class="o">=</span> <span class="n">key_neighbors</span> <span class="o">-</span> <span class="n">query_expanded</span>  <span class="c1"># shape (N, K, out_dim)
# A vector difference rather than a scalar product
</span></code></pre></div></div> <p>This vector difference captures relative relationships without producing a full N×N matrix and without creating invalid cross-cloud attention.</p> <h3 id="position-encoding">Position encoding</h3> <p>Positions are explicitly encoded and added to the attention input:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">relative_positions</span> <span class="o">=</span> <span class="n">neighbor_positions</span> <span class="o">-</span> <span class="n">query_position</span>  <span class="c1"># (N, K, 3)
</span><span class="n">encoded_positions</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">relative_positions</span><span class="p">)</span>              <span class="c1"># (N, K, out_dim)
</span><span class="n">attention_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span> <span class="o">-</span> <span class="n">K</span><span class="p">)</span> <span class="o">+</span> <span class="n">encoded_positions</span>
</code></pre></div></div> <h3 id="vectorial-attention-with-groups">Vectorial attention with groups</h3> <p>Instead of a single scalar weight per neighbor, Point Transformer produces <strong><code class="language-plaintext highlighter-rouge">num_groups</code> weights per neighbor</strong>. Let’s understand why and how this works.</p> <h4 id="visual-diagram">Visual Diagram</h4> <p>Here’s what happens for <strong>one point</strong> with <strong>K=3 neighbors</strong> and <strong>num_groups=4, out_dim=16</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Each neighbor's value vector (16 dims):
┌─────┬─────┬─────┬─────┐
│ G0  │ G1  │ G2  │ G3  │  ← 4 groups of 4 dimensions
│ [4] │ [4] │ [4] │ [4] │
└─────┴─────┴─────┴─────┘

Attention weights for each neighbor (4 weights):
Neighbor 1: [w₁⁽⁰⁾, w₁⁽¹⁾, w₁⁽²⁾, w₁⁽³⁾]
Neighbor 2: [w₂⁽⁰⁾, w₂⁽¹⁾, w₂⁽²⁾, w₂⁽³⁾]
Neighbor 3: [w₃⁽⁰⁾, w₃⁽¹⁾, w₃⁽²⁾, w₃⁽³⁾]

Weighted multiplication:
            ┌─────────────────────────────────┐
Neighbor 1: │w₁⁽⁰⁾·G0│w₁⁽¹⁾·G1│w₁⁽²⁾·G2│w₁⁽³⁾·G3│
            ├─────────────────────────────────┤
Neighbor 2: │w₂⁽⁰⁾·G0│w₂⁽¹⁾·G1│w₂⁽²⁾·G2│w₂⁽³⁾·G3│
            ├─────────────────────────────────┤
Neighbor 3: │w₃⁽⁰⁾·G0│w₃⁽¹⁾·G1│w₃⁽²⁾·G2│w₃⁽³⁾·G3│
            └─────────────────────────────────┘
                        ↓ sum over neighbors
            ┌─────────────────────────────────┐
Output:     │  G0   │  G1   │  G2   │  G3   │  (16 dims)
            └─────────────────────────────────┘
</code></pre></div></div> <p>The shape <code class="language-plaintext highlighter-rouge">(N, K, num_groups, dim_per_group)</code> represents:</p> <ul> <li>For each of N points</li> <li>For each of K neighbors</li> <li>We have num_groups separate feature groups</li> <li>Each group has dim_per_group dimensions</li> </ul> <p>And each group gets its own attention weight, allowing fine-grained control over feature aggregation.</p> <p>For example a group may focus on</p> <ul> <li>Dimensions 0-15: color information</li> <li>Dimensions 16-31: geometric properties</li> <li>Dimensions 32-47: texture features</li> <li>Dimensions 48-63: semantic context</li> </ul> <hr/> <h2 id="transitiondown-spatial-downsampling">TransitionDown: Spatial Downsampling</h2> <p><code class="language-plaintext highlighter-rouge">TransitionDown</code> reduces the number of points (analogous to strided conv).</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/transitionDown_stride!=1.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Pipeline (high-level):</p> <ol> <li><strong>Compute new counts</strong>: for each cloud, new_count = old_count // stride.</li> <li><strong>Farthest Point Sampling (FPS)</strong>: choose M ≈ N/stride representative points that maximize minimal distance; ensures spatial coverage.</li> <li><strong>K-NN grouping</strong>: for each sampled point, gather its K neighbors in the original cloud (with relative positions if <code class="language-plaintext highlighter-rouge">use_xyz=True</code>). Result: <code class="language-plaintext highlighter-rouge">(M, K, 3 + in_dim)</code>.</li> <li><strong>Projection + normalization</strong>: linear on neighbor features, BatchNorm + ReLU → <code class="language-plaintext highlighter-rouge">(M, out_dim, K)</code>.</li> <li><strong>MaxPooling</strong>: aggregate K neighbors by channel-wise max → <code class="language-plaintext highlighter-rouge">(M, out_dim)</code>.</li> </ol> <p>Result: reduce N points to M points (M ≈ N/stride) with locally aggregated features.</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/fps_knn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="transitionup-upsampling-with-skip-connections">TransitionUp: Upsampling with Skip Connections</h2> <p><code class="language-plaintext highlighter-rouge">TransitionUp</code> increases resolution and fuses multi-scale information.</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/transitionUp_with_pxoo.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Explanation:</strong></p> <p>The interpolation transfers features from M source points to N target points (typically M &lt; N for upsampling).</p> <figure> <picture> <img src="/assets/img/pointTransformerv1/interpolation.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Algorithm:</strong></p> <ol> <li><strong>K-NN</strong>: For each target point, find its K=3 nearest neighbors in the source cloud</li> <li><strong>Weights</strong>: Compute normalized inverse distance weights: points closer to the target have higher weights</li> <li><strong>Interpolation</strong>: Weighted average of the K neighbor features</li> </ol> <p><strong>Code:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">interpolation</span><span class="p">(</span><span class="n">p_source</span><span class="p">,</span> <span class="n">p_target</span><span class="p">,</span> <span class="n">x_source</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Args:
        p_source: (M, 3) - source positions
        p_target: (N, 3) - target positions  
        x_source: (M, C) - source features
    Returns:
        output: (N, C) - interpolated features
    </span><span class="sh">"""</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">p_target</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_source</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="c1"># Find K nearest neighbors
</span>        <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">p_source</span> <span class="o">-</span> <span class="n">p_target</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">k_nearest</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[:</span><span class="n">K</span><span class="p">]</span>
        
        <span class="c1"># Inverse distance weighting
</span>        <span class="n">dists</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">k_nearest</span><span class="p">]</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">dists</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">/=</span> <span class="n">weights</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>  <span class="c1"># normalize
</span>        
        <span class="c1"># Weighted average
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">output</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">+=</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_source</span><span class="p">[</span><span class="n">k_nearest</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p><strong>Formula:</strong> \(\text{output}[n] = \sum_{i=0}^{K-1} w_i \cdot \text{x}_\text{source}[\text{neighbor}_i], \quad w_i = \frac{1/d_i}{\sum_j 1/d_j}\)</p> <h2 id="references">References</h2> <ul> <li>Point Transformer paper (ICCV 2021): <a href="https://arxiv.org/abs/2012.09164">https://arxiv.org/abs/2012.09164</a></li> <li>Official code: <a href="https://github.com/POSTECH-CVLab/point-transformer">https://github.com/POSTECH-CVLab/point-transformer</a></li> <li>See also my post on <a href="/blog/2025/batchingPointclouds/">Batching of Point Clouds</a></li> </ul>]]></content><author><name></name></author><category term="computer-vision"/><category term="deep-learning"/><category term="point-cloud"/><category term="transformer"/><category term="architecture"/><summary type="html"><![CDATA[Detailed analysis of the Point Transformer v1 architecture for point-cloud segmentation and classification]]></summary></entry><entry><title type="html">Batching PointClouds</title><link href="http://antoineach.github.io//blog/2025/batchingPointclouds/" rel="alternate" type="text/html" title="Batching PointClouds"/><published>2025-10-09T00:00:00+00:00</published><updated>2025-10-09T00:00:00+00:00</updated><id>http://antoineach.github.io//blog/2025/batchingPointclouds</id><content type="html" xml:base="http://antoineach.github.io//blog/2025/batchingPointclouds/"><![CDATA[<h2 id="️-characteristics-of-point-clouds">☁️ Characteristics of Point Clouds</h2> <ol> <li><strong>Variable size</strong> – each point cloud contains a different number of points \(N\).</li> <li><strong>Unordered</strong> – permuting the points does not change the represented object.</li> <li><strong>Irregular</strong> – there is no fixed neighborhood structure like in images.</li> <li><strong>Continuous</strong> – each point lives in continuous 3D space:<br/> \((x, y, z) \in \mathbb{R}^3\)</li> </ol> <hr/> <h2 id="️-the-variable-number-of-points-problem">⚠️ The Variable Number of Points Problem</h2> <p>The fact that each point cloud has a different number of points \(N\) prevents <strong>batch parallelization</strong> like in image-based neural networks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Classic computer vision: easy batching
</span><span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">224</span><span class="p">)</span>
<span class="c1"># ✅ All images have the same shape → can be stacked together
</span>
<span class="c1"># With point clouds — impossible!
</span><span class="n">obj1</span> <span class="o">=</span> <span class="mi">1523</span> <span class="n">points</span>   <span class="c1"># chair
</span><span class="n">obj2</span> <span class="o">=</span> <span class="mi">3891</span> <span class="n">points</span>   <span class="c1"># table
</span><span class="n">obj3</span> <span class="o">=</span> <span class="mi">892</span> <span class="n">points</span>    <span class="c1"># lamp
</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">obj1</span><span class="p">,</span> <span class="n">obj2</span><span class="p">,</span> <span class="n">obj3</span><span class="p">])</span>  <span class="c1"># ❌ Different sizes → error!
</span></code></pre></div></div> <hr/> <h2 id="-common-strategies-to-handle-variable-point-counts">🧩 Common Strategies to Handle Variable Point Counts</h2> <h3 id="1️⃣-batch-size--1">1️⃣ Batch Size = 1</h3> <p>Process each object individually. → <strong>Drawback:</strong> training is extremely slow, and statistical relations between samples in a batch are lost.</p> <hr/> <h3 id="2️⃣-downsampling">2️⃣ <strong>Downsampling</strong></h3> <p>Randomly sample each point cloud to reach a fixed size (e.g. 1024 points). → <strong>Pros:</strong> Simple to implement → <strong>Cons:</strong> Loss of geometric detail, especially when point counts differ greatly (e.g. from 1k to 10k → 90% data loss).</p> <hr/> <h3 id="3️⃣-oversampling">3️⃣ <strong>Oversampling</strong></h3> <p>Duplicate some points to reach the target size ( \(N' = N + \Delta N\) ). This works for architectures like <strong>PointNet</strong>, since each point is independently projected via a shared MLP, then aggregated with <strong>max pooling</strong>.</p> <div class="row justify-content-center"> <div class="col-md-8 text-center"> <figure> <picture> <img src="/assets/img/maxpool.png" class="img-fluid rounded z-depth-1 shadow-sm" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2 text-muted">Example of PointNet using shared MLP + max pooling.</div> </div> </div> <p>However, if we replaced max pooling with <strong>mean pooling</strong>, duplicates would bias the average and distort the representation.</p> <hr/> <h3 id="4️⃣-sparse-tensor-representation-practical-solution">4️⃣ <strong>Sparse Tensor Representation (Practical Solution)</strong></h3> <p>In practice, frameworks like <strong>Torch Scatter</strong> allow concatenation of all points from a batch while preserving object boundaries.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Instead of stacking → concatenate all points
</span><span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">obj1_points</span><span class="p">,</span> <span class="n">obj2_points</span><span class="p">,</span> <span class="n">obj3_points</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [6306, 3]  (x, y, z)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">obj1_features</span><span class="p">,</span> <span class="n">obj2_features</span><span class="p">,</span> <span class="n">obj3_features</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [6306, 64]  (features)
</span>
<span class="c1"># Track where each object ends using offsets
</span><span class="n">o</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1523</span><span class="p">,</span> <span class="mi">5414</span><span class="p">,</span> <span class="mi">6306</span><span class="p">])</span>  <span class="c1"># cumulative end indices
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|----------obj1----------|---------------obj2--------------|---obj3---|
0                       1523                             5414        6306
                         ↑                                 ↑          ↑
                      offset[0]                       offset[1]  offset[2]
</code></pre></div></div> <p>These <strong>offsets</strong> let the model know where each object starts and ends in the concatenated tensor.</p> <div class="row justify-content-center"> <div class="col-md-8 text-center"> <figure> <picture> <img src="/assets/img/torch_scatter.svg" class="img-fluid rounded z-depth-1 shadow-sm" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2 text-muted">Example of Torch Scatter add operator.</div> </div> </div> <hr/> <h1 id="other-operator-that-supports-the-concatenation-of-pointclouds--nnlinear">Other operator that supports the concatenation of pointclouds : nn.Linear</h1> <h2 id="definition">Definition</h2> <p>The <code class="language-plaintext highlighter-rouge">torch.nn.Linear</code> layer applies an affine linear transformation:</p> \[y = xA^T + b\] <p>Where:</p> <ul> <li>\(x \in \mathbb{R}^{n \times d_{in}}\) is the input matrix (n samples, \(d_{in}\) input features)</li> <li>\(A \in \mathbb{R}^{d_{out} \times d_{in}}\) is the weight matrix</li> <li>\(b \in \mathbb{R}^{d_{out}}\) is the bias vector</li> <li>\(y \in \mathbb{R}^{n \times d_{out}}\) is the output matrix</li> </ul> <p>The bias \(b\) is broadcast and added to each row of \(xA^T\).</p> <h2 id="application-to-concatenated-point-clouds">Application to Concatenated Point Clouds</h2> <h3 id="setup">Setup</h3> <p>Consider two point clouds with different numbers of points:</p> <ul> <li>Point cloud 1: $N_1 = 2$ points</li> <li>Point cloud 2: $N_2 = 3$ points</li> <li>Each point has 3 coordinates (x, y, z)</li> </ul> <p><strong>Point Cloud 1:</strong> \(X_1 = \begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\ x_{21} &amp; x_{22} &amp; x_{23} \end{bmatrix} \in \mathbb{R}^{2 \times 3}\)</p> <p><strong>Point Cloud 2:</strong> \(X_2 = \begin{bmatrix} x_{31} &amp; x_{32} &amp; x_{33} \\ x_{41} &amp; x_{42} &amp; x_{43} \\ x_{51} &amp; x_{52} &amp; x_{53} \end{bmatrix} \in \mathbb{R}^{3 \times 3}\)</p> <h3 id="concatenation">Concatenation</h3> <p>Concatenate along the point dimension:</p> \[X = \begin{bmatrix} X_1 \\ X_2 \end{bmatrix} = \begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\ x_{21} &amp; x_{22} &amp; x_{23} \\ x_{31} &amp; x_{32} &amp; x_{33} \\ x_{41} &amp; x_{42} &amp; x_{43} \\ x_{51} &amp; x_{52} &amp; x_{53} \end{bmatrix} \in \mathbb{R}^{5 \times 3}\] <h3 id="linear-transformation">Linear Transformation</h3> <p>Apply <code class="language-plaintext highlighter-rouge">nn.Linear(in_features=3, out_features=2)</code>:</p> <p><strong>Weight matrix:</strong> \(A = \begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13} \\ w_{21} &amp; w_{22} &amp; w_{23} \end{bmatrix} \in \mathbb{R}^{2 \times 3}\)</p> <p><strong>Bias vector:</strong> \(b = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix} \in \mathbb{R}^{2}\)</p> <p><strong>Transpose of weight matrix:</strong> \(A^T = \begin{bmatrix} w_{11} &amp; w_{21} \\ w_{12} &amp; w_{22} \\ w_{13} &amp; w_{23} \end{bmatrix} \in \mathbb{R}^{3 \times 2}\)</p> <h3 id="matrix-multiplication-y--xat--b">Matrix Multiplication: $Y = XA^T + b$</h3> \[Y = \begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\ x_{21} &amp; x_{22} &amp; x_{23} \\ x_{31} &amp; x_{32} &amp; x_{33} \\ x_{41} &amp; x_{42} &amp; x_{43} \\ x_{51} &amp; x_{52} &amp; x_{53} \end{bmatrix} \begin{bmatrix} w_{11} &amp; w_{21} \\ w_{12} &amp; w_{22} \\ w_{13} &amp; w_{23} \end{bmatrix} + \begin{bmatrix} b_1 &amp; b_2 \end{bmatrix}\] <h3 id="row-by-row-computation">Row-by-Row Computation</h3> <p>Each output row is computed independently:</p> \[y_1 = \begin{bmatrix} x_{11}w_{11} + x_{12}w_{12} + x_{13}w_{13} + b_1 &amp; x_{11}w_{21} + x_{12}w_{22} + x_{13}w_{23} + b_2 \end{bmatrix}\] \[y_2 = \begin{bmatrix} x_{21}w_{11} + x_{22}w_{12} + x_{23}w_{13} + b_1 &amp; x_{21}w_{21} + x_{22}w_{22} + x_{23}w_{23} + b_2 \end{bmatrix}\] \[y_3 = \begin{bmatrix} x_{31}w_{11} + x_{32}w_{12} + x_{33}w_{13} + b_1 &amp; x_{31}w_{21} + x_{32}w_{22} + x_{33}w_{23} + b_2 \end{bmatrix}\] \[y_4 = \begin{bmatrix} x_{41}w_{11} + x_{42}w_{12} + x_{43}w_{13} + b_1 &amp; x_{41}w_{21} + x_{42}w_{22} + x_{43}w_{23} + b_2 \end{bmatrix}\] \[y_5 = \begin{bmatrix} x_{51}w_{11} + x_{52}w_{12} + x_{53}w_{13} + b_1 &amp; x_{51}w_{21} + x_{52}w_{22} + x_{53}w_{23} + b_2 \end{bmatrix}\] <h2 id="key-property-no-mixing-between-points">Key Property: No Mixing Between Points</h2> <p>Each output row $y_i$ depends <strong>only</strong> on its corresponding input row \(x_i\):</p> \[y_i = x_i A^T + b, \quad i = 1, 2, \ldots, 5\] <p>Therefore:</p> <ul> <li>Rows 1-2 (from PC1) are transformed independently</li> <li>Rows 3-5 (from PC2) are transformed independently</li> <li><strong>No information is mixed between different points or different point clouds</strong></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[A comprehensive explanation of Batching PointClouds.]]></summary></entry><entry><title type="html">MultiHead Attention Visualized</title><link href="http://antoineach.github.io//blog/2025/multiheadattention/" rel="alternate" type="text/html" title="MultiHead Attention Visualized"/><published>2025-10-08T00:00:00+00:00</published><updated>2025-10-08T00:00:00+00:00</updated><id>http://antoineach.github.io//blog/2025/multiheadattention</id><content type="html" xml:base="http://antoineach.github.io//blog/2025/multiheadattention/"><![CDATA[<p>This post is adapted from <a href="https://www.geeksforgeeks.org/nlp/multi-head-attention-mechanism/">GeeksforGeeks</a>’s article on the Multi-Head Attention Mechanism by <em>sanjulika_sharma</em>.<br/> It provides an intuitive understanding of how multiple attention heads work in parallel to capture different representation subspaces.</p> <hr/> <h2 id="-what-is-multi-head-attention">🧠 What is Multi-Head Attention?</h2> <p>The Multi-Head Attention mechanism allows a model to <strong>focus on different parts of a sequence simultaneously</strong>.<br/> Each head learns different contextual relationships — for example, one might focus on word order while another captures long-range dependencies.</p> <hr/> <h2 id="-visualization">📊 Visualization</h2> <p>Below is a simple diagram illustrating how queries (Q), keys (K), and values (V) interact across multiple heads.</p> <div class="row justify-content-center mt-4"> <div class="col-md-8 text-center"> <figure> <picture> <img src="/assets/img/multiheadAttention.svg" class="img-fluid rounded z-depth-1 shadow-sm" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2 text-muted"> Multi-Head Attention — each head performs scaled dot-product attention in parallel. </div> </div> </div> <hr/> <h2 id="️-implementation-example">⚙️ Implementation Example</h2> <p>Below is a PyTorch implementation of <strong>Multi-Head Attention</strong>.<br/> It combines several attention heads computed in parallel, each with its own query, key, and value subspace.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiheadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">H</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_dim</span> <span class="o">=</span> <span class="n">in_dim</span>      <span class="c1"># Input embedding size
</span>        <span class="n">self</span><span class="p">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">D</span>                <span class="c1"># Model embedding size
</span>        <span class="n">self</span><span class="p">.</span><span class="n">H</span> <span class="o">=</span> <span class="n">H</span>                <span class="c1"># Number of attention heads
</span>
        <span class="c1"># Compute Q, K, V for all heads at once
</span>        <span class="n">self</span><span class="p">.</span><span class="n">qkv_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">D</span><span class="p">)</span>
        <span class="c1"># Final projection layer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">in_dim</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>

        <span class="c1"># 1️⃣ Compute concatenated Q, K, V
</span>        <span class="n">qkv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">qkv_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, N, 3*D)
</span>
        <span class="c1"># 2️⃣ Split heads
</span>        <span class="n">qkv</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">H</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">D</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">H</span><span class="p">)</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Each: (B, H, N, D//H)
</span>
        <span class="c1"># 3️⃣ Scaled dot-product attention
</span>        <span class="n">d_k</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scaled</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">scaled</span> <span class="o">+=</span> <span class="n">mask</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scaled</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 4️⃣ Apply attention to values
</span>        <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="c1"># 5️⃣ Concatenate heads
</span>        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">D</span><span class="p">)</span>

        <span class="c1"># 6️⃣ Final linear projection
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_layer</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="-key-takeaway">🧩 Key Takeaway</h2> <blockquote> <p>Multi-Head Attention enhances a model’s representational capacity by letting it attend to information from <strong>different representation subspaces</strong> simultaneously — leading to richer contextual understanding.</p> </blockquote> <hr/>]]></content><author><name></name></author><summary type="html"><![CDATA[A comprehensive explanation of MultiHead Attention with dimensions.]]></summary></entry></feed>