---
layout: post
title: "Point Transformer v2: Architecture and Implementation Details"
date: 2025-10-26
description: Detailed analysis of the Point Transformer v2 architecture for point-cloud segmentation and classification
tags: deep-learning point-cloud transformer architecture
categories: computer-vision
---

# Point Transformer v2: Architecture et AmÃ©liorations

## Introduction

**Point Transformer v2** amÃ©liore significativement son prÃ©dÃ©cesseur en termes d'efficacitÃ© computationnelle et de performances. Les innovations clÃ©s incluent :

- **Grid Pooling** au lieu de Furthest Point Sampling (3-5Ã— plus rapide)
- **Map Unpooling** qui rÃ©utilise l'information du downsampling
- **GroupedLinear** pour rÃ©duire drastiquement le nombre de paramÃ¨tres
- **Attention vectorielle enrichie** avec encodage de position sur les values
- **Masking des voisins invalides** pour gÃ©rer les nuages de tailles variables

CommenÃ§ons par l'architecture globale avant de dÃ©tailler chaque composant.

---

## Architecture Globale

{% include figure.liquid 
   path="assets/img/pointTransformerv2/main_architecture.svg" 
   class="img-fluid rounded z-depth-1" 
   style="height:400px; object-fit:contain;"
%}

PTv2 suit une architecture U-Net avec :

**Encodeur (Downsampling):**
```
Input (N points, in_channels)
    â†“ GVAPatchEmbed
N points, 48 channels
    â†“ Encoder 1 (GridPool)
N1 points, 96 channels
    â†“ Encoder 2 (GridPool)
N2 points, 192 channels
    â†“ Encoder 3 (GridPool)
N3 points, 384 channels
    â†“ Encoder 4 (GridPool)
N4 points, 512 channels [BOTTLENECK]
```

**DÃ©codeur (Upsampling):**
```
N4 points, 512 channels
    â†“ Decoder 4 (Unpool + skip)
N3 points, 384 channels
    â†“ Decoder 3 (Unpool + skip)
N2 points, 192 channels
    â†“ Decoder 2 (Unpool + skip)
N1 points, 96 channels
    â†“ Decoder 1 (Unpool + skip)
N points, 48 channels
    â†“ Segmentation Head
N points, num_classes
```

**Points clÃ©s:**
- Chaque **Encoder** rÃ©duit le nombre de points via **GridPool** (voxelisation)
- Chaque **Decoder** remonte en rÃ©solution via **Map Unpooling** + skip connection
- Les **clusters** stockent le mapping de voxelisation pour l'unpooling
- **Pas de Furthest Point Sampling** â†’ beaucoup plus rapide !

---

## GroupedLinear : RÃ©duction ParamÃ©trique Intelligente

### Le problÃ¨me avec Linear classique

Dans un rÃ©seau profond, gÃ©nÃ©rer des poids d'attention via des couches Linear standard accumule rapidement des paramÃ¨tres :

```python
# Linear classique pour gÃ©nÃ©rer 8 poids d'attention depuis 64 features
Linear(in_features=64, out_features=8)
# ParamÃ¨tres: 64 Ã— 8 = 512 poids + 8 biais = 520 paramÃ¨tres
```

### L'innovation GroupedLinear

{% include figure.liquid path="assets/img/poinTransformerV2/groupedLinear.svg" class="img-fluid rounded z-depth-1" %}

**GroupedLinear** remplace la matrice de poids par un **vecteur de poids partagÃ©** :

```python
# GroupedLinear
weight: (1, 64)  # UN SEUL vecteur au lieu d'une matrice
# ParamÃ¨tres: 64 (pas de biais)
```

### Fonctionnement Ã©tape par Ã©tape

```python
def forward(self, input):
    # input: (N, in_features) = (N, 64)
    # weight: (1, in_features) = (1, 64)
    
    # Ã‰tape 1: Multiplication Ã©lÃ©ment par Ã©lÃ©ment
    temp = input * weight  # (N, 64)
    
    # Ã‰tape 2: Reshape en groupes
    temp = temp.reshape(N, groups, in_features/groups)
    # temp: (N, 8, 8)
    
    # Ã‰tape 3: Somme par groupe
    output = temp.sum(dim=-1)  # (N, 8)
    
    return output
```

### Exemple numÃ©rique concret

Prenons **N=1, in_features=8, groups=4** pour simplifier :

```python
# Input
x = [2, 3, 1, 4, 5, 2, 3, 1]  # (8,)

# Weight (vecteur partagÃ©)
w = [0.5, 1.0, 0.2, 0.8, 0.3, 0.9, 0.4, 0.7]  # (8,)

# Ã‰tape 1: Multiplication Ã©lÃ©ment par Ã©lÃ©ment
temp = [2Ã—0.5, 3Ã—1.0, 1Ã—0.2, 4Ã—0.8, 5Ã—0.3, 2Ã—0.9, 3Ã—0.4, 1Ã—0.7]
     = [1.0, 3.0, 0.2, 3.2, 1.5, 1.8, 1.2, 0.7]

# Ã‰tape 2: Reshape en 4 groupes de 2 dimensions
temp_grouped = [
    [1.0, 3.0],     # Groupe 0
    [0.2, 3.2],     # Groupe 1
    [1.5, 1.8],     # Groupe 2
    [1.2, 0.7]      # Groupe 3
]

# Ã‰tape 3: Somme par groupe
output = [
    1.0 + 3.0 = 4.0,    # Groupe 0
    0.2 + 3.2 = 3.4,    # Groupe 1
    1.5 + 1.8 = 3.3,    # Groupe 2
    1.2 + 0.7 = 1.9     # Groupe 3
]
# RÃ©sultat: [4.0, 3.4, 3.3, 1.9]
```

### Comparaison des paramÃ¨tres

| Configuration | Linear classique | GroupedLinear | RÃ©duction |
|---------------|------------------|---------------|-----------|
| 64 â†’ 8 | 64Ã—8 = **512** | **64** | 8Ã— |
| 128 â†’ 16 | 128Ã—16 = **2048** | **128** | 16Ã— |
| 256 â†’ 32 | 256Ã—32 = **8192** | **256** | 32Ã— |

### Pourquoi Ã§a fonctionne ?

**Inductive bias structurÃ© :** GroupedLinear force le modÃ¨le Ã  utiliser les mÃªmes poids pour tous les groupes, mais appliquÃ©s sur des portions diffÃ©rentes de l'input. C'est comme dire :

*"Les 8 premiers channels utilisent les poids wâ‚€-wâ‚‡ pour former le poids d'attention du groupe 0, les 8 suivants utilisent les poids wâ‚ˆ-wâ‚â‚… pour le groupe 1, etc."*

Cette contrainte :
- âœ… RÃ©duit le risque d'overfitting (moins de paramÃ¨tres)
- âœ… Force des reprÃ©sentations plus gÃ©nÃ©rales
- âœ… Maintient les performances (validÃ© empiriquement dans le papier)

---

## GroupedVectorAttention : Attention Locale Enrichie

### Vue d'ensemble

`GroupedVectorAttention` est le cÅ“ur de PTv2, avec plusieurs amÃ©liorations par rapport Ã  PTv1.

{% include figure.liquid path="assets/img/poinTransformerV2/groupedVectorAttention.svg" class="img-fluid rounded z-depth-1" %}

**DiffÃ©rences clÃ©s avec PTv1:**

| Aspect | PTv1 | PTv2 |
|--------|------|------|
| **Position Encoding sur values** | âŒ Non | âœ… Oui |
| **Masking voisins invalides** | âŒ Non | âœ… Oui |
| **Weight generation** | MLP standard | **GroupedLinear** (8Ã— moins de params) |
| **Normalization** | BatchNorm aprÃ¨s Linear | **BatchNorm + ReLU entre** Q/K/V |

### Innovation 1 : Position Encoding sur les Values

**Dans PTv1**, l'encodage de position n'Ã©tait ajoutÃ© qu'Ã  la relation Q-K :

```python
# PTv1
relation_qk = (key - query) + position_encoding(pos)
# Les values ne sont PAS affectÃ©es par la gÃ©omÃ©trie
```

**Dans PTv2**, on ajoute aussi l'encodage aux **values** :

```python
# PTv2
pe_bias = MLP(relative_positions)  # (N, K, 3) â†’ (N, K, C)

# Sur la relation Q-K (comme PTv1)
relation_qk = (key - query) + pe_bias

# NOUVEAU: aussi sur les values !
value = value + pe_bias
```

**Pourquoi c'est important ?**

L'encodage de position sur les values permet d'**injecter directement l'information gÃ©omÃ©trique** dans les features qui seront agrÃ©gÃ©es.

**Exemple physique :**

Imaginons un point reprÃ©sentant le coin d'une table, avec 3 voisins :

```
Voisin 1: dessus de la table (Î”pos = [0, 0, 0.05m])
    â†’ pe_biasâ‚ = [0.8, 0.1, 0.1, ...]  # proche, mÃªme surface
    
Voisin 2: pied de table (Î”pos = [0, 0, -0.8m])
    â†’ pe_biasâ‚‚ = [0.2, 0.1, 0.9, ...]  # loin, objet diffÃ©rent
    
Voisin 3: air vide (Î”pos = [0.5, 0, 0])
    â†’ pe_biasâ‚ƒ = [0.1, 0.8, 0.1, ...]  # Ã©loignÃ© latÃ©ralement
```

Ces encodages, ajoutÃ©s aux values, permettent au modÃ¨le de savoir **oÃ¹ se trouvent gÃ©omÃ©triquement** les features qu'il agrÃ¨ge, pas seulement leur importance relative (via les poids d'attention).

### Innovation 2 : Masking des Voisins Invalides

**ProblÃ¨me :** Dans un batch, certains points (au bord du nuage, ou dans des rÃ©gions peu denses) ont moins de K voisins. Le K-NN "pad" avec des indices `-1`.

**Solution PTv2 :**

```python
# reference_index: (N, K)
# Contient -1 pour les voisins invalides (padding)

# CrÃ©ation du masque
mask = torch.sign(reference_index + 1)  # (N, K)
# Si reference_index[n, k] = -1  â†’ sign(-1+1) = sign(0) = 0
# Si reference_index[n, k] â‰¥ 0   â†’ sign(â‰¥1) = 1

# Application sur les poids d'attention
attention_weights = attention_weights * mask.unsqueeze(-1)
# Shape: (N, K, groups) Ã— (N, K, 1) â†’ (N, K, groups)
```

**Exemple concret :**

```python
# Point isolÃ© au bord du nuage avec seulement 3 vrais voisins
reference_index[point_42] = [15, 23, 8, -1, -1, -1, -1, -1, ...]  # K=16
mask[point_42] = [1, 1, 1, 0, 0, 0, 0, 0, ...]

# Poids d'attention avant masking (aprÃ¨s softmax)
attention[point_42] = [
    [0.3, 0.2, ...],  # Voisin 15 (valide)
    [0.25, 0.18, ...], # Voisin 23 (valide)
    [0.2, 0.15, ...],  # Voisin 8 (valide)
    [0.08, 0.12, ...], # Invalide mais a des poids !
    [0.05, 0.10, ...], # Invalide
    ...
]

# AprÃ¨s masking
attention[point_42] = [
    [0.3, 0.2, ...],   # OK
    [0.25, 0.18, ...], # OK
    [0.2, 0.15, ...],  # OK
    [0, 0, ...],       # AnnulÃ© âœ“
    [0, 0, ...],       # AnnulÃ© âœ“
    ...
]
```

Sans ce masking, les voisins "padding" contribueraient avec des **features alÃ©atoires/garbage**, polluant l'agrÃ©gation finale !

### Innovation 3 : GroupedLinear pour les Poids d'Attention

Au lieu d'un MLP standard `Linear(C, groups)` avec CÃ—groups paramÃ¨tres, PTv2 utilise `GroupedLinear(C, groups)` avec seulement C paramÃ¨tres.

```python
# PTv1: MLP standard
self.linear_w = nn.Sequential(
    nn.Linear(mid_planes, mid_planes // share_planes),  # C Ã— C/G paramÃ¨tres
    ...
)

# PTv2: avec GroupedLinear
self.weight_encoding = nn.Sequential(
    GroupedLinear(embed_channels, groups, groups),  # Seulement C paramÃ¨tres !
    ...
)
```

**Gain :** 8Ã— moins de paramÃ¨tres pour gÃ©nÃ©rer les poids d'attention, sans perte de performance.

### Flux Complet avec Exemple NumÃ©rique

Prenons un exemple complet avec **N=1000 points, K=16 voisins, C=64 channels, groups=8**.

#### Ã‰tape 1 : Projections Q, K, V

```python
# Input
feat: (1000, 64)

# Projections avec normalisation
query = Linear(feat) â†’ BatchNorm1d â†’ ReLU  # (1000, 64)
key = Linear(feat) â†’ BatchNorm1d â†’ ReLU    # (1000, 64)
value = Linear(feat)                        # (1000, 64)
```

#### Ã‰tape 2 : Grouping des Voisins

```python
# RÃ©cupÃ©ration des K voisins via reference_index
key_neighbors = grouping(reference_index, key, coord, with_xyz=True)
# Shape: (1000, 16, 3+64) = (1000, 16, 67)
# Les 3 premiÃ¨res dims sont les positions relatives

value_neighbors = grouping(reference_index, value, coord, with_xyz=False)
# Shape: (1000, 16, 64)
```

**Note :** `reference_index` (N, K) contient les indices des K voisins pour chaque point, prÃ©-calculÃ©s dans `BlockSequence`.

#### Ã‰tape 3 : SÃ©paration Positions / Features

```python
relative_positions = key_neighbors[:, :, 0:3]  # (1000, 16, 3)
key_neighbors = key_neighbors[:, :, 3:]        # (1000, 16, 64)
```

#### Ã‰tape 4 : Encodage des Positions

```python
# MLP sur positions relatives
pe_bias = MLP(relative_positions)  # (1000, 16, 3) â†’ (1000, 16, 64)
# Le MLP transforme les positions 3D en features de dimension C
```

**Exemple pour un point :**
```python
# Positions relatives de ses 16 voisins
relative_positions[point_0] = [
    [0.05, 0.02, 0.01],   # Voisin trÃ¨s proche
    [0.15, -0.10, 0.05],  # Voisin moyen
    [0.50, 0.30, -0.20],  # Voisin lointain
    ...
]

# Encodage via MLP
pe_bias[point_0] = [
    [0.8, 0.2, 0.1, ...],   # Features pour voisin proche
    [0.5, 0.4, 0.3, ...],   # Features pour voisin moyen
    [0.2, 0.1, 0.9, ...],   # Features pour voisin lointain
    ...
]
```

#### Ã‰tape 5 : Application Position Encoding

```python
# Sur la relation Q-K
relation_qk = (key_neighbors - query.unsqueeze(1)) + pe_bias
# Shape: (1000, 16, 64)

# Sur les values (NOUVEAU dans PTv2 !)
value_with_pos = value_neighbors + pe_bias
# Shape: (1000, 16, 64)
```

#### Ã‰tape 6 : GÃ©nÃ©ration des Poids d'Attention

```python
# MLP contenant GroupedLinear
attention_scores = weight_encoding(relation_qk)
# Shape: (1000, 16, 64) â†’ (1000, 16, 8)

# Normalization: softmax sur les voisins
attention_weights = softmax(attention_scores, dim=1)
# Shape: (1000, 16, 8)
# Pour chaque point, les poids des 16 voisins somment Ã  1 (par groupe)
```

#### Ã‰tape 7 : Masking

```python
mask = sign(reference_index + 1)  # (1000, 16)
attention_weights = attention_weights * mask.unsqueeze(-1)
# Shape: (1000, 16, 8)
# Les poids des voisins invalides (-1) sont mis Ã  0
```

#### Ã‰tape 8 : AgrÃ©gation par Groupes

```python
# Reshape values en groupes
value_grouped = value_with_pos.view(1000, 16, 8, 8)
# Shape: (N, K, groups, C/groups)

# PrÃ©paration des poids pour broadcasting
attention_exp = attention_weights.unsqueeze(-1)
# Shape: (1000, 16, 8, 1)

# Multiplication groupe par groupe
weighted = value_grouped * attention_exp
# Shape: (1000, 16, 8, 8)

# AgrÃ©gation sur les K=16 voisins
aggregated = weighted.sum(dim=1)
# Shape: (1000, 8, 8)

# Flatten
output = aggregated.reshape(1000, 64)
# Shape: (1000, 64)
```

**Visualisation pour un point avec 3 voisins :**

```
Point central â—‰ avec 3 voisins:

Voisin 1 â—â‚: value = [vâ‚â°, vâ‚Â¹, ..., vâ‚â¶Â³]
    â†’ DÃ©coupe en 8 groupes de 8 dims
    â†’ Poids: [wâ‚â°=0.5, wâ‚Â¹=0.3, ..., wâ‚â·=0.2]
    
Voisin 2 â—â‚‚: value = [vâ‚‚â°, vâ‚‚Â¹, ..., vâ‚‚â¶Â³]
    â†’ DÃ©coupe en 8 groupes de 8 dims
    â†’ Poids: [wâ‚‚â°=0.3, wâ‚‚Â¹=0.4, ..., wâ‚‚â·=0.5]
    
Voisin 3 â—â‚ƒ: value = [vâ‚ƒâ°, vâ‚ƒÂ¹, ..., vâ‚ƒâ¶Â³]
    â†’ DÃ©coupe en 8 groupes de 8 dims
    â†’ Poids: [wâ‚ƒâ°=0.2, wâ‚ƒÂ¹=0.3, ..., wâ‚ƒâ·=0.3]

AgrÃ©gation pour le groupe g=0 (dims 0-7):
output[groupe_0] = wâ‚â° Ã— valueâ‚[0:8] + wâ‚‚â° Ã— valueâ‚‚[0:8] + wâ‚ƒâ° Ã— valueâ‚ƒ[0:8]
                 = 0.5 Ã— [vâ‚â°,...,vâ‚â·] + 0.3 Ã— [vâ‚‚â°,...,vâ‚‚â·] + 0.2 Ã— [vâ‚ƒâ°,...,vâ‚ƒâ·]

... rÃ©pÃ©tÃ© pour les 8 groupes
```

---
Voici la section comparative enrichie pour GroupedVectorAttention :

---

## GroupedVectorAttention : Attention Locale Enrichie

### Vue d'ensemble

`GroupedVectorAttention` est le cÅ“ur de PTv2, avec plusieurs amÃ©liorations par rapport Ã  PTv1.

{% include figure.liquid path="assets/img/poinTransformerV2/groupedVectorAttention.svg" class="img-fluid rounded z-depth-1" %}

### Comparaison dÃ©taillÃ©e avec PTv1

| Aspect | PTv1 (PointTransformerLayer) | PTv2 (GroupedVectorAttention) |
|--------|------------------------------|-------------------------------|
| **Projections Q, K, V** | Simple Linear | Linear + **BatchNorm1d + ReLU** |
| **Position Encoding** | Additif uniquement | Additif (+ option multiplicatif) |
| **PE sur values** | âŒ Non | âœ… **Oui** |
| **Masking voisins invalides** | âŒ Non (assume tous valides) | âœ… **Oui** |
| **Weight generation** | MLP standard (CÃ—C/G params) | **GroupedLinear** (C params seulement) |
| **Normalisation** | AprÃ¨s weight encoding | **Avant et aprÃ¨s** attention |

### Innovation 1 : Normalisation des Projections Q, K, V

**PTv1 :**
```python
# Projections simples sans normalisation
self.linear_q = nn.Linear(in_planes, mid_planes)
self.linear_k = nn.Linear(in_planes, mid_planes)
self.linear_v = nn.Linear(in_planes, out_planes)

# Usage
query = self.linear_q(feat)  # (N, C)
key = self.linear_k(feat)    # (N, C)
value = self.linear_v(feat)  # (N, C)
```

**PTv2 :**
```python
# Projections avec normalisation et activation
self.linear_q = nn.Sequential(
    nn.Linear(embed_channels, embed_channels, bias=qkv_bias),
    PointBatchNorm(embed_channels),  # Normalisation !
    nn.ReLU(inplace=True)            # Activation !
)
# Idem pour linear_k

# Usage
query = self.linear_q(feat)  # (N, C) - normalisÃ© et activÃ©
```

**Pourquoi c'est important ?**

La normalisation des Q, K stabilise l'entraÃ®nement en Ã©vitant des valeurs extrÃªmes dans la relation Q-K :

```python
# Sans normalisation (PTv1)
query = Linear(feat)  # Peut avoir de grandes variations
key = Linear(feat)    
relation_qk = key - query  # Peut exploser en magnitude !

# Avec normalisation (PTv2)
query = Linear(feat) â†’ BatchNorm â†’ ReLU  # ContrÃ´lÃ© et stable
key = Linear(feat) â†’ BatchNorm â†’ ReLU
relation_qk = key - query  # Magnitude stable
```

**Impact :** Convergence plus rapide et training plus stable.

---

### Innovation 2 : Position Encoding sur les Values

**PTv1 :** L'encodage de position n'est ajoutÃ© qu'Ã  la relation Q-K

```python
# Code PTv1 (simplifiÃ©)
relative_positions = neighbor_positions - query_position  # (N, K, 3)
encoded_positions = MLP(relative_positions)               # (N, K, out_dim)

# Application UNIQUEMENT sur relation Q-K
relation_qk = (key - query) + encoded_positions
# Les values ne sont PAS modifiÃ©es par la gÃ©omÃ©trie

# AgrÃ©gation
output = sum(attention_weights * value)
```

**PTv2 :** L'encodage est ajoutÃ© Ã  la relation Q-K **ET aux values**

```python
# Code PTv2
pe_bias = MLP(relative_positions)  # (N, K, C)

# Sur la relation Q-K (comme PTv1)
relation_qk = (key - query) + pe_bias

# NOUVEAU: aussi sur les values !
value = value + pe_bias

# AgrÃ©gation (values contiennent maintenant l'info gÃ©omÃ©trique)
output = sum(attention_weights * value)
```

**Exemple physique comparatif :**

Imaginons un point reprÃ©sentant le coin d'une table avec 3 voisins :

```
Voisin 1: dessus de la table    (Î”pos = [0, 0, 0.05m])
Voisin 2: pied de table         (Î”pos = [0, 0, -0.8m])
Voisin 3: air environnant       (Î”pos = [0.5, 0, 0])
```

**Avec PTv1 :**
```python
# Encodage position
pe = MLP([0, 0, 0.05]) â†’ [0.8, 0.1, 0.1, ...]  # proche

# Application sur attention seulement
relation_qk[voisin_1] = (key - query) + pe
# â†’ Le poids d'attention capture la gÃ©omÃ©trie

# Mais la value reste inchangÃ©e !
value[voisin_1] = [semantic_features...]  # Pas d'info gÃ©omÃ©trique

# AgrÃ©gation
output = 0.6 Ã— value[voisin_1] + ...
#        â†‘ poids tient compte de la gÃ©omÃ©trie
#            â†‘ mais la value non !
```

**Avec PTv2 :**
```python
# Encodage position
pe = MLP([0, 0, 0.05]) â†’ [0.8, 0.1, 0.1, ...]

# Application sur attention (comme PTv1)
relation_qk[voisin_1] = (key - query) + pe

# NOUVEAU: aussi sur la value !
value[voisin_1] = value_original + pe
# â†’ La value contient maintenant l'info : "je suis proche et au-dessus"

# AgrÃ©gation
output = 0.6 Ã— value[voisin_1] + ...
#        â†‘ poids gÃ©omÃ©trique
#            â†‘ value aussi gÃ©omÃ©trique !
```

**Intuition :** PTv2 permet au modÃ¨le d'apprendre des patterns du type :

*"Quand j'agrÃ¨ge des features de voisins proches au-dessus de moi (dessus de table), leurs features doivent Ãªtre modifiÃ©es diffÃ©remment que des voisins lointains en-dessous (pied de table)"*

PTv1 ne pouvait capturer cela que via les poids d'attention - les features agrÃ©gÃ©es Ã©taient "aveugles" Ã  la gÃ©omÃ©trie.

---

### Innovation 3 : Masking des Voisins Invalides

**ProblÃ¨me commun :** Dans un batch, certains points ont moins de K voisins.

**PTv1 : Pas de masking explicite**

```python
# PTv1 assume que tous les voisins sont valides
# Si un point a seulement 10 voisins au lieu de 16 :
# - Les 10 vrais voisins sont dans reference_index
# - Les 6 restants sont des duplicates du dernier voisin valide
#   (comportement de queryandgroup avec padding)

attention_weights = softmax(attention_scores)  # (N, K, C/G)
# Les poids des voisins "padding" ne sont PAS mis Ã  zÃ©ro
# â†’ Ils contribuent avec des features dupliquÃ©es
```

**PTv2 : Masking explicite avec -1**

```python
# PTv2 utilise -1 pour marquer les voisins invalides
reference_index[point_42] = [15, 23, 8, -1, -1, -1, ...]
#                            â†‘ valides  â†‘ invalides (padding)

# CrÃ©ation du masque
mask = torch.sign(reference_index + 1)  # (N, K)
# sign(-1 + 1) = sign(0) = 0  â†’ voisin invalide
# sign(i + 1)  = sign(>0) = 1 â†’ voisin valide

# Application sur les poids
attention_weights = attention_weights * mask.unsqueeze(-1)
# Les poids des voisins invalides deviennent exactement 0
```

**Comparaison sur un exemple :**

```python
# Point isolÃ© avec 3 vrais voisins sur K=8

# PTv1 behavior
reference_index = [15, 23, 8, 8, 8, 8, 8, 8]  # duplicates du dernier
attention_weights = [0.25, 0.20, 0.15, 0.10, 0.10, 0.08, 0.07, 0.05]
# Les voisins "padding" (indices 3-7) ont des poids non-nuls
# â†’ AgrÃ©gation polluÃ©e par 5Ã— la mÃªme feature (voisin 8)

# PTv2 behavior
reference_index = [15, 23, 8, -1, -1, -1, -1, -1]  # explicit invalid
mask = [1, 1, 1, 0, 0, 0, 0, 0]
attention_weights = [0.25, 0.20, 0.15, 0, 0, 0, 0, 0]
# Les voisins invalides sont complÃ¨tement ignorÃ©s âœ“
```

**Impact :** Plus robuste pour les nuages avec densitÃ© variable ou points isolÃ©s.

---

### Innovation 4 : GroupedLinear vs MLP Standard

**PTv1 : MLP standard pour gÃ©nÃ©rer les poids d'attention**

```python
# PTv1
self.linear_w = nn.Sequential(
    nn.BatchNorm1d(mid_planes),
    nn.ReLU(inplace=True),
    nn.Linear(mid_planes, mid_planes // share_planes),  # C â†’ C/G
    nn.BatchNorm1d(mid_planes // share_planes),
    nn.ReLU(inplace=True),
    nn.Linear(out_planes // share_planes, out_planes // share_planes)  # C/G â†’ C/G
)

# ParamÃ¨tres totaux pour C=64, G=8:
# PremiÃ¨re Linear: 64 Ã— 8 = 512 paramÃ¨tres
# DeuxiÃ¨me Linear: 8 Ã— 8 = 64 paramÃ¨tres
# Total: ~576 paramÃ¨tres
```

**PTv2 : GroupedLinear**

```python
# PTv2
self.weight_encoding = nn.Sequential(
    GroupedLinear(embed_channels, groups, groups),  # C â†’ G
    PointBatchNorm(groups),
    nn.ReLU(inplace=True),
    nn.Linear(groups, groups)  # G â†’ G
)

# ParamÃ¨tres totaux pour C=64, G=8:
# GroupedLinear: 64 paramÃ¨tres (vecteur partagÃ©)
# Linear: 8 Ã— 8 = 64 paramÃ¨tres
# Total: ~128 paramÃ¨tres
```

**RÃ©duction : 576 â†’ 128 paramÃ¨tres (4.5Ã— moins !)**

---

### Innovation 5 : Architecture de Normalisation

**PTv1 :** Normalisation minimale

```python
# PTv1 - Pas de normalisation sur les projections Q, K, V
query = Linear(x)  # Pas normalisÃ©
key = Linear(x)
value = Linear(x)

# Normalisation seulement dans le MLP des poids
attention_scores = MLP_with_BatchNorm(relation_qk)
```

**PTv2 :** Normalisation extensive

```python
# PTv2 - Normalisation partout
query = Linear(x) â†’ BatchNorm â†’ ReLU  # NormalisÃ©
key = Linear(x) â†’ BatchNorm â†’ ReLU
value = Linear(x)  # Pas d'activation (reste linÃ©aire)

# Position encoding aussi normalisÃ©
pe_bias = Linear(pos) â†’ BatchNorm â†’ ReLU â†’ Linear

# Weight encoding aussi normalisÃ©
attention_scores = GroupedLinear â†’ BatchNorm â†’ ReLU â†’ Linear
```

**Impact :** Training plus stable, convergence plus rapide, moins sensible aux hyperparamÃ¨tres.

---

### Flux Complet Comparatif

**PTv1 :**
```
1. Q, K, V = Linear(feat)
2. Grouper K voisins (K-NN Ã  chaque couche)
3. PE = MLP(relative_pos)
4. relation = (K - Q) + PE
5. weights = MLP_standard(relation) â†’ softmax
6. output = sum(weights Ã— V)
```

**PTv2 :**
```
1. Q, K = Linear(feat) â†’ BatchNorm â†’ ReLU
   V = Linear(feat)
2. Grouper K voisins (rÃ©fÃ©rence prÃ©-calculÃ©e)
3. PE = MLP_normalized(relative_pos)
4. relation = (K - Q) + PE
5. V = V + PE  â† NOUVEAU
6. weights = GroupedLinear(relation) â†’ softmax
7. mask = sign(ref_index + 1)  â† NOUVEAU
8. weights = weights Ã— mask
9. output = sum(weights Ã— V)
```

---

### Tableau RÃ©capitulatif

| Innovation | PTv1 | PTv2 | Gain |
|------------|------|------|------|
| **Normalisation Q/K/V** | âŒ Non | âœ… Oui | StabilitÃ© training |
| **PE sur values** | âŒ Non | âœ… Oui | Features gÃ©omÃ©triques |
| **Masking invalides** | âŒ Non | âœ… Oui | Robustesse densitÃ© variable |
| **ParamÃ¨tres weight gen** | ~576 | ~128 | 4.5Ã— rÃ©duction |
| **K-NN par couche** | Oui | Non (prÃ©-calc) | ~6Ã— speedup |

PTv2 amÃ©liore donc **significativement** l'attention locale tout en rÃ©duisant les paramÃ¨tres et le coÃ»t computationnel ! ğŸ¯

# Block et BlockSequence : Architecture RÃ©siduelle

## Block : Residual Block avec DropPath

Le `Block` de PTv2 encapsule `GroupedVectorAttention` dans une structure rÃ©siduelle similaire Ã  ResNet, avec une innovation clÃ© : **DropPath**.

{% include figure.liquid path="assets/img/poinTransformerV2/block.svg" class="img-fluid rounded z-depth-1" %}

### Comparaison avec PTv1

| Aspect | PTv1 (PointTransformerBlock) | PTv2 (Block) |
|--------|------------------------------|--------------|
| **Structure** | Linear â†’ Attention â†’ Linear + Skip | Linear â†’ Attention â†’ Linear + Skip |
| **RÃ©gularisation** | Dropout uniquement | **DropPath** + Dropout |
| **Normalisation** | 3Ã— BatchNorm | 3Ã— BatchNorm (identique) |
| **Skip connection** | Simple addition | Addition avec **DropPath** |

### Architecture DÃ©taillÃ©e

```
Input features (N, C)
    â†“
[Linear + BatchNorm1d + ReLU]  â† Pre-activation (expansion)
    â†“
[GroupedVectorAttention]  â† Attention locale sur K voisins
    â†“
[BatchNorm1d + ReLU]  â† Post-attention normalization
    â†“
[Linear + BatchNorm1d]  â† Projection
    â†“
[DropPath]  â† RÃ©gularisation stochastique (NOUVEAU)
    â†“
[+ Skip Connection]  â† Connexion rÃ©siduelle
    â†“
[ReLU]  â† Activation finale
    â†“
Output features (N, C)
```

### DropPath : Stochastic Depth

**DropPath** (Stochastic Depth) est une technique de rÃ©gularisation qui **dropout des chemins entiers** dans un rÃ©seau rÃ©siduel, plutÃ´t que des neurones individuels.

**Dropout classique vs DropPath :**

```python
# Dropout classique (agit sur les features)
def dropout(x, p=0.5):
    mask = random(x.shape) > p  # Masque alÃ©atoire par Ã©lÃ©ment
    return x * mask / (1 - p)

output = x + dropout(f(x))
# Certaines features de f(x) sont mises Ã  0


# DropPath (agit sur le chemin entier)
def drop_path(x, p=0.1):
    if training and random() < p:
        return 0  # Tout le chemin est ignorÃ© !
    return x

output = x + drop_path(f(x))
# Soit tout f(x) est gardÃ©, soit tout est ignorÃ©
```

**Fonctionnement en pratique :**

Durant l'entraÃ®nement, avec probabilitÃ© `drop_path_rate` (typiquement 0.1), on saute complÃ¨tement le bloc transformÃ© :

```python
# Sans DropPath (PTv1)
feat_transformed = Linear â†’ Attention â†’ Linear
output = identity + feat_transformed  # Toujours calculÃ©

# Avec DropPath (PTv2)
feat_transformed = Linear â†’ Attention â†’ Linear

if training and random() < drop_path_rate:
    output = identity  # On saute feat_transformed complÃ¨tement !
else:
    output = identity + feat_transformed

# Ã€ l'infÃ©rence
output = identity + feat_transformed  # Toujours actif
```

**Visualisation sur un rÃ©seau de 12 blocs :**

```
Avec drop_path_rate = 0.1 (10% de chance de drop par bloc)

Training iteration 1:
Input â†’ [Block1] â†’ [Block2] â†’ [SKIP] â†’ [Block4] â†’ ... â†’ [SKIP] â†’ [Block12]
        âœ“          âœ“          âœ—          âœ“              âœ—          âœ“
        (rÃ©seau de ~10 blocs actifs)

Training iteration 2:
Input â†’ [Block1] â†’ [SKIP] â†’ [Block3] â†’ [Block4] â†’ ... â†’ [Block11] â†’ [Block12]
        âœ“          âœ—        âœ“          âœ“                  âœ“          âœ“
        (rÃ©seau de ~11 blocs actifs)

Inference:
Input â†’ [Block1] â†’ [Block2] â†’ [Block3] â†’ [Block4] â†’ ... â†’ [Block11] â†’ [Block12]
        âœ“          âœ“          âœ“          âœ“                  âœ“          âœ“
        (tous les 12 blocs actifs)
```

**Pourquoi Ã§a marche ?**

1. **RÃ©gularisation :** Force chaque bloc Ã  Ãªtre utile indÃ©pendamment
2. **Gradient flow :** CrÃ©e des "chemins courts" pendant l'entraÃ®nement
3. **Ensemble implicite :** EntraÃ®ne effectivement plusieurs sous-rÃ©seaux de profondeurs diffÃ©rentes
4. **RÃ©duit l'overfitting :** Les blocs ne peuvent pas trop dÃ©pendre les uns des autres

**Drop Path Rate Scheduler :**

Dans PTv2, le `drop_path_rate` augmente progressivement Ã  travers les couches :

```python
# Configuration PTv2 avec drop_path_rate = 0.3
enc_depths = [2, 2, 6, 2]  # 12 couches au total

drop_path_rates = linspace(0, 0.3, sum(enc_depths))
# [0.00, 0.03, 0.05, 0.08, 0.11, 0.14, 0.16, 0.19, 0.22, 0.24, 0.27, 0.30]

# Les premiÃ¨res couches ont drop_path_rate faible (plus stables)
# Les derniÃ¨res couches ont drop_path_rate Ã©levÃ© (plus rÃ©gularisÃ©es)
```

**Intuition :** Les couches profondes bÃ©nÃ©ficient plus de la rÃ©gularisation car elles ont tendance Ã  overfitter.

---

## BlockSequence : RÃ©utilisation du K-NN

`BlockSequence` empile plusieurs `Block` et introduit une optimisation majeure : **partage du reference_index**.

{% include figure.liquid path="assets/img/poinTransformerV2/blockSequence.svg" class="img-fluid rounded z-depth-1" %}

### Innovation ClÃ© : K-NN CalculÃ© Une Seule Fois

**ProblÃ¨me PTv1 :**

Dans PTv1, chaque `PointTransformerLayer` recalcule les K plus proches voisins via K-NN :

```python
# PTv1 - Dans PointTransformerLayer.forward()
def forward(self, pxo):
    p, x, o = pxo
    
    # K-NN calculÃ© Ã€ CHAQUE COUCHE
    x_k = pointops.queryandgroup(self.nsample, p, p, x_k, None, o, o, use_xyz=True)
    x_v = pointops.queryandgroup(self.nsample, p, p, x_v, None, o, o, use_xyz=False)
    # ...
```

Pour un bloc avec 6 couches `PointTransformerLayer`, on fait **6 fois** la mÃªme recherche K-NN !

```
Bloc avec 6 couches PTv1:
Layer 1: K-NN (N points, find K=16 neighbors) â†’ O(N log N)
Layer 2: K-NN (N points, find K=16 neighbors) â†’ O(N log N)
Layer 3: K-NN (N points, find K=16 neighbors) â†’ O(N log N)
Layer 4: K-NN (N points, find K=16 neighbors) â†’ O(N log N)
Layer 5: K-NN (N points, find K=16 neighbors) â†’ O(N log N)
Layer 6: K-NN (N points, find K=16 neighbors) â†’ O(N log N)

CoÃ»t total: 6 Ã— O(N log N)
```

**Solution PTv2 :**

Dans PTv2, `BlockSequence` calcule le K-NN **une seule fois** au dÃ©but et tous les `Block` partagent le mÃªme `reference_index` :

```python
# PTv2 - Dans BlockSequence.forward()
def forward(self, points):
    coord, feat, offset = points
    
    # K-NN calculÃ© UNE SEULE FOIS au dÃ©but
    reference_index, _ = pointops.knn_query(self.neighbours, coord, offset)
    # reference_index: (N, K) - indices des K voisins pour chaque point
    
    # Tous les blocks partagent le mÃªme reference_index
    for block in self.blocks:
        points = block(points, reference_index)  # Pas de recalcul !
    
    return points
```

```
Bloc avec 6 couches PTv2:
K-NN (une fois): O(N log N)
Layer 1: Utilise reference_index â†’ O(1) lookup
Layer 2: Utilise reference_index â†’ O(1) lookup
Layer 3: Utilise reference_index â†’ O(1) lookup
Layer 4: Utilise reference_index â†’ O(1) lookup
Layer 5: Utilise reference_index â†’ O(1) lookup
Layer 6: Utilise reference_index â†’ O(1) lookup

CoÃ»t total: O(N log N)  â† 6Ã— plus rapide !
```

### Pourquoi c'est Valide ?

**Question :** Peut-on vraiment rÃ©utiliser les mÃªmes voisins Ã  travers toutes les couches ?

**RÃ©ponse :** **OUI**, car dans `BlockSequence`, les **positions ne changent pas** !

```python
# Dans Block.forward()
def forward(self, points, reference_index):
    coord, feat, offset = points
    
    # coord (positions) reste INCHANGÃ‰ Ã  travers le bloc
    feat = self.fc1(feat)  # Seulement les features changent
    feat = self.attn(feat, coord, reference_index)  # coord fixe
    feat = self.fc3(feat)
    # ...
    
    return [coord, feat, offset]  # coord identique en sortie
```

Les positions 3D (`coord`) sont **constantes** dans un `BlockSequence` - seules les **features** Ã©voluent. Les K plus proches voisins restent donc identiques gÃ©omÃ©triquement !

**Cas oÃ¹ on DOIT recalculer le K-NN :**

Les positions changent uniquement lors des transitions entre niveaux de l'architecture (downsampling/upsampling) :

```python
# Encoder
points = BlockSequence(points)  # Positions fixes, K-NN partagÃ© âœ“
points = GridPool(points)        # Positions changent (downsampling) âœ—
points = BlockSequence(points)  # Nouvelles positions â†’ nouveau K-NN âœ“

# Decoder
points = UnpoolWithSkip(points, skip)  # Positions changent (upsampling) âœ—
points = BlockSequence(points)         # Nouvelles positions â†’ nouveau K-NN âœ“
```

### Comparaison des CoÃ»ts

**Pour un Encoder avec 4 niveaux Ã— 6 couches chacun (24 couches totales) :**

| OpÃ©ration | PTv1 | PTv2 | Speedup |
|-----------|------|------|---------|
| **K-NN queries** | 24 fois | 4 fois | 6Ã— |
| **K-NN cost** | 24 Ã— O(N log N) | 4 Ã— O(N log N) | 6Ã— |
| **Memory** | RecalculÃ© chaque fois | StockÃ© et rÃ©utilisÃ© | - |

**Note :** Le speedup rÃ©el dÃ©pend du ratio (coÃ»t K-NN / coÃ»t attention), mais empiriquement PTv2 est ~2-3Ã— plus rapide en pratique sur cette optimisation seule.

### Gestion du reference_index

**Structure du reference_index :**

```python
# reference_index: (N, K)
# Pour chaque point n âˆˆ [0, N), contient les indices de ses K voisins

# Exemple avec N=5 points, K=3 voisins
reference_index = [
    [1, 2, 4],    # Point 0: ses 3 voisins sont les points 1, 2, 4
    [0, 2, 3],    # Point 1: ses 3 voisins sont les points 0, 2, 3
    [0, 1, 3],    # Point 2: ...
    [1, 2, 4],    # Point 3: ...
    [0, 2, 3]     # Point 4: ...
]
```

**Gestion des voisins invalides (padding) :**

```python
# Point isolÃ© avec seulement 2 vrais voisins (K=3)
reference_index[point_42] = [15, 23, -1]
#                                    â†‘ Pas assez de voisins â†’ -1 (invalide)

# Le masking dans GroupedVectorAttention gÃ¨re automatiquement
mask = sign(reference_index + 1)  # [1, 1, 0]
attention_weights = attention_weights * mask.unsqueeze(-1)
# Le voisin invalide (-1) est ignorÃ© âœ“
```

### Exemple Complet

**Configuration :**
- BlockSequence avec `depth=6` (6 blocks)
- `neighbours=16` (K=16 voisins)
- N=1000 points, C=64 channels

**Flux :**

```python
# Input
coord: (1000, 3)
feat: (1000, 64)
offset: (B,)

# Ã‰tape 1: K-NN une fois
reference_index = knn_query(K=16, coord, offset)
# reference_index: (1000, 16)
# Pour chaque point: indices de ses 16 voisins

# Ã‰tape 2: Block 1
coord, feat, offset = Block_1(coord, feat, offset, reference_index)
# coord inchangÃ©: (1000, 3)
# feat transformÃ©: (1000, 64)

# Ã‰tape 3: Block 2 (rÃ©utilise reference_index)
coord, feat, offset = Block_2(coord, feat, offset, reference_index)
# coord toujours inchangÃ©: (1000, 3)
# feat transformÃ©: (1000, 64)

# Ã‰tapes 4-6: Blocks 3-6 (tous rÃ©utilisent reference_index)
...

# Output
coord: (1000, 3)  â† Identique Ã  l'input
feat: (1000, 64)  â† TransformÃ© par 6 couches d'attention
offset: (B,)      â† Identique Ã  l'input
```

**Visualisation :**

```
Point central â—‰ Ã  la position (x, y, z)

Au dÃ©but de BlockSequence:
  K-NN â†’ Trouve ses 16 voisins: â—â‚, â—â‚‚, ..., â—â‚â‚†
  Stocke dans reference_index[â—‰] = [idxâ‚, idxâ‚‚, ..., idxâ‚â‚†]

Block 1:
  Attention sur â—â‚, â—â‚‚, ..., â—â‚â‚† (lookup via reference_index)
  â†’ Features de â—‰ mises Ã  jour
  â†’ Position de â—‰ INCHANGÃ‰E

Block 2:
  Attention sur les MÃŠMES â—â‚, â—â‚‚, ..., â—â‚â‚† (lookup via reference_index)
  â†’ Features de â—‰ encore mises Ã  jour
  â†’ Position de â—‰ toujours INCHANGÃ‰E

...

Block 6:
  Attention sur les MÃŠMES voisins
  â†’ Features finales de â—‰
```

Les voisins gÃ©omÃ©triques restent identiques, mais leurs **features Ã©voluent** Ã  chaque couche !

---

## GVAPatchEmbed : Embedding Initial

Avant de downsampler, PTv2 applique un `GVAPatchEmbed` qui enrichit les features Ã  pleine rÃ©solution.

{% include figure.liquid path="assets/img/poinTransformerV2/GVAPatchEmbed.svg" class="img-fluid rounded z-depth-1" %}

### RÃ´le

**GVAPatchEmbed** = Projection linÃ©aire + BlockSequence (sans downsampling)

```python
Input: (N, in_channels)
    â†“
Linear + BatchNorm1d + ReLU
    â†“
(N, embed_channels)
    â†“
BlockSequence (depth blocks)
    â†“
Output: (N, embed_channels)
```

### Comparaison avec PTv1

| Aspect | PTv1 | PTv2 |
|--------|------|------|
| **Initial embedding** | âŒ Aucun | âœ… GVAPatchEmbed |
| **PremiÃ¨re opÃ©ration** | TransitionDown (downsampling immÃ©diat) | GVAPatchEmbed (attention Ã  pleine rÃ©solution) |
| **Philosophy** | Downsample vite | Apprendre des features riches d'abord |

**PTv1 :**
```
Input (N, in_channels)
    â†“
TransitionDown (stride=1)  â† Simple Linear + BN + ReLU
    â†“
PointTransformerBlock
    â†“
TransitionDown (stride=4)  â† Downsampling immÃ©diat
```

**PTv2 :**
```
Input (N, in_channels)
    â†“
GVAPatchEmbed:
  - Linear + BN + ReLU
  - depth Ã— Block (GroupedVectorAttention)
    â†“
(N, embed_channels)  â† Features riches avant downsampling
    â†“
Encoder 1 (GridPool)  â† Premier downsampling
```

### Pourquoi c'est Important ?

**Analogie avec les CNNs :**

Dans les CNNs modernes (ResNet, EfficientNet), on a un **"stem"** initial qui traite l'image Ã  haute rÃ©solution avant le pooling :

```python
# ResNet stem
Input (224Ã—224)
    â†“
Conv 7Ã—7, stride=2  â†’ (112Ã—112)
    â†“
MaxPool 3Ã—3, stride=2 â†’ (56Ã—56)
    â†“
ResNet blocks...
```

**PTv2 adopte la mÃªme philosophie :**
- Apprendre des features riches Ã  **pleine rÃ©solution** avant de downsampler
- Permet de capturer des dÃ©tails fins dÃ¨s le dÃ©but
- Les features initiales de meilleure qualitÃ© aident tout le rÃ©seau

### Configuration Typique

```python
# PTv2 default config
GVAPatchEmbed(
    in_channels=6,          # xyz + rgb
    embed_channels=48,      # Dimension d'embedding
    groups=6,               # 6 groupes pour l'attention
    depth=1,                # 1 block (peut Ãªtre plus)
    neighbours=8            # K=8 voisins
)
```

Avec `depth=1`, c'est modeste, mais dÃ©jÃ  bÃ©nÃ©fique. Certaines variantes utilisent `depth=2` pour des features encore plus riches.

# GridPool : Downsampling par Voxelisation

## Vue d'ensemble

`GridPool` est l'une des innovations majeures de PTv2, remplaÃ§ant le **Furthest Point Sampling (FPS)** de PTv1 par une approche basÃ©e sur la **voxelisation**.

{% include figure.liquid path="assets/img/poinTransformerV2/gridPool.svg" class="img-fluid rounded z-depth-1" %}

### Comparaison : FPS vs Grid Pooling

| Aspect | PTv1 (FPS) | PTv2 (Grid Pooling) |
|--------|------------|---------------------|
| **ComplexitÃ©** | O(NÂ²) | **O(N)** |
| **MÃ©thode** | SÃ©lection itÃ©rative des points les plus Ã©loignÃ©s | Voxelisation + agrÃ©gation |
| **DÃ©terminisme** | Non (ordre dÃ©pend du point de dÃ©part) | Oui (basÃ© sur la grille) |
| **Speedup** | - | **3-5Ã— plus rapide** |
| **Couverture spatiale** | Maximise les distances | Uniforme par design |
| **Unpooling** | K-NN interpolation (coÃ»teux) | **Map unpooling** (gratuit) |

---

## ProblÃ¨me avec FPS (PTv1)

### Algorithme FPS

**Furthest Point Sampling** sÃ©lectionne itÃ©rativement les points les plus Ã©loignÃ©s :

```python
def furthest_point_sampling(points, num_samples):
    # 1. Choisir un point de dÃ©part alÃ©atoire
    selected = [random_point]
    
    # 2. RÃ©pÃ©ter jusqu'Ã  avoir num_samples points
    for i in range(num_samples - 1):
        # Pour chaque point non sÃ©lectionnÃ©:
        #   - Calculer sa distance au point sÃ©lectionnÃ© le plus proche
        distances = [min_distance_to_selected(p) for p in points]
        
        # SÃ©lectionner le point le plus Ã©loignÃ©
        farthest = argmax(distances)
        selected.append(farthest)
    
    return selected
```

**Exemple visuel :**

```
Nuage de 16 points, on veut en garder 4:

Ã‰tape 1: Point alÃ©atoire
    â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—
    â”‚    â”‚    â”‚    â”‚
    â—‰â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—   â† Point de dÃ©part
    â”‚    â”‚    â”‚    â”‚
    â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—
    â”‚    â”‚    â”‚    â”‚
    â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—

Ã‰tape 2: Point le plus Ã©loignÃ©
    â—‰â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—
    â”‚    â”‚    â”‚    â”‚
    â—‰â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—
    â”‚    â”‚    â”‚    â”‚
    â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—
    â”‚    â”‚    â”‚    â”‚
    â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—‰   â† Coin opposÃ© (le plus loin)

Ã‰tape 3: Encore le plus Ã©loignÃ©
    â—‰â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—‰
    â”‚    â”‚    â”‚    â”‚
    â—‰â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—
    â”‚    â”‚    â”‚    â”‚
    â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—
    â”‚    â”‚    â”‚    â”‚
    â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—‰

Ã‰tape 4: Dernier point
    â—‰â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—‰
    â”‚    â”‚    â”‚    â”‚
    â—‰â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—
    â”‚    â”‚    â”‚    â”‚
    â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—
    â”‚    â”‚    â”‚    â”‚
    â—‰â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—‰   â† 4 points bien espacÃ©s
```

**ProblÃ¨mes :**

1. **ComplexitÃ© O(NÂ²)** : Pour sÃ©lectionner M points parmi N, on doit calculer O(M Ã— N) distances
2. **CoÃ»t Ã©levÃ©** : Pour N=100k points â†’ M=25k points, c'est 2.5 milliards de calculs de distance !
3. **Non-dÃ©terministe** : DÃ©pend du point de dÃ©part alÃ©atoire
4. **Pas de mapping** : On perd l'information de correspondance pour l'unpooling

```python
# CoÃ»t FPS pour downsampler 4 niveaux dans PTv1
Level 1: FPS(N â†’ N/4)     â†’ O(NÂ²)
Level 2: FPS(N/4 â†’ N/16)  â†’ O((N/4)Â²)
Level 3: FPS(N/16 â†’ N/64) â†’ O((N/16)Â²)
Level 4: FPS(N/64 â†’ N/256)â†’ O((N/64)Â²)

Total: O(NÂ²) dominant pour les premiers niveaux
```

---

## Solution : Grid Pooling (PTv2)

### Principe : Voxelisation

Au lieu de sÃ©lectionner des points, on **partitionne l'espace en voxels** (cubes 3D) et on agrÃ¨ge tous les points d'un mÃªme voxel.

```
Nuage de points + grille 3D:

        grid_size = 0.5m
        
    â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
    â”‚ â—   â”‚     â”‚  â—  â”‚     â”‚
    â”‚   â— â”‚     â”‚     â”‚  â—  â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
    â”‚     â”‚ â—â—  â”‚     â”‚     â”‚
    â”‚ â—   â”‚  â—  â”‚  â—  â”‚     â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
    â”‚  â—  â”‚     â”‚ â—   â”‚  â—  â”‚
    â”‚     â”‚  â—  â”‚   â— â”‚     â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
    â”‚     â”‚     â”‚     â”‚ â—â—  â”‚
    â”‚  â—  â”‚  â—  â”‚  â—  â”‚  â—  â”‚
    â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

AprÃ¨s voxelisation (1 point par voxel):

    â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
    â”‚  â—‰  â”‚     â”‚  â—‰  â”‚  â—‰  â”‚
    â”‚     â”‚     â”‚     â”‚     â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
    â”‚  â—‰  â”‚  â—‰  â”‚  â—‰  â”‚     â”‚
    â”‚     â”‚     â”‚     â”‚     â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
    â”‚  â—‰  â”‚  â—‰  â”‚  â—‰  â”‚  â—‰  â”‚
    â”‚     â”‚     â”‚     â”‚     â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
    â”‚  â—‰  â”‚  â—‰  â”‚  â—‰  â”‚  â—‰  â”‚
    â”‚     â”‚     â”‚     â”‚     â”‚
    â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```

### Algorithme Ã‰tape par Ã‰tape

#### Ã‰tape 1 : Projection des Features

```python
# Input
coord: (N, 3)
feat: (N, in_channels)

# Projection linÃ©aire + normalisation
feat = Linear(feat)  # (N, in_channels) â†’ (N, out_channels)
feat = BatchNorm1d(feat) + ReLU(feat)
# feat: (N, out_channels)
```

#### Ã‰tape 2 : Calcul du Point de DÃ©part par Nuage

Chaque nuage dans le batch a besoin d'un point de rÃ©fÃ©rence (coin minimal) :

```python
# Conversion offset â†’ batch
batch = offset2batch(offset)  # (N,)
# batch[i] indique Ã  quel nuage appartient le point i

# Calcul du coin minimal de chaque nuage
start = segment_csr(coord, batch_ptr, reduce="min")
# start: (B, 3) - coin minimal (x_min, y_min, z_min) de chaque nuage
```

**Exemple :**

```python
# 2 nuages dans le batch
Nuage 1: points aux positions [[1,2,3], [2,3,4], [1.5,2.5,3.5]]
    â†’ start[0] = [1, 2, 3]  # Minimum de chaque dimension

Nuage 2: points aux positions [[5,6,7], [6,7,8]]
    â†’ start[1] = [5, 6, 7]
```

#### Ã‰tape 3 : Voxelisation

```python
# Normalisation des coordonnÃ©es par rapport au dÃ©but de chaque nuage
coord_normalized = coord - start[batch]  # (N, 3)

# Assignation Ã  une grille avec voxels de taille grid_size
cluster = voxel_grid(
    pos=coord_normalized, 
    size=grid_size,  # ex: 0.06m
    batch=batch,
    start=0
)
# cluster: (N,) - ID du voxel pour chaque point
```

**Exemple avec grid_size=1.0 :**

```python
# Points d'un nuage (aprÃ¨s normalisation)
points = [
    [0.2, 0.3, 0.1],  # Voxel (0, 0, 0)
    [0.8, 0.5, 0.2],  # Voxel (0, 0, 0)
    [1.2, 0.4, 0.3],  # Voxel (1, 0, 0)
    [1.5, 1.8, 0.1],  # Voxel (1, 1, 0)
    [0.1, 1.1, 0.9],  # Voxel (0, 1, 0)
]

# Calcul du voxel ID
voxel_id = floor(coord / grid_size)

cluster = [
    0,  # (0,0,0) â†’ ID unique du voxel
    0,  # (0,0,0) â†’ mÃªme voxel
    1,  # (1,0,0)
    2,  # (1,1,0)
    3,  # (0,1,0)
]
```

#### Ã‰tape 4 : Identification des Voxels Uniques

```python
unique, cluster_inverse, counts = torch.unique(
    cluster, 
    sorted=True, 
    return_inverse=True, 
    return_counts=True
)
```

**Que retourne torch.unique ?**

```python
# Input cluster (exemple)
cluster = [0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3]
#          â†‘â”€â”€â”€â”€â”€â†‘  â†‘â”€â”€â†‘  â†‘â”€â”€â”€â”€â”€â”€â”€â”€â†‘  â†‘â”€â”€â”€â”€â”€â”€â†‘
#          3 pts   2 pts  4 points   3 points

unique = [0, 1, 2, 3]  # Les voxels uniques
# Nvoxel = 4

cluster_inverse = [0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3]
# Mapping: point i appartient au voxel unique[cluster_inverse[i]]

counts = [3, 2, 4, 3]  # Nombre de points par voxel
```

#### Ã‰tape 5 : Tri et Index Pointers

```python
# Tri les points par voxel
_, sorted_indices = torch.sort(cluster_inverse)
# sorted_indices: ordre pour regrouper les points du mÃªme voxel ensemble

# CrÃ©ation des pointeurs pour chaque voxel
idx_ptr = torch.cat([
    torch.zeros(1), 
    torch.cumsum(counts, dim=0)
])
# idx_ptr: (Nvoxel + 1,)
```

**Exemple :**

```python
# AprÃ¨s tri
sorted_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
# Points triÃ©s par voxel

# Index pointers
counts = [3, 2, 4, 3]
idx_ptr = [0, 3, 5, 9, 12]
#          â†‘  â†‘  â†‘  â†‘  â†‘
#          â”‚  â”‚  â”‚  â”‚  â””â”€ Fin (12 points)
#          â”‚  â”‚  â”‚  â””â”€â”€â”€â”€ Voxel 3 commence Ã  l'indice 9
#          â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€ Voxel 2 commence Ã  l'indice 5
#          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Voxel 1 commence Ã  l'indice 3
#          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Voxel 0 commence Ã  l'indice 0
```

#### Ã‰tape 6 : AgrÃ©gation des CoordonnÃ©es (Moyenne)

```python
coord_pooled = segment_csr(
    coord[sorted_indices],  # CoordonnÃ©es triÃ©es par voxel
    idx_ptr, 
    reduce="mean"
)
# coord_pooled: (Nvoxel, 3)
# Position moyenne de tous les points dans chaque voxel
```

**Exemple :**

```python
# Voxel 0 contient 3 points aux positions:
points_voxel_0 = [[0.2, 0.3, 0.1], [0.8, 0.5, 0.2], [0.1, 0.2, 0.15]]
coord_pooled[0] = mean(points_voxel_0) = [0.37, 0.33, 0.15]

# Voxel 1 contient 2 points:
points_voxel_1 = [[1.2, 0.4, 0.3], [1.4, 0.6, 0.4]]
coord_pooled[1] = mean(points_voxel_1) = [1.3, 0.5, 0.35]
```

#### Ã‰tape 7 : AgrÃ©gation des Features (Max)

```python
feat_pooled = segment_csr(
    feat[sorted_indices],  # Features triÃ©es par voxel
    idx_ptr,
    reduce="max"
)
# feat_pooled: (Nvoxel, out_channels)
# Maximum des features dans chaque voxel
```

**Pourquoi Max au lieu de Mean ?**

```python
# Exemple avec 3 points dans un voxel

# Mean pooling
feat_mean = (feat1 + feat2 + feat3) / 3
# Peut "diluer" les features importantes

# Max pooling (utilisÃ© par PTv2)
feat_max = max(feat1, feat2, feat3)
# PrÃ©serve les features dominantes de chaque canal
# Plus robuste au bruit et aux outliers
```

**Exemple concret :**

```python
# Voxel contient 3 points avec features:
feat1 = [0.8, 0.2, 0.5, 0.1]
feat2 = [0.3, 0.9, 0.4, 0.6]
feat3 = [0.5, 0.4, 0.7, 0.3]

# Max pooling (canal par canal)
feat_pooled = [
    max(0.8, 0.3, 0.5) = 0.8,  # Canal 0
    max(0.2, 0.9, 0.4) = 0.9,  # Canal 1
    max(0.5, 0.4, 0.7) = 0.7,  # Canal 2
    max(0.1, 0.6, 0.3) = 0.6   # Canal 3
]
= [0.8, 0.9, 0.7, 0.6]
```

#### Ã‰tape 8 : Reconstruction des Offsets

```python
# RÃ©cupÃ©ration du batch ID pour chaque voxel
# (prend le batch du premier point de chaque voxel)
batch_pooled = batch[idx_ptr[:-1]]
# batch_pooled: (Nvoxel,)

# Conversion batch â†’ offset
offset_pooled = batch2offset(batch_pooled)
# offset_pooled: (B,)
```

#### Ã‰tape 9 : Retour du Cluster Mapping

```python
return [coord_pooled, feat_pooled, offset_pooled], cluster_inverse
```

Le `cluster_inverse` est **crucial** car il permet le **Map Unpooling** plus tard :

```python
# cluster_inverse: (N,) - pour chaque point, son voxel d'appartenance
cluster_inverse[point_i] = voxel_id

# Exemple
cluster_inverse = [0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3]
#                  â†‘â”€â”€â”€â”€â”€â†‘  â†‘â”€â”€â†‘  â†‘â”€â”€â”€â”€â”€â”€â”€â”€â†‘  â†‘â”€â”€â”€â”€â”€â”€â†‘
#                  Points du voxel 0, 1, 2, 3
```

Ce mapping sera rÃ©utilisÃ© dans `UnpoolWithSkip` pour "dÃ©pooler" efficacement !

---

## Exemple Complet NumÃ©rique

**Configuration :**
- N = 12 points
- grid_size = 1.0
- in_channels = 3, out_channels = 4

**Input :**

```python
coord = [
    [0.2, 0.3, 0.1],  # Point 0
    [0.8, 0.5, 0.2],  # Point 1
    [0.1, 0.2, 0.15], # Point 2
    [1.2, 0.4, 0.3],  # Point 3
    [1.4, 0.6, 0.4],  # Point 4
    [1.5, 1.8, 0.1],  # Point 5
    [0.1, 1.1, 0.9],  # Point 6
    [2.1, 0.3, 0.2],  # Point 7
    [2.3, 0.5, 0.4],  # Point 8
    [0.9, 1.9, 0.3],  # Point 9
    [1.1, 1.8, 0.5],  # Point 10
    [2.2, 1.1, 0.6],  # Point 11
]  # (12, 3)

feat = [...] # (12, 3)
offset = [12]  # 1 nuage avec 12 points
```

**Ã‰tape 1 : Projection**

```python
feat = Linear(feat) â†’ BatchNorm â†’ ReLU
# feat: (12, 4)
```

**Ã‰tape 2 : Start point**

```python
start = [0.1, 0.2, 0.1]  # Minimum de chaque dimension
```

**Ã‰tape 3 : Voxelisation**

```python
# CoordonnÃ©es normalisÃ©es
coord_norm = coord - start

# Assignation aux voxels (floor(coord_norm / 1.0))
cluster = [
    0,  # (0,0,0)  Points 0, 1, 2
    0,  # (0,0,0)
    0,  # (0,0,0)
    1,  # (1,0,0)  Points 3, 4
    1,  # (1,0,0)
    2,  # (1,1,0)  Points 5, 9, 10
    3,  # (0,1,0)  Point 6
    4,  # (2,0,0)  Points 7, 8
    4,  # (2,0,0)
    2,  # (1,1,0)
    2,  # (1,1,0)
    5,  # (2,1,0)  Point 11
]
```

**Ã‰tape 4 : Unique**

```python
unique = [0, 1, 2, 3, 4, 5]  # 6 voxels
counts = [3, 2, 3, 1, 2, 1]  # Points par voxel
cluster_inverse = [0, 0, 0, 1, 1, 2, 3, 4, 4, 2, 2, 5]
```

**Ã‰tape 5 : Tri**

```python
sorted_indices = [0, 1, 2, 3, 4, 5, 9, 10, 6, 7, 8, 11]
idx_ptr = [0, 3, 5, 8, 9, 11, 12]
```

**Ã‰tape 6 : AgrÃ©gation CoordonnÃ©es**

```python
coord_pooled = [
    mean(coord[0,1,2]) = [0.37, 0.33, 0.15],   # Voxel 0
    mean(coord[3,4])   = [1.3, 0.5, 0.35],     # Voxel 1
    mean(coord[5,9,10])= [1.17, 1.83, 0.3],    # Voxel 2
    coord[6]           = [0.1, 1.1, 0.9],      # Voxel 3
    mean(coord[7,8])   = [2.2, 0.4, 0.3],      # Voxel 4
    coord[11]          = [2.2, 1.1, 0.6],      # Voxel 5
]  # (6, 3)
```

**Ã‰tape 7 : AgrÃ©gation Features**

```python
feat_pooled = [
    max(feat[0,1,2], dim=0),   # (4,)
    max(feat[3,4], dim=0),     # (4,)
    max(feat[5,9,10], dim=0),  # (4,)
    feat[6],                   # (4,)
    max(feat[7,8], dim=0),     # (4,)
    feat[11],                  # (4,)
]  # (6, 4)
```

**Output :**

```python
coord_pooled: (6, 3)     # 6 voxels au lieu de 12 points
feat_pooled: (4, 4)      # 6 voxels avec 4 channels
offset_pooled: [6]       # 1 nuage avec 6 points
cluster_inverse: (12,)   # Mapping points â†’ voxels
```

**RÃ©duction :** 12 points â†’ 6 voxels (2Ã— downsampling)

---

## ComplexitÃ© et Performance

### ComplexitÃ© Algorithmique

```python
# GridPool
1. Linear: O(N Ã— C)
2. Start computation: O(N)
3. Voxel assignment: O(N)
4. Unique: O(N log N)
5. Sort: O(N log N)
6. Aggregation: O(N)

Total: O(N log N)  â† DominÃ© par le tri
```

### Comparaison avec FPS

| OpÃ©ration | FPS (PTv1) | Grid Pooling (PTv2) |
|-----------|------------|---------------------|
| **ComplexitÃ©** | O(NÂ²) | O(N log N) |
| **N=10k** | 100M ops | ~133k ops |
| **N=100k** | 10B ops | ~1.7M ops |
| **Speedup empirique** | - | **3-5Ã—** |

### Visualisation du Speedup

```
Temps de downsampling (Nâ†’N/4):

FPS (PTv1):
N=10k:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 120ms
N=50k:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 3000ms
N=100k: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 12000ms

Grid Pooling (PTv2):
N=10k:  â–ˆâ–ˆâ–ˆ 30ms
N=50k:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 200ms
N=100k: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 350ms

Speedup: ~4Ã—  ~15Ã—  ~34Ã—
```

---

## Avantages de Grid Pooling

### 1. Vitesse

**3-5Ã— plus rapide** que FPS, surtout pour les grands nuages.

### 2. DÃ©terminisme

```python
# FPS: rÃ©sultat dÃ©pend du point de dÃ©part alÃ©atoire
run1 = FPS(points, seed=42)
run2 = FPS(points, seed=43)
# run1 â‰  run2  (points sÃ©lectionnÃ©s diffÃ©rents)

# Grid Pooling: toujours le mÃªme rÃ©sultat
run1 = GridPool(points, grid_size=0.06)
run2 = GridPool(points, grid_size=0.06)
# run1 == run2  (voxels identiques)
```

### 3. Couverture Spatiale Uniforme

```python
# FPS: peut "rater" des rÃ©gions
    â—â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
    â”‚              â”‚
    â”‚   â—â—â—â—       â”‚  â† Zone dense peu Ã©chantillonnÃ©e
    â”‚              â”‚
    â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—

# Grid Pooling: couverture garantie
    â—â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â—
    â”‚     â”‚     â”‚     â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
    â”‚  â—  â”‚ â—â—  â”‚     â”‚  â† Chaque voxel reprÃ©sentÃ©
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
    â—â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â—
```

### 4. Map Unpooling Gratuit

Le `cluster_inverse` permet un unpooling **sans calcul** :

```python
# PTv1: doit recalculer K-NN pour l'interpolation
upsampled = knn_interpolation(low_res, high_res)  # CoÃ»teux !

# PTv2: rÃ©utilise le cluster mapping
upsampled = feat_low_res[cluster_inverse]  # Lookup instantanÃ© !
```

# UnpoolWithSkip : Map Unpooling avec Skip Connections

## Vue d'ensemble

`UnpoolWithSkip` est le pendant de `GridPool` dans le dÃ©codeur, permettant de remonter en rÃ©solution tout en fusionnant l'information multi-Ã©chelle via les skip connections.

{% include figure.liquid path="assets/img/poinTransformerV2/unpoolWithSkip.svg" class="img-fluid rounded z-depth-1" %}

### Comparaison : Interpolation vs Map Unpooling

| Aspect | PTv1 (K-NN Interpolation) | PTv2 (Map Unpooling) |
|--------|---------------------------|----------------------|
| **MÃ©thode** | K-NN + weighted average | Lookup direct via cluster |
| **ComplexitÃ©** | O(M log N) K-NN search | **O(1) lookup** |
| **Information utilisÃ©e** | Distances gÃ©omÃ©triques | Mapping du downsampling |
| **CoÃ»t** | CoÃ»teux (recherche K-NN) | **Gratuit** (indexing) |
| **PrÃ©cision** | Interpolation lisse | Mapping exact |

---

## ProblÃ¨me avec K-NN Interpolation (PTv1)

### Algorithme d'Interpolation PTv1

Dans PTv1, pour passer de M points (basse rÃ©solution) Ã  N points (haute rÃ©solution), on utilise une **interpolation par K-NN** :

```python
def interpolation(coord_low, coord_high, feat_low, K=3):
    """
    Args:
        coord_low: (M, 3) - positions basse rÃ©solution
        coord_high: (N, 3) - positions haute rÃ©solution (cibles)
        feat_low: (M, C) - features basse rÃ©solution
    Returns:
        feat_high: (N, C) - features interpolÃ©es
    """
    N = coord_high.shape[0]
    
    for n in range(N):
        # 1. Trouver les K voisins les plus proches dans coord_low
        distances = ||coord_low - coord_high[n]||  # (M,)
        k_nearest = argsort(distances)[:K]  # K indices
        
        # 2. Poids inversement proportionnels aux distances
        dists = distances[k_nearest]  # (K,)
        weights = 1.0 / (dists + Îµ)
        weights = weights / sum(weights)  # Normalisation
        
        # 3. Moyenne pondÃ©rÃ©e
        feat_high[n] = Î£ weights[k] Ã— feat_low[k_nearest[k]]
    
    return feat_high
```

**Visualisation :**

```
Basse rÃ©solution (M=4 points):        Haute rÃ©solution (N=9 cibles):
    
    â—‰â‚         â—‰â‚‚                         â—â‚    â—â‚‚    â—â‚ƒ
                                          
                                          â—â‚„    â—â‚…    â—â‚†
                                          
    â—‰â‚ƒ         â—‰â‚„                         â—â‚‡    â—â‚ˆ    â—â‚‰

Pour interpoler â—â‚… (centre):
1. K-NN: trouver les 3 plus proches parmi {â—‰â‚, â—‰â‚‚, â—‰â‚ƒ, â—‰â‚„}
   â†’ â—‰â‚ (dist=1.4), â—‰â‚‚ (dist=1.4), â—‰â‚ƒ (dist=1.4), â—‰â‚„ (dist=1.4)
   â†’ Tous Ã©quidistants !

2. Poids: wâ‚=wâ‚‚=wâ‚ƒ=wâ‚„ = 0.25

3. Interpolation:
   feat[â—â‚…] = 0.25Ã—feat[â—‰â‚] + 0.25Ã—feat[â—‰â‚‚] + 0.25Ã—feat[â—‰â‚ƒ] + 0.25Ã—feat[â—‰â‚„]
```

### ProblÃ¨mes de l'Interpolation

**1. CoÃ»t computationnel :**

```python
# Pour chaque point haute rÃ©solution N:
#   - Calculer M distances
#   - Trier pour trouver les K plus proches
#   - Calculer la moyenne pondÃ©rÃ©e

ComplexitÃ©: O(N Ã— M log M)

# Exemple: M=25k, N=100k
OpÃ©rations: 100k Ã— 25k Ã— log(25k) â‰ˆ 35 milliards !
```

**2. Perte d'information :**

L'interpolation crÃ©e de **nouvelles** features qui n'existaient pas dans la rÃ©solution d'origine :

```python
# GridPool: Point original â†’ Voxel A
point_42: [x, y, z], feat_original

# AprÃ¨s downsampling
voxel_A: [x_mean, y_mean, z_mean], feat_pooled

# AprÃ¨s interpolation (PTv1)
point_42: [x, y, z], feat_interpolated  # â‰  feat_original !
# L'interpolation "invente" des features
```

**3. Non-dÃ©terminisme pour les cas ambigus :**

```python
# Point Ã©quidistant de plusieurs voisins
distances = [1.0, 1.0, 1.0, 1.0]  # 4 voisins Ã©quidistants

# Avec K=3, lesquels choisir ?
# â†’ DÃ©pend de l'ordre dans le tableau (non dÃ©terministe)
```

---

## Solution : Map Unpooling (PTv2)

### Principe : RÃ©utilisation du Cluster Mapping

L'idÃ©e gÃ©niale de PTv2 : **stocker le mapping lors du downsampling** et le **rÃ©utiliser lors de l'upsampling** !

**Rappel du GridPool :**

```python
# GridPool retourne le cluster_inverse
coord_pooled, feat_pooled, offset_pooled, cluster = GridPool(points)

# cluster: (N,) - pour chaque point original, son voxel d'appartenance
cluster = [0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3]
#          â””â”€â”€â”¬â”€â”€â”˜  â””â”€â”¬â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜
#          Voxel 0  Voxel 1  Voxel 2   Voxel 3
```

**Map Unpooling :**

```python
# Pour remonter en rÃ©solution, simple indexing !
feat_upsampled = feat_pooled[cluster]  # (N, C)

# Chaque point rÃ©cupÃ¨re les features de son voxel d'origine
```

**C'est tout !** Un simple lookup, complexitÃ© **O(1)** par point, donc **O(N)** total.

---

## Algorithme DÃ©taillÃ©

### Inputs

```python
# Points actuels (basse rÃ©solution)
coord_low: (M, 3)        # Positions des voxels
feat_low: (M, in_ch)     # Features des voxels
offset_low: (B,)

# Points skip (haute rÃ©solution - de l'encodeur)
coord_skip: (N, 3)       # Positions originales
feat_skip: (N, skip_ch)  # Features originales
offset_skip: (B,)

# Cluster mapping (du GridPool correspondant)
cluster: (N,)            # Pour chaque point, son voxel
```

### Ã‰tape 1 : Projection des Features Basse RÃ©solution

```python
feat_low_proj = Linear(feat_low) â†’ BatchNorm1d â†’ ReLU
# feat_low_proj: (M, out_ch)
```

### Ã‰tape 2 : Map Unpooling

```python
# Lookup direct via cluster
feat_mapped = feat_low_proj[cluster]
# feat_mapped: (N, out_ch)
```

**Explication :**

```python
# Exemple avec M=4 voxels, N=12 points
feat_low_proj = [
    [fâ‚€â°, fâ‚€Â¹, fâ‚€Â², fâ‚€Â³],  # Features du voxel 0
    [fâ‚â°, fâ‚Â¹, fâ‚Â², fâ‚Â³],  # Features du voxel 1
    [fâ‚‚â°, fâ‚‚Â¹, fâ‚‚Â², fâ‚‚Â³],  # Features du voxel 2
    [fâ‚ƒâ°, fâ‚ƒÂ¹, fâ‚ƒÂ², fâ‚ƒÂ³],  # Features du voxel 3
]  # (4, 4)

cluster = [0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3]

# Map unpooling
feat_mapped = [
    feat_low_proj[0],  # Point 0 â†’ voxel 0
    feat_low_proj[0],  # Point 1 â†’ voxel 0
    feat_low_proj[0],  # Point 2 â†’ voxel 0
    feat_low_proj[1],  # Point 3 â†’ voxel 1
    feat_low_proj[1],  # Point 4 â†’ voxel 1
    feat_low_proj[2],  # Point 5 â†’ voxel 2
    feat_low_proj[2],  # Point 6 â†’ voxel 2
    feat_low_proj[2],  # Point 7 â†’ voxel 2
    feat_low_proj[2],  # Point 8 â†’ voxel 2
    feat_low_proj[3],  # Point 9 â†’ voxel 3
    feat_low_proj[3],  # Point 10 â†’ voxel 3
    feat_low_proj[3],  # Point 11 â†’ voxel 3
]  # (12, 4)
```

Chaque point rÃ©cupÃ¨re **exactement** les features de son voxel d'origine !

### Ã‰tape 3 : Projection des Features Skip

```python
feat_skip_proj = Linear(feat_skip) â†’ BatchNorm1d â†’ ReLU
# feat_skip_proj: (N, out_ch)
```

### Ã‰tape 4 : Fusion Skip Connection

```python
feat_fused = feat_mapped + feat_skip_proj
# feat_fused: (N, out_ch)
```

**Visualisation :**

```
Basse rÃ©solution (upsampled):        Skip (haute rÃ©solution):
    feat_mapped                           feat_skip_proj
         â†“                                      â†“
    [0.2, 0.5, 0.1, 0.8]              [0.3, 0.1, 0.6, 0.2]
         â†“                                      â†“
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ + â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                    [0.5, 0.6, 0.7, 1.0]
                         feat_fused
```

### Ã‰tape 5 : Output

```python
return [coord_skip, feat_fused, offset_skip]
# On retourne les coordonnÃ©es skip (haute rÃ©solution)
# Avec les features fusionnÃ©es
```

---

## Exemple Complet NumÃ©rique

**Configuration :**
- M = 4 voxels (basse rÃ©solution)
- N = 12 points (haute rÃ©solution)
- in_ch = 3, skip_ch = 3, out_ch = 4

**Inputs :**

```python
# Basse rÃ©solution (voxels du GridPool)
coord_low = [
    [0.37, 0.33, 0.15],  # Voxel 0 (moyenne de 3 points)
    [1.3, 0.5, 0.35],    # Voxel 1 (moyenne de 2 points)
    [1.17, 1.83, 0.3],   # Voxel 2 (moyenne de 4 points)
    [0.1, 1.1, 0.9],     # Voxel 3 (moyenne de 3 points)
]  # (4, 3)

feat_low = [
    [0.8, 0.9, 0.7],  # Features voxel 0
    [0.6, 0.5, 0.8],  # Features voxel 1
    [0.9, 0.7, 0.6],  # Features voxel 2
    [0.4, 0.6, 0.9],  # Features voxel 3
]  # (4, 3)

# Haute rÃ©solution (points originaux de l'encodeur)
coord_skip = [
    [0.2, 0.3, 0.1],   # Point 0 (Ã©tait dans voxel 0)
    [0.8, 0.5, 0.2],   # Point 1 (Ã©tait dans voxel 0)
    [0.1, 0.2, 0.15],  # Point 2 (Ã©tait dans voxel 0)
    [1.2, 0.4, 0.3],   # Point 3 (Ã©tait dans voxel 1)
    [1.4, 0.6, 0.4],   # Point 4 (Ã©tait dans voxel 1)
    [1.5, 1.8, 0.1],   # Point 5 (Ã©tait dans voxel 2)
    [0.1, 1.1, 0.9],   # Point 6 (Ã©tait dans voxel 3)
    [2.1, 0.3, 0.2],   # Point 7 (Ã©tait dans voxel 2)
    [2.3, 0.5, 0.4],   # Point 8 (Ã©tait dans voxel 2)
    [0.9, 1.9, 0.3],   # Point 9 (Ã©tait dans voxel 2)
    [1.1, 1.8, 0.5],   # Point 10 (Ã©tait dans voxel 3)
    [2.2, 1.1, 0.6],   # Point 11 (Ã©tait dans voxel 3)
]  # (12, 3)

feat_skip = [...] # (12, 3)

# Cluster mapping (du GridPool correspondant)
cluster = [0, 0, 0, 1, 1, 2, 3, 2, 2, 2, 3, 3]
```

**Ã‰tape 1 : Projection feat_low**

```python
feat_low_proj = Linear(feat_low) â†’ BatchNorm â†’ ReLU
# feat_low_proj: (4, 4) par exemple
feat_low_proj = [
    [0.9, 0.8, 0.6, 0.7],  # Voxel 0
    [0.7, 0.6, 0.9, 0.5],  # Voxel 1
    [0.8, 0.7, 0.5, 0.9],  # Voxel 2
    [0.5, 0.7, 0.8, 0.6],  # Voxel 3
]
```

**Ã‰tape 2 : Map Unpooling**

```python
feat_mapped = feat_low_proj[cluster]  # Simple indexing !

# Point 0 â†’ cluster[0]=0 â†’ feat_low_proj[0]
feat_mapped[0] = [0.9, 0.8, 0.6, 0.7]

# Point 1 â†’ cluster[1]=0 â†’ feat_low_proj[0]
feat_mapped[1] = [0.9, 0.8, 0.6, 0.7]

# Point 2 â†’ cluster[2]=0 â†’ feat_low_proj[0]
feat_mapped[2] = [0.9, 0.8, 0.6, 0.7]

# Point 3 â†’ cluster[3]=1 â†’ feat_low_proj[1]
feat_mapped[3] = [0.7, 0.6, 0.9, 0.5]

# Point 4 â†’ cluster[4]=1 â†’ feat_low_proj[1]
feat_mapped[4] = [0.7, 0.6, 0.9, 0.5]

# Point 5 â†’ cluster[5]=2 â†’ feat_low_proj[2]
feat_mapped[5] = [0.8, 0.7, 0.5, 0.9]

# Point 6 â†’ cluster[6]=3 â†’ feat_low_proj[3]
feat_mapped[6] = [0.5, 0.7, 0.8, 0.6]

# Point 7 â†’ cluster[7]=2 â†’ feat_low_proj[2]
feat_mapped[7] = [0.8, 0.7, 0.5, 0.9]

# Point 8 â†’ cluster[8]=2 â†’ feat_low_proj[2]
feat_mapped[8] = [0.8, 0.7, 0.5, 0.9]

# Point 9 â†’ cluster[9]=2 â†’ feat_low_proj[2]
feat_mapped[9] = [0.8, 0.7, 0.5, 0.9]

# Point 10 â†’ cluster[10]=3 â†’ feat_low_proj[3]
feat_mapped[10] = [0.5, 0.7, 0.8, 0.6]

# Point 11 â†’ cluster[11]=3 â†’ feat_low_proj[3]
feat_mapped[11] = [0.5, 0.7, 0.8, 0.6]

# RÃ©sultat: (12, 4)
```

**Ã‰tape 3 : Projection feat_skip**

```python
feat_skip_proj = Linear(feat_skip) â†’ BatchNorm â†’ ReLU
# feat_skip_proj: (12, 4)
```

**Ã‰tape 4 : Fusion**

```python
feat_fused = feat_mapped + feat_skip_proj
# feat_fused: (12, 4)

# Exemple pour point 0
feat_fused[0] = [0.9, 0.8, 0.6, 0.7] + [0.3, 0.4, 0.5, 0.2]
              = [1.2, 1.2, 1.1, 0.9]
```

**Output :**

```python
coord_skip: (12, 3)   # Positions haute rÃ©solution
feat_fused: (12, 4)   # Features fusionnÃ©es
offset_skip: (B,)
```

---

## Comparaison Visuelle : Interpolation vs Map Unpooling

### PTv1 : K-NN Interpolation

```
Downsampling (FPS):              Upsampling (K-NN Interpolation):
                                 
16 points â†’ 4 points              4 points â†’ 16 points
                                  
â—â—â—â—                              â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—
â—â—â—â—    â†’ FPS â†’    â—‰   â—‰         â”‚   â”‚   â”‚   â”‚
â—â—â—â—                              â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—   â† K-NN pour chacun
â—â—â—â—               â—‰   â—‰         â”‚   â”‚   â”‚   â”‚
                                  â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—
                                  â”‚   â”‚   â”‚   â”‚
                                  â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—

Chaque â— cible:
  1. Cherche K=3 voisins parmi les 4 â—‰
  2. Calcule les distances
  3. Moyenne pondÃ©rÃ©e

CoÃ»t: 16 Ã— O(4 log 4) = O(N Ã— M log M)
```

### PTv2 : Map Unpooling

```
Downsampling (GridPool):         Upsampling (Map Unpooling):

16 points â†’ 4 voxels              4 voxels â†’ 16 points
+ cluster mapping

â—â—â—â—                              â—â—â—â—
â—â—â—â—  â†’ Grid â†’  [0,0,0,0,         [0,0,0,0,  â†’ â—â—â—â—
â—â—â—â—             1,1,1,1,         1,1,1,1,     â—â—â—â—
â—â—â—â—             2,2,2,2,         2,2,2,2,     â—â—â—â—
                 3,3,3,3]         3,3,3,3]

Voxel 0 â†’ 4 points                feat[â—â‚€] = feat_voxel[0]
Voxel 1 â†’ 4 points                feat[â—â‚] = feat_voxel[0]
Voxel 2 â†’ 4 points                feat[â—â‚‚] = feat_voxel[0]
Voxel 3 â†’ 4 points                feat[â—â‚ƒ] = feat_voxel[0]
                                  ...

CoÃ»t: 16 Ã— O(1) = O(N)  â† Lookup direct !
```

---

## Avantages du Map Unpooling

### 1. Vitesse

**ComplexitÃ© :**

```python
# PTv1 : K-NN Interpolation
ComplexitÃ©: O(N Ã— M log M)

# PTv2 : Map Unpooling
ComplexitÃ©: O(N)  â† Juste un indexing !

# Exemple: N=100k, M=25k
PTv1: 100k Ã— 25k Ã— log(25k) â‰ˆ 35 milliards d'ops
PTv2: 100k â‰ˆ 100k ops

Speedup: ~350,000Ã— sur cette opÃ©ration !
```

**En pratique, le speedup global est ~10-20Ã— car l'interpolation n'est qu'une partie du dÃ©codeur.**

### 2. Exactitude

```python
# PTv1 : Interpolation
point_original â†’ voxel_A â†’ interpolation
feat_final â‰ˆ feat_original  # Approximation

# PTv2 : Map exact
point_original â†’ voxel_A â†’ map unpooling
feat_final = feat_voxel[A]  # Exact (pas d'interpolation)
```

Les points rÃ©cupÃ¨rent **exactement** les features de leur voxel d'origine, sans interpolation artificielle.

### 3. MÃ©moire

```python
# PTv1 : Doit stocker les K-NN indices temporaires
knn_indices: (N, K)

# PTv2 : Cluster mapping dÃ©jÃ  stockÃ© du downsampling
cluster: (N,)  # DÃ©jÃ  en mÃ©moire, rÃ©utilisÃ©
```

### 4. DÃ©terminisme

```python
# PTv1 : K-NN peut Ãªtre ambigu
distances = [1.0, 1.0, 1.0, 1.0]  # 4 Ã©quidistants, K=3
# â†’ Quel trio choisir ? Non dÃ©terministe

# PTv2 : Mapping exact du downsampling
cluster[point_i] = voxel_id  # Toujours le mÃªme
```

---

## Encoder et Decoder : Vue ComplÃ¨te

### Encoder

{% include figure.liquid path="assets/img/poinTransformerV2/encoder.svg" class="img-fluid rounded z-depth-1" %}

```python
class Encoder:
    def forward(self, points):
        # Downsampling + enrichissement features
        points_pooled, cluster = GridPool(points)
        
        # Attention locale sur les voxels
        points_out = BlockSequence(points_pooled)
        
        return points_out, cluster
```

**Flow :**
```
Input: N points, in_ch
    â†“
GridPool (voxelisation)
    â†“
Nvoxel points, embed_ch
+ cluster mapping (N,)
    â†“
BlockSequence (depth blocks)
    â†“
Output: Nvoxel points, embed_ch
+ cluster (N,)  â† StockÃ© pour le dÃ©codeur !
```

### Decoder

{% include figure.liquid path="assets/img/poinTransformerV2/decoder.svg" class="img-fluid rounded z-depth-1" %}

```python
class Decoder:
    def forward(self, points_low, points_skip, cluster):
        # Upsampling + fusion skip
        points_up = UnpoolWithSkip(points_low, points_skip, cluster)
        
        # Attention locale sur les points upsampled
        points_out = BlockSequence(points_up)
        
        return points_out
```

**Flow :**
```
Input basse rÃ©solution: M points, in_ch
Input skip (encodeur): N points, skip_ch
cluster mapping: (N,)
    â†“
UnpoolWithSkip (map unpooling + fusion)
    â†“
N points, embed_ch
    â†“
BlockSequence (depth blocks)
    â†“
Output: N points, embed_ch
```

---

## Tableau RÃ©capitulatif des Innovations

| Composant | PTv1 | PTv2 | Gain |
|-----------|------|------|------|
| **Downsampling** | FPS O(NÂ²) | **GridPool O(N log N)** | 3-5Ã— speedup |
| **Upsampling** | K-NN Interpolation O(NM log M) | **Map Unpooling O(N)** | 10-20Ã— speedup |
| **Mapping stockÃ©** | âŒ Non | âœ… **cluster** rÃ©utilisÃ© | MÃ©moire efficient |
| **DÃ©terminisme** | âŒ Non (FPS alÃ©atoire) | âœ… Oui (grille fixe) | ReproductibilitÃ© |
| **Exactitude** | Interpolation approximative | **Mapping exact** | Plus prÃ©cis |

---

## Performance Globale : PTv1 vs PTv2

### Speedup par Composant

```
Component                PTv1        PTv2        Speedup
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
K-NN queries            24Ã—         4Ã—          6Ã—
Downsampling (FPS)      O(NÂ²)       O(N log N)  3-5Ã—
Upsampling (Interp)     O(NM log M) O(N)        10-20Ã—
Attention weights       576 params  128 params  4.5Ã—
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Overall                 Baseline    2-3Ã— faster
```

### MÃ©moire

```python
# PTv1
- Pas de cluster mapping
- K-NN temporaire Ã  chaque couche
Total: ~1.2Ã— baseline

# PTv2
- cluster mapping stockÃ© (N,) par niveau
- K-NN une fois par BlockSequence
Total: ~1.0Ã— baseline  (plus efficient !)
```

### PrÃ©cision

```
Dataset: S3DIS (segmentation sÃ©mantique)

PTv1: 70.4% mIoU
PTv2: 72.5% mIoU  (+2.1 points)

Speedup + meilleure prÃ©cision ! ğŸ¯
```

---

VoilÃ  ! Nous avons couvert toute l'architecture de PTv2 :

âœ… **GroupedLinear** : RÃ©duction paramÃ©trique  
âœ… **GroupedVectorAttention** : Attention enrichie  
âœ… **Block & BlockSequence** : Architecture rÃ©siduelle + K-NN partagÃ©  
âœ… **GVAPatchEmbed** : Embedding initial  
âœ… **GridPool** : Downsampling par voxelisation  
âœ… **UnpoolWithSkip** : Map unpooling + skip connections  
âœ… **Encoder & Decoder** : Architecture U-Net complÃ¨te  



